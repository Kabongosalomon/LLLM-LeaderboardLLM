{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMeOslZ9Qxlx",
    "outputId": "dc92a995-ef87-404f-e1da-2a9da7fbb6d6"
   },
   "outputs": [],
   "source": [
    "# !pip install --quiet  datasets #to access squad dataset\n",
    "# !pip install --quiet pyarrow   #to deal with parquet files for saving dataset if required\n",
    "# !pip install --quiet  tqdm     #for progress bars\n",
    "# !pip install --quiet transformers # for t5 model\n",
    "# !pip install --quiet tokenizers  #tokenizers from HuggingFace\n",
    "# !pip install --quiet sentencepiece #subword tokenizer used by T5\n",
    "# !pip install --quiet pytorch-lightning # pytorch wrapper \n",
    "# !pip install --quiet torchtext # text utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnkwYBQ6TWB4"
   },
   "source": [
    "# Fetching Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QnjddTNlS6Dz"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, Dataset, load_from_disk\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from pprint import pprint\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "INC4ECm5AFip"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device  = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_source = f\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_full_5_10_10000_clone_latex_compare/10Neg10000unk/twofoldwithunk\"\n",
    "path_to_csv = f\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_10000/10Neg10000unk/twofoldwithunk\"\n",
    "\n",
    "fold1 = \"fold1\"\n",
    "train_f1_pd = pd.read_csv(f\"{path_to_csv}/{fold1}/train.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "dev_f1_pd = pd.read_csv(f\"{path_to_csv}/{fold1}/dev.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "\n",
    "fold2 = \"fold2\"\n",
    "train_f2_pd = pd.read_csv(f\"{path_to_csv}/{fold2}/train.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "dev_f2_pd = pd.read_csv(f\"{path_to_csv}/{fold2}/dev.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4369</td>\n",
       "      <td>4369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4369</td>\n",
       "      <td>4365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0912.4438.pdf</td>\n",
       "      <td>! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title Context\n",
       "count            4369    4369\n",
       "unique           4369    4365\n",
       "top     0912.4438.pdf     ! !\n",
       "freq                1       2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no_leaderboard_pd = pd.read_csv(f\"/nfs/home/kabenamualus/Research/T5-Leaderboard-QA/data_proccess/arxiv_no_leaderboard_links_pdf_short/DocTAET_full.tsv\", \n",
    "#                     sep=\"\\t\", names=[\"title\", \"Context\"])\n",
    "\n",
    "no_leaderboard_pd = pd.read_csv(f\"/nfs/home/kabenamualus/Research/T5-Leaderboard-QA/data_proccess/arxiv_no_leaderboard_links_pdf_short/DocTAET_150.tsv\", \n",
    "                    sep=\"\\t\", names=[\"title\", \"Context\"])\n",
    "\n",
    "no_leaderboard_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>TDMSs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1704.03549v4.pdf</td>\n",
       "      <td>Optical Character Recognition#FSNS - Test#Sequ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1712.05404.pdf</td>\n",
       "      <td>Optical Character Recognition#FSNS - Test#Sequ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1702.03970v1.pdf</td>\n",
       "      <td>Optical Character Recognition#FSNS - Test#Sequ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2104.02324v1.pdf</td>\n",
       "      <td>Active Object Detection#COCO#AP#(7.3, 13.8, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008.12995v3.pdf</td>\n",
       "      <td>Handwriting Recognition#BanglaLekha Isolated D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5724</th>\n",
       "      <td>2104.01378v1.pdf</td>\n",
       "      <td>Phone-level pronunciation scoring#speechocean7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5725</th>\n",
       "      <td>2104.10283v1.pdf</td>\n",
       "      <td>Graph Question Answering#GQA#Accuracy#96.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5726</th>\n",
       "      <td>2104.11980v1.pdf</td>\n",
       "      <td>Trajectory Modeling#NBA SportVU#1x1 NLL#0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5727</th>\n",
       "      <td>1704.00077v1.pdf</td>\n",
       "      <td>Video Segmentation#SegTrack v2#Accuracy#86.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5728</th>\n",
       "      <td>2004.07922v1.pdf</td>\n",
       "      <td>Document Text Classification#Tobacco small-348...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5729 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title                                              TDMSs\n",
       "0     1704.03549v4.pdf  Optical Character Recognition#FSNS - Test#Sequ...\n",
       "1       1712.05404.pdf  Optical Character Recognition#FSNS - Test#Sequ...\n",
       "2     1702.03970v1.pdf  Optical Character Recognition#FSNS - Test#Sequ...\n",
       "3     2104.02324v1.pdf  Active Object Detection#COCO#AP#(7.3, 13.8, 16...\n",
       "4     2008.12995v3.pdf  Handwriting Recognition#BanglaLekha Isolated D...\n",
       "...                ...                                                ...\n",
       "5724  2104.01378v1.pdf  Phone-level pronunciation scoring#speechocean7...\n",
       "5725  2104.10283v1.pdf        Graph Question Answering#GQA#Accuracy#96.30\n",
       "5726  2104.11980v1.pdf      Trajectory Modeling#NBA SportVU#1x1 NLL#0.472\n",
       "5727  1704.00077v1.pdf      Video Segmentation#SegTrack v2#Accuracy#86.86\n",
       "5728  2004.07922v1.pdf  Document Text Classification#Tobacco small-348...\n",
       "\n",
       "[5729 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsAnnotation_pd = pd.read_csv(f\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/annotations_final/resultsAnnotation.tsv\",\n",
    "                                   sep=\"\\t\", names=[\"Title\", \"TDMSs\"])\n",
    "resultsAnnotation_pd = resultsAnnotation_pd.fillna(\"NAN\")\n",
    "resultsAnnotation_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5729/5729 [00:00<00:00, 124102.22it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This will take care of papers with more than one learderboard \n",
    "\"\"\"\n",
    "records = resultsAnnotation_pd.to_dict(\"records\")\n",
    "\n",
    "title_to_tdms_dict = defaultdict(\n",
    "    lambda : \n",
    "        list()\n",
    "    )\n",
    "\n",
    "for i, row in tqdm(enumerate(records), total = len(records)):\n",
    "    if row['TDMSs'] == 'NAN':\n",
    "        continue\n",
    "\n",
    "    for tdms in row['TDMSs'].split(\"$\"):\n",
    "        if len(tdms.split(\"#\")) != 4:\n",
    "            # ipdb.set_trace()\n",
    "            continue \n",
    "        t, d, m, s = tdms.split(\"#\")\n",
    "        title_to_tdms_dict[row['Title']].append(\n",
    "            {\n",
    "                \"LEADERBOARD\": {\n",
    "                    \"Task\": t,\n",
    "                    \"Dataset\": d,\n",
    "                    \"Metric\": m,\n",
    "                    # \"Score\": s,\n",
    "                }\n",
    "            }            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5725"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_to_tdms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f1_pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12613</td>\n",
       "      <td>12613</td>\n",
       "      <td>12613</td>\n",
       "      <td>12613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>3753</td>\n",
       "      <td>1792</td>\n",
       "      <td>3747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.00933v1.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12613</td>\n",
       "      <td>58</td>\n",
       "      <td>923</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label             title      TDM  \\\n",
       "count   12613             12613    12613   \n",
       "unique      1              3753     1792   \n",
       "top      True  1803.00933v1.pdf  unknown   \n",
       "freq    12613                58      923   \n",
       "\n",
       "                                                  Context  \n",
       "count                                               12613  \n",
       "unique                                               3747  \n",
       "top     IMPALA: Scalable Distributed Deep-RL with Impo...  \n",
       "freq                                                   58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_f1_pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5472</td>\n",
       "      <td>5472</td>\n",
       "      <td>5472</td>\n",
       "      <td>5472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1608</td>\n",
       "      <td>1557</td>\n",
       "      <td>1606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>1911.08265v2.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mastering Atari, Go, Chess and Shogi by Planni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5472</td>\n",
       "      <td>58</td>\n",
       "      <td>378</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label             title      TDM  \\\n",
       "count   5472              5472     5472   \n",
       "unique     1              1608     1557   \n",
       "top     True  1911.08265v2.pdf  unknown   \n",
       "freq    5472                58      378   \n",
       "\n",
       "                                                  Context  \n",
       "count                                                5472  \n",
       "unique                                               1606  \n",
       "top     Mastering Atari, Go, Chess and Shogi by Planni...  \n",
       "freq                                                   58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f2_pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12677</td>\n",
       "      <td>12677</td>\n",
       "      <td>12677</td>\n",
       "      <td>12677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>3753</td>\n",
       "      <td>1821</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>1911.08265v2.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mastering Atari, Go, Chess and Shogi by Planni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12677</td>\n",
       "      <td>58</td>\n",
       "      <td>920</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label             title      TDM  \\\n",
       "count   12677             12677    12677   \n",
       "unique      1              3753     1821   \n",
       "top      True  1911.08265v2.pdf  unknown   \n",
       "freq    12677                58      920   \n",
       "\n",
       "                                                  Context  \n",
       "count                                               12677  \n",
       "unique                                               3749  \n",
       "top     Mastering Atari, Go, Chess and Shogi by Planni...  \n",
       "freq                                                   58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_f2_pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5408</td>\n",
       "      <td>5408</td>\n",
       "      <td>5408</td>\n",
       "      <td>5408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1608</td>\n",
       "      <td>1542</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>True</td>\n",
       "      <td>1802.01561v3.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>IMPALA: Scalable Distributed Deep-RL with Impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5408</td>\n",
       "      <td>58</td>\n",
       "      <td>381</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label             title      TDM  \\\n",
       "count   5408              5408     5408   \n",
       "unique     1              1608     1542   \n",
       "top     True  1802.01561v3.pdf  unknown   \n",
       "freq    5408                58      381   \n",
       "\n",
       "                                                  Context  \n",
       "count                                                5408  \n",
       "unique                                               1608  \n",
       "top     IMPALA: Scalable Distributed Deep-RL with Impo...  \n",
       "freq                                                   58  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No need for negative instances, but will still have 'duplicate' for paper with more than one leaderboard\n",
    "train_f1_pd = train_f1_pd[train_f1_pd.label==True]\n",
    "print(\"train_f1_pd\")\n",
    "display(train_f1_pd.describe())\n",
    "\n",
    "dev_f1_pd = dev_f1_pd[dev_f1_pd.label==True]\n",
    "print(\"dev_f1_pd\")\n",
    "display(dev_f1_pd.describe())\n",
    "\n",
    "train_f2_pd = train_f2_pd[train_f2_pd.label==True]\n",
    "print(\"train_f2_pd\")\n",
    "display(train_f2_pd.describe())\n",
    "\n",
    "dev_f2_pd = dev_f2_pd[dev_f2_pd.label==True]\n",
    "print(\"dev_f2_pd\")\n",
    "display(dev_f2_pd.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_pd.title.unique())\n",
    "records_train_f1 = train_f1_pd.to_dict(\"records\")\n",
    "records_dev_f1 = dev_f1_pd.to_dict(\"records\")\n",
    "records_train_f2 = train_f2_pd.to_dict(\"records\")\n",
    "records_dev_f2 = dev_f2_pd.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1707.03497v2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_id = records_train_f1[0][\"title\"].split(\".pdf\")[0]\n",
    "title_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/12613 [00:00<01:28, 142.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12613/12613 [03:17<00:00, 63.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f1 missed long context: 938/12613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5472/5472 [01:05<00:00, 83.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_f1 missed long context: 416/5472\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12677/12677 [00:03<00:00, 3987.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f2 missed long context: 939/12677\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5408/5408 [00:01<00:00, 4311.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f2 missed long context: 415/5408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# title_to_tdms_dict = defaultdict(lambda : defaultdict(lambda : str(\"| \")))\n",
    "title_to_content = {\n",
    "    \"train_f1\":{},\n",
    "    \"dev_f1\":{},\n",
    "    \"train_f2\":{},\n",
    "    \"dev_f2\":{},\n",
    "    }\n",
    "\n",
    "arxiv_leaderboard_full_txt = \"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/arxiv_leaderboard_full_txt\"\n",
    "\n",
    "missed = 0\n",
    "for i, row in tqdm(enumerate(records_train_f1), total = len(records_train_f1)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in title_to_content[\"train_f1\"]:\n",
    "        continue \n",
    "    else:\n",
    "        try:\n",
    "            with open(f'{arxiv_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "                # Read the file\n",
    "                data = file.read()\n",
    "                \n",
    "        except :\n",
    "            # print(f\"Error on file {row['title']}\")\n",
    "            data = \"\" \n",
    "            missed += 1\n",
    "        \n",
    "        title_to_content[\"train_f1\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "        # title_to_content[\"train_f1\"][row['title']] = row['Context']\n",
    "\n",
    "print(f\"train_f1 missed long context: {missed}/{len(records_train_f1)}\\n\")\n",
    "\n",
    "missed = 0\n",
    "for i, row in tqdm(enumerate(records_dev_f1), total = len(records_dev_f1)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in title_to_content[\"dev_f1\"]:\n",
    "        continue \n",
    "    else:\n",
    "        try:\n",
    "            with open(f'{arxiv_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "                # Read the file\n",
    "                data = file.read()\n",
    "                \n",
    "        except :\n",
    "            # print(f\"Error on file {row['title']}\")\n",
    "            data = \"\"\n",
    "            missed += 1\n",
    "            # continue \n",
    "        \n",
    "        title_to_content[\"dev_f1\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "\n",
    "print(f\"dev_f1 missed long context: {missed}/{len(records_dev_f1)}\\n\")       \n",
    "        \n",
    "missed = 0        \n",
    "for i, row in tqdm(enumerate(records_train_f2), total = len(records_train_f2)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in title_to_content[\"train_f2\"]:\n",
    "        continue \n",
    "    else:\n",
    "        try:\n",
    "            with open(f'{arxiv_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "                # Read the file\n",
    "                data = file.read()\n",
    "                \n",
    "        except :\n",
    "            # print(f\"Error on file {row['title']}\")\n",
    "            data = \"\"\n",
    "            missed += 1\n",
    "        \n",
    "        title_to_content[\"train_f2\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "           \n",
    "print(f\"train_f2 missed long context: {missed}/{len(records_train_f2)}\\n\")       \n",
    "\n",
    "missed = 0\n",
    "for i, row in tqdm(enumerate(records_dev_f2), total = len(records_dev_f2)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in title_to_content[\"dev_f2\"]:\n",
    "        continue \n",
    "    else:\n",
    "        try:\n",
    "            with open(f'{arxiv_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "                # Read the file\n",
    "                data = file.read()\n",
    "                \n",
    "        except :\n",
    "            # print(f\"Error on file {row['title']}\")\n",
    "            data = \"\"\n",
    "            missed += 1\n",
    "            # continue\n",
    "            \n",
    "        title_to_content[\"dev_f2\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "        # title_to_content[\"dev_f2\"][row['title']] = row['Context']\n",
    "\n",
    "print(f\"train_f2 missed long context: {missed}/{len(records_dev_f2)}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_leaderboard_pourcentage_train_f1: 1876\n",
      "no_leaderboard_pourcentage_dev_f1: 804\n",
      "no_leaderboard_pourcentage_train_f2: 1876\n",
      "no_leaderboard_pourcentage_dev_f2: 804\n"
     ]
    }
   ],
   "source": [
    "no_leaderboard_pourcentage_train_f1 = int(len(train_f1_pd.title.unique())*50/100)\n",
    "no_leaderboard_pourcentage_dev_f1 = int(len(dev_f1_pd.title.unique())*50/100)\n",
    "no_leaderboard_pourcentage_train_f2 = int(len(train_f2_pd.title.unique())*50/100)\n",
    "no_leaderboard_pourcentage_dev_f2 = int(len(dev_f2_pd.title.unique())*50/100)\n",
    "\n",
    "print(f\"no_leaderboard_pourcentage_train_f1: {no_leaderboard_pourcentage_train_f1}\")\n",
    "print(f\"no_leaderboard_pourcentage_dev_f1: {no_leaderboard_pourcentage_dev_f1}\")\n",
    "print(f\"no_leaderboard_pourcentage_train_f2: {no_leaderboard_pourcentage_train_f2}\")\n",
    "print(f\"no_leaderboard_pourcentage_dev_f2: {no_leaderboard_pourcentage_dev_f2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_leaderboard_pourcentage = int(len(train_pd.title.unique())*50/100)\n",
    "# no_leaderboard_pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4369 [00:00<02:08, 34.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1876/4369 [01:01<01:22, 30.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f1 missed long context: 171/1876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 2680/4369 [00:33<00:21, 79.84it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_f1 missed long context: 55/804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1876/4369 [00:01<00:01, 1247.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f2 missed long context: 171/1876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 2680/4369 [00:00<00:00, 3517.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_f2 missed long context: 55/804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "records = no_leaderboard_pd.to_dict(\"records\")\n",
    "\n",
    "# For train only F1\n",
    "no_lead_papers_train_f1 = []\n",
    "\n",
    "already_seen = no_lead_papers_train_f1\n",
    "no_lead_papers_train_f1 = []\n",
    "i = 0\n",
    "\n",
    "arxiv_no_leaderboard_full_txt = \"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/arxiv_no_leaderboard_full_txt\"\n",
    "\n",
    "missed = 0\n",
    "for _, row in tqdm(enumerate(records), total = len(records)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in already_seen:\n",
    "        continue \n",
    "        \n",
    "    if i >= no_leaderboard_pourcentage_train_f1:\n",
    "        break \n",
    "    \n",
    "    try:\n",
    "        with open(f'{arxiv_no_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "            # Read the file\n",
    "            data = file.read()\n",
    "            \n",
    "    except :\n",
    "        # print(f\"Error on file {row['title']}\")\n",
    "        data = \"\"\n",
    "        missed += 1\n",
    "                \n",
    "    title_to_content[\"train_f1\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "    no_lead_papers_train_f1.append(row['title'])\n",
    "    i += 1\n",
    "print(f\"train_f1 missed long context: {missed}/{i}\\n\")       \n",
    "    \n",
    "no_lead_papers_dev_f1 = no_lead_papers_train_f1\n",
    "already_seen = no_lead_papers_dev_f1\n",
    "no_lead_papers_dev_f1 = []\n",
    "i = 0\n",
    "missed = 0\n",
    "for _, row in tqdm(enumerate(records), total = len(records)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in already_seen:\n",
    "        continue \n",
    "        \n",
    "    if i >= no_leaderboard_pourcentage_dev_f1:\n",
    "        break \n",
    "     \n",
    "    try:\n",
    "        with open(f'{arxiv_no_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "            # Read the file\n",
    "            data = file.read()\n",
    "            \n",
    "    except :\n",
    "        # print(f\"Error on file {row['title']}\")\n",
    "        data = \"\"\n",
    "        missed += 1\n",
    "           \n",
    "    title_to_content[\"dev_f1\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "    no_lead_papers_dev_f1.append(row['title'])  \n",
    "    i += 1\n",
    "print(f\"dev_f1 missed long context: {missed}/{i}\\n\")       \n",
    "\n",
    "    \n",
    "# For train only F2\n",
    "no_lead_papers_train_f2 = []\n",
    "\n",
    "already_seen = no_lead_papers_train_f2\n",
    "no_lead_papers_train_f2 = []\n",
    "j = 0\n",
    "missed = 0\n",
    "for _, row in tqdm(enumerate(records), total = len(records)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in already_seen:\n",
    "        continue \n",
    "        \n",
    "    if j >= no_leaderboard_pourcentage_train_f2:\n",
    "        break \n",
    "    \n",
    "    try:\n",
    "        with open(f'{arxiv_no_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "            # Read the file\n",
    "            data = file.read()\n",
    "            \n",
    "    except :\n",
    "        # print(f\"Error on file {row['title']}\")\n",
    "        data = \"\"\n",
    "        missed += 1\n",
    "        \n",
    "    title_to_content[\"train_f2\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "    no_lead_papers_train_f2.append(row['title'])\n",
    "    j += 1\n",
    "print(f\"train_f2 missed long context: {missed}/{j}\\n\")  \n",
    "    \n",
    "no_lead_papers_dev_f2 = no_lead_papers_train_f2\n",
    "already_seen = no_lead_papers_dev_f2\n",
    "no_lead_papers_dev_f2 = []\n",
    "j = 0\n",
    "missed = 0\n",
    "for _, row in tqdm(enumerate(records), total = len(records)):\n",
    "    title_id = row['title'].split(\".pdf\")[0]\n",
    "    if row['title'] in already_seen:\n",
    "        continue \n",
    "        \n",
    "    if j >= no_leaderboard_pourcentage_dev_f2:\n",
    "        break \n",
    "        \n",
    "    try:\n",
    "        with open(f'{arxiv_no_leaderboard_full_txt}/{title_id}.txt', 'r') as file:\n",
    "            # Read the file\n",
    "            data = file.read()\n",
    "            \n",
    "    except :\n",
    "        # print(f\"Error on file {row['title']}\")\n",
    "        data = \"\"\n",
    "        missed += 1\n",
    "        \n",
    "    title_to_content[\"dev_f2\"][row['title']] = row['Context'] if len(data.split()) < 100 else data\n",
    "    no_lead_papers_dev_f2.append(row['title'])\n",
    "    j += 1\n",
    "print(f\"train_f2 missed long context: {missed}/{j}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_f1_pd[\"Lenght context\"] = train_f1_pd.Context.apply(lambda x: len(x.split()))\n",
    "# dev_f1_pd[\"Lenght context\"] = dev_f1_pd.Context.apply(lambda x: len(x.split()))\n",
    "# train_f2_pd[\"Lenght context\"] = train_f2_pd.Context.apply(lambda x: len(x.split()))\n",
    "# dev_f2_pd[\"Lenght context\"] = dev_f2_pd.Context.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pd[train_pd[\"Lenght context\"] < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pd = train_pd[train_pd[\"Lenght context\"] < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"train_f1_pd describe: \")\n",
    "# display(train_f1_pd.describe())\n",
    "# print(\"dev_f1_pd describe: \")\n",
    "# display(dev_f1_pd.describe())\n",
    "\n",
    "# print(\"train_f2_pd describe: \")\n",
    "# display(train_f2_pd.describe())\n",
    "# print(\"dev_f2_pd describe: \")\n",
    "# display(dev_f2_pd.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 311/5629 [00:00<00:03, 1558.72it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5629/5629 [00:03<00:00, 1485.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_f1 describe: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght Context</th>\n",
       "      <th>Lenght TDMSs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5512.000000</td>\n",
       "      <td>5512.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3769.126814</td>\n",
       "      <td>38.429064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3641.882691</td>\n",
       "      <td>85.362865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>354.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3930.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5700.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47576.000000</td>\n",
       "      <td>2455.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lenght Context  Lenght TDMSs\n",
       "count     5512.000000   5512.000000\n",
       "mean      3769.126814     38.429064\n",
       "std       3641.882691     85.362865\n",
       "min          3.000000      1.000000\n",
       "25%        354.000000      1.000000\n",
       "50%       3930.000000     16.000000\n",
       "75%       5700.000000     44.000000\n",
       "max      47576.000000   2455.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2412/2412 [00:01<00:00, 1617.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dev_f1 describe: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght Context</th>\n",
       "      <th>Lenght TDMSs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2353.000000</td>\n",
       "      <td>2353.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3821.762431</td>\n",
       "      <td>38.007650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3472.276893</td>\n",
       "      <td>71.784381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>360.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4122.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5758.000000</td>\n",
       "      <td>43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36493.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lenght Context  Lenght TDMSs\n",
       "count     2353.000000   2353.000000\n",
       "mean      3821.762431     38.007650\n",
       "std       3472.276893     71.784381\n",
       "min          6.000000      1.000000\n",
       "25%        360.000000      1.000000\n",
       "50%       4122.000000     16.000000\n",
       "75%       5758.000000     43.000000\n",
       "max      36493.000000   1530.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5629/5629 [00:03<00:00, 1510.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_f2 describe: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght Context</th>\n",
       "      <th>Lenght TDMSs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5513.000000</td>\n",
       "      <td>5513.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3755.273173</td>\n",
       "      <td>38.690731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3561.476945</td>\n",
       "      <td>83.986774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>344.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3981.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5712.000000</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>41780.000000</td>\n",
       "      <td>2455.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lenght Context  Lenght TDMSs\n",
       "count     5513.000000   5513.000000\n",
       "mean      3755.273173     38.690731\n",
       "std       3561.476945     83.986774\n",
       "min          3.000000      1.000000\n",
       "25%        344.000000      1.000000\n",
       "50%       3981.000000     16.000000\n",
       "75%       5712.000000     44.000000\n",
       "max      41780.000000   2455.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2412/2412 [00:01<00:00, 1453.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dev_f2 describe: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lenght Context</th>\n",
       "      <th>Lenght TDMSs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2352.000000</td>\n",
       "      <td>2352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3854.257228</td>\n",
       "      <td>37.394133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3661.857514</td>\n",
       "      <td>75.481169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>379.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4074.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5722.500000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>47576.000000</td>\n",
       "      <td>1537.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Lenght Context  Lenght TDMSs\n",
       "count     2352.000000   2352.000000\n",
       "mean      3854.257228     37.394133\n",
       "std       3661.857514     75.481169\n",
       "min          6.000000      1.000000\n",
       "25%        379.000000      1.000000\n",
       "50%       4074.000000     16.000000\n",
       "75%       5722.500000     45.000000\n",
       "max      47576.000000   1537.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_f1 = pd.DataFrame(columns = [\"Title\", \"TDMSs\", \"Context\"])\n",
    "for i, title in tqdm(enumerate(title_to_content[\"train_f1\"].keys()), total = len(title_to_content[\"train_f1\"].keys())):\n",
    "    \n",
    "    if (len(title_to_content[\"train_f1\"][title]) < 10):\n",
    "        continue \n",
    "    \n",
    "    if (title not in no_lead_papers_train_f1) :\n",
    "        if (title_to_tdms_dict[title] == []):\n",
    "             continue\n",
    "\n",
    "    df_train_f1 = pd.concat([df_train_f1, pd.DataFrame.from_records(\n",
    "        [\n",
    "            {\n",
    "                'Title' : title, \n",
    "                'TDMSs' : title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\",\n",
    "                'Context' : title_to_content[\"train_f1\"][title],\n",
    "                'Lenght Context': len(title_to_content[\"train_f1\"][title].split()),\n",
    "                'Lenght TDMSs': len(str(title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\").split())\n",
    "            }\n",
    "        ])], ignore_index = True)\n",
    "print(\"df_train_f1 describe: \")\n",
    "display(df_train_f1.describe())  \n",
    "\n",
    "df_dev_f1 = pd.DataFrame(columns = [\"Title\", \"TDMSs\", \"Context\"])  \n",
    "for i, title in tqdm(enumerate(title_to_content[\"dev_f1\"].keys()), total = len(title_to_content[\"dev_f1\"].keys())):\n",
    "    \n",
    "    if (len(title_to_content[\"dev_f1\"][title]) < 10):\n",
    "        continue \n",
    "    \n",
    "    if (title not in no_lead_papers_dev_f1) :\n",
    "        if (title_to_tdms_dict[title] == []):\n",
    "             continue\n",
    "\n",
    "    df_dev_f1 = pd.concat([df_dev_f1, pd.DataFrame.from_records(\n",
    "        [\n",
    "            {\n",
    "                'Title' : title, \n",
    "                'TDMSs' : title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\",\n",
    "                'Context' : title_to_content[\"dev_f1\"][title],\n",
    "                'Lenght Context': len(title_to_content[\"dev_f1\"][title].split()),\n",
    "                'Lenght TDMSs': len(str(title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\").split())\n",
    "            }\n",
    "        ])], ignore_index = True)\n",
    "print(\"df_dev_f1 describe: \")\n",
    "display(df_dev_f1.describe())  \n",
    "\n",
    "df_train_f2 = pd.DataFrame(columns = [\"Title\", \"TDMSs\", \"Context\"])\n",
    "for i, title in tqdm(enumerate(title_to_content[\"train_f2\"].keys()), total = len(title_to_content[\"train_f2\"].keys())):\n",
    "    \n",
    "    if (len(title_to_content[\"train_f2\"][title]) < 10):\n",
    "        continue \n",
    "    \n",
    "    if (title not in no_lead_papers_train_f2) :\n",
    "        if (title_to_tdms_dict[title] == []):\n",
    "             continue\n",
    "\n",
    "    df_train_f2 = pd.concat([df_train_f2, pd.DataFrame.from_records(\n",
    "        [\n",
    "            {\n",
    "                'Title' : title, \n",
    "                'TDMSs' : title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\",\n",
    "                'Context' : title_to_content[\"train_f2\"][title],\n",
    "                'Lenght Context': len(title_to_content[\"train_f2\"][title].split()),\n",
    "                'Lenght TDMSs': len(str(title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\").split())\n",
    "            }\n",
    "        ])], ignore_index = True)\n",
    "print(\"df_train_f2 describe: \")\n",
    "display(df_train_f2.describe())  \n",
    " \n",
    "df_dev_f2 = pd.DataFrame(columns = [\"Title\", \"TDMSs\", \"Context\"])  \n",
    "for i, title in tqdm(enumerate(title_to_content[\"dev_f2\"].keys()), total = len(title_to_content[\"dev_f2\"].keys())):\n",
    "    \n",
    "    if (len(title_to_content[\"dev_f2\"][title]) < 10):\n",
    "        continue \n",
    "    \n",
    "    if (title not in no_lead_papers_dev_f2) :\n",
    "        if (title_to_tdms_dict[title] == []):\n",
    "             continue\n",
    "\n",
    "    df_dev_f2 = pd.concat([df_dev_f2, pd.DataFrame.from_records(\n",
    "        [\n",
    "            {\n",
    "                'Title' : title, \n",
    "                'TDMSs' : title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\",\n",
    "                'Context' : title_to_content[\"dev_f2\"][title],\n",
    "                'Lenght Context': len(title_to_content[\"dev_f2\"][title].split()),\n",
    "                'Lenght TDMSs': len(str(title_to_tdms_dict[title] if title in title_to_tdms_dict.keys() else \"unanswerable\").split())\n",
    "            }\n",
    "        ])], ignore_index = True)\n",
    "print(\"df_dev_f2 describe: \")\n",
    "display(df_dev_f2.describe())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please answer a question about this article. If the question is unanswerable, say \"unanswerable\"',\n",
       " 'Read this and answer the question. If the question is unanswerable, say \"unanswerable\".',\n",
       " 'If the question is unanswerable, say \"unanswerable\"',\n",
       " 'Try to answer this question if possible (otherwise reply \"unanswerable\"',\n",
       " 'If it is possible to answer this question, answer it for me (else, reply \"unanswerable\"',\n",
       " 'Answer this question, if possible (if impossible, reply \"unanswerable\"',\n",
       " 'Read this: What is the answer? (If it cannot be answered, return \"unanswerable\"',\n",
       " 'Read this: Now answer this question, if there is an answer (If it cannot be answered, return \"unanswerable\"',\n",
       " 'Answer based on context:',\n",
       " 'Answer this question based on the article:',\n",
       " 'Answer this question:',\n",
       " 'Read this article and answer this question',\n",
       " 'Based on the above article, answer a question.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ['Please answer a question about this article. If the question is unanswerable, say \\\"unanswerable\\\"',\n",
    "'Read this and answer the question. If the question is unanswerable, say \\\"unanswerable\\\".',\n",
    "'If the question is unanswerable, say \\\"unanswerable\\\"',\n",
    "'Try to answer this question if possible (otherwise reply \\\"unanswerable\\\"',\n",
    "'If it is possible to answer this question, answer it for me (else, reply \\\"unanswerable\\\"',\n",
    "'Answer this question, if possible (if impossible, reply \\\"unanswerable\\\"',\n",
    "'Read this: What is the answer? (If it cannot be answered, return \\\"unanswerable\\\"',\n",
    "'Read this: Now answer this question, if there is an answer (If it cannot be answered, return \\\"unanswerable\\\"',\n",
    "'Answer based on context:',\n",
    "'Answer this question based on the article:',\n",
    "# (\"{context}\\n\\n{question}\", \"{answer}\"),\n",
    "'Answer this question:',\n",
    "'Read this article and answer this question',\n",
    "'Based on the above article, answer a question.',\n",
    "# 'Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\", \"{answer}\"),\n",
    "]\n",
    "\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cRRLG8gZ1IcN"
   },
   "outputs": [],
   "source": [
    "def create_pandas_dataset_from_pandas(df,\n",
    "                          answer_threshold=7,\n",
    "                          verbose = False):\n",
    "\n",
    "  ''' Create a Pandas Dataframe from pandas.\n",
    "  Params:\n",
    "        answer_threshold: Only consider those Question Answer pairs where the Answer is short.\n",
    "  '''\n",
    "  count_index = 0\n",
    "  result_df  = pd.DataFrame(columns = ['prompt', 'answer'])   \n",
    "  # q_type_1 = \"Which Tasks are addressed in this article\"\n",
    "  # q_type_2 = \"Which Datasets are addressed in this article\"\n",
    "  # q_type_3 = \"Which Metrics are addressed in this article\"\n",
    "  # q_type_4 = \"Which Tasks, Datasets, Metrics are addressed in this article\"\n",
    "  # q_type_5 = \"Which Tasks, Datasets, Metrics and Scores are addressed in this article\" \n",
    "  \n",
    "  # q_types = [\n",
    "  #   {\"q\": \"Which Tasks are addressed in this article\", \"a_key\": \"Tasks\"}, \n",
    "  #   {\"q\": \"Which Datasets are addressed in this article\", \"a_key\": \"Datasets\"}, \n",
    "  #   {\"q\": \"Which Metrics are addressed in this article\", \"a_key\": \"Metrics\"},\n",
    "  #   {\"q\": \"Which Tasks, Datasets, Metrics are addressed in this article\", \"a_key\": \"TDMs\"},\n",
    "  #   {\"q\": \"Which Tasks, Datasets, Metrics and Scores are addressed in this article\", \"a_key\": \"TDMSs\"}\n",
    "  #   ]\n",
    "  \n",
    "  q_types = [\n",
    "    {\"q\": \"What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, and metric?\", \"a_key\": \"TDMSs\"},\n",
    "    # {\"q\": \"What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\", \"a_key\": \"TDMSs\"},\n",
    "    ]\n",
    "  \n",
    "  records = df.to_dict(\"records\")\n",
    "  # db_dict = defaultdict(lambda : list())\n",
    "  for i, row in tqdm(enumerate(records), total = len(records)):        \n",
    "      for q_type in q_types:\n",
    "        \n",
    "        # Squad_v2\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\n\\nPlease answer a question about this article. If the question is unanswerable, say \\\"unanswerable\\\". {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'Read this and answer the question. If the question is unanswerable, say \\\"unanswerable\\\".\\n\\n{row[\"Context\"]}\\n\\n{q_type[\"q\"]}'\n",
    "] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\n{q_type[\"q\"]} (If the question is unanswerable, say \\\"unanswerable\\\"'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\nTry to answer this question if possible (otherwise reply \\\"unanswerable\\\"): {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\nIf it is possible to answer this question, answer it for me (else, reply \\\"unanswerable\\\"): {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\n\\nAnswer this question, if possible (if impossible, reply \\\"unanswerable\\\"): {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'Read this: {row[\"Context\"]}\\n\\n{q_type[\"q\"]}\\nWhat is the answer? (If it cannot be answered, return \\\"unanswerable\\\")'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'Read this: {row[\"Context\"]}\\nNow answer this question, if there is an answer (If it cannot be answered, return \\\"unanswerable\\\"): {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        \n",
    "        # Drop\n",
    "        result_df.loc[count_index] = [f'Answer based on context:\\n\\n{row[\"Context\"]}\\n\\n{q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\n\\nAnswer this question based on the article: {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\n\\n{q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\nAnswer this question: {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'Read this article and answer this question {row[\"Context\"]}\\n{q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'{row[\"Context\"]}\\n\\nBased on the above article, answer a question. {q_type[\"q\"]}'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "        result_df.loc[count_index] = [f'Context: {row[\"Context\"]}\\n\\nQuestion: {q_type[\"q\"]}\\n\\nAnswer:'] \\\n",
    "          + [str(row[q_type[\"a_key\"]])] \n",
    "        count_index += 1\n",
    "         \n",
    "            \n",
    "  if verbose:\n",
    "    # return (result_df,\n",
    "    #         count_long,\n",
    "    #         count_short)\n",
    "    return (result_df)\n",
    "  else:\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/5512 [00:00<00:40, 134.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5512/5512 [05:45<00:00, 15.95it/s]\n",
      "100%|██████████| 2353/2353 [00:38<00:00, 60.68it/s]\n",
      "100%|██████████| 5513/5513 [06:04<00:00, 15.13it/s]\n",
      "100%|██████████| 2352/2352 [00:42<00:00, 54.99it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train_f1_all_templates = create_pandas_dataset_from_pandas(df_train_f1) \n",
    "df_dev_f1_all_templates = create_pandas_dataset_from_pandas(df_dev_f1) \n",
    "df_train_f2_all_templates = create_pandas_dataset_from_pandas(df_train_f2) \n",
    "df_dev_f2_all_templates = create_pandas_dataset_from_pandas(df_dev_f2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82680</td>\n",
       "      <td>82680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>82335</td>\n",
       "      <td>2821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Title\\tLaTeX Author Guidelines for CVPR Procee...</td>\n",
       "      <td>unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>28080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt        answer\n",
       "count                                               82680         82680\n",
       "unique                                              82335          2821\n",
       "top     Title\\tLaTeX Author Guidelines for CVPR Procee...  unanswerable\n",
       "freq                                                    5         28080"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_f1_all_templates.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_f1_all_templates describe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82680</td>\n",
       "      <td>82680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>82335</td>\n",
       "      <td>2821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Title\\tLaTeX Author Guidelines for CVPR Procee...</td>\n",
       "      <td>unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>28080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt        answer\n",
       "count                                               82680         82680\n",
       "unique                                              82335          2821\n",
       "top     Title\\tLaTeX Author Guidelines for CVPR Procee...  unanswerable\n",
       "freq                                                    5         28080"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dev_f1_all_templates describe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35295</td>\n",
       "      <td>35295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>35145</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Read this: Title\\tLaTeX Author Guidelines for ...</td>\n",
       "      <td>unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>12060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt        answer\n",
       "count                                               35295         35295\n",
       "unique                                              35145          1338\n",
       "top     Read this: Title\\tLaTeX Author Guidelines for ...  unanswerable\n",
       "freq                                                    5         12060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_f2_all_templates describe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>82695</td>\n",
       "      <td>82695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>82260</td>\n",
       "      <td>2849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Title\\tLaTeX Author Guidelines for CVPR Procee...</td>\n",
       "      <td>unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>28080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt        answer\n",
       "count                                               82695         82695\n",
       "unique                                              82260          2849\n",
       "top     Title\\tLaTeX Author Guidelines for CVPR Procee...  unanswerable\n",
       "freq                                                    6         28080"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dev_f2_all_templates describe: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35280</td>\n",
       "      <td>35280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>35205</td>\n",
       "      <td>1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Title\\tBare Advanced Demo of IEEEtran.cls for\\...</td>\n",
       "      <td>unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>12060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   prompt        answer\n",
       "count                                               35280         35280\n",
       "unique                                              35205          1326\n",
       "top     Title\\tBare Advanced Demo of IEEEtran.cls for\\...  unanswerable\n",
       "freq                                                    2         12060"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"df_train_f1_all_templates describe: \")\n",
    "display(df_train_f1_all_templates.describe())\n",
    "print(\"df_dev_f1_all_templates describe: \")\n",
    "display(df_dev_f1_all_templates.describe())\n",
    "\n",
    "print(\"df_train_f2_all_templates describe: \")\n",
    "display(df_train_f2_all_templates.describe())\n",
    "print(\"df_dev_f2_all_templates describe: \")\n",
    "display(df_dev_f2_all_templates.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "<class 'str'>    82680\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_f1_all_templates['answer'].apply(type).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "<class 'str'>    35295\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_f1_all_templates['answer'].apply(type).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title\\tValue Prediction Network\\n\\nAbstract:\\tThis paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.\\n\\nIntroduction\\n\\nModel-based reinforcement learning (RL) approaches attempt to learn a model that predicts future observations conditioned on actions and can thus be used to simulate the real environment and do multi-step lookaheads for planning. We will call such models an observation-prediction model to distinguish it from another form of model introduced in this paper. Building an accurate observation-prediction model is often very challenging when the observation space is large\\xa0 (e.g., high-dimensional pixel-level image frames), and even more difficult when the environment is stochastic. Therefore, a natural question is whether it is possible to plan without predicting future observations.\\n\\nIn fact, raw observations may contain information unnecessary for planning, such as dynamically changing backgrounds in visual observations that are irrelevant to their value/utility. The starting point of this work is the premise that what planning truly requires is the ability to predict the rewards and values of future states. An observation-prediction model relies on its predictions of observations to predict future rewards and values. What if we could predict future rewards and values directly without predicting future observations? Such a model could be more easily learnable for complex domains or more flexible for dealing with stochasticity. In this paper, we address the problem of learning and planning from a value-prediction model that can directly generate/predict the value/reward of future states without generating future observations.\\n\\nOur main contribution is a novel neural network architecture we call the Value Prediction Network (VPN). The VPN combines model-based RL (i.e., learning the dynamics of an abstract state space sufficient for computing future rewards and values) and model-free RL (i.e., mapping the learned abstract states to rewards and values) in a unified framework. In order to train a VPN, we propose a combination of temporal-difference search\\xa0 (TD search) and n-step Q-learning\\xa0. In brief, VPNs learn to predict values via Q-learning and rewards via supervised learning. At the same time, VPNs perform lookahead planning to choose actions and compute bootstrapped target Q-values.\\n\\nOur empirical results on a 2D navigation task demonstrate the advantage of VPN over model-free baselines (e.g., Deep Q-Network\\xa0). We also show that VPN is more robust to stochasticity in the environment than an observation-prediction model approach. Furthermore, we show that our VPN outperforms DQN on several Atari games\\xa0 even with short-lookahead planning, which suggests that our approach can be potentially useful for learning better abstract-state representations and reducing sample-complexity.\\n\\nRelated Work\\n\\nModel-based Reinforcement Learning.\\n\\nDyna-Q\\xa0 integrates model-free and model-based RL by learning an observation-prediction model and using it to generate samples for Q-learning in addition to the model-free samples obtained by acting in the real environment. Gu et al.\\xa0 extended these ideas to continuous control problems. Our work is similar to Dyna-Q in the sense that planning and learning are integrated into one architecture. However, VPNs perform a lookahead tree search to choose actions and compute bootstrapped targets, whereas Dyna-Q uses a learned model to generate imaginary samples. In addition, Dyna-Q learns a model of the environment separately from a value function approximator. In contrast, the dynamics model in VPN is combined with the value function approximator in a single neural network and indirectly learned from reward and value predictions through backpropagation.\\n\\nAnother line of work\\xa0 uses observation-prediction models not for planning, but for improving exploration. A key distinction from these prior works is that our method learns abstract-state dynamics not to predict future observations, but instead to predict future rewards/values. For continuous control problems, deep learning has been combined with model predictive control (MPC)\\xa0, a specific way of using an observation-prediction model. In cases where the observation-prediction model is differentiable with respect to continuous actions, backpropagation can be used to find the optimal action\\xa0 or to compute value gradients\\xa0. In contrast, our work focuses on learning and planning using lookahead for discrete control problems.\\n\\nOur VPNs are related to Value Iteration Networks\\xa0 (VINs) which perform value iteration (VI) by approximating the Bellman-update through a convolutional neural network (CNN). However, VINs perform VI over the entire state space, which in practice requires that 1) the state space is small and representable as a vector with each dimension corresponding to a separate state and 2) the states have a topology with local transition dynamics (e.g., 2D grid). VPNs do not have these limitations and are thus more generally applicable, as we will show empirically in this paper.\\n\\nVPN is close to and in-part inspired by Predictron\\xa0 in that a recurrent neural network (RNN) acts as a transition function over abstract states. VPN can be viewed as a grounded Predictron in that each rollout corresponds to the transition in the environment, whereas each rollout in Predictron is purely abstract. In addition, Predictrons are limited to uncontrolled settings and thus policy evaluation, whereas our VPNs can learn an optimal policy in controlled settings.\\n\\nModel-free Deep Reinforcement Learning.\\n\\nMnih et al.\\xa0 proposed the Deep Q-Network (DQN) architecture which learns to estimate Q-values using deep neural networks. A lot of variations of DQN have been proposed for learning better state representation\\xa0, including the use of memory-based networks for handling partial observability\\xa0, estimating both state-values and advantage-values as a decomposition of Q-values\\xa0, learning successor state representations\\xa0, and learning several auxiliary predictions in addition to the main RL values\\xa0. Our VPN can be viewed as a model-free architecture which 1) decomposes Q-value into reward, discount, and the value of the next state and 2) uses multi-step reward/value predictions as auxiliary tasks to learn a good representation. A key difference from the prior work listed above is that our VPN learns to simulate the future rewards/values which enables planning. Although STRAW\\xa0 can maintain a sequence of future actions using an external memory, it cannot explicitly perform planning by simulating future rewards/values.\\n\\nMonte-Carlo Planning.\\n\\nMonte-Carlo Tree Search (MCTS) methods\\xa0 have been used for complex search problems, such as the game of Go, where a simulator of the environment is already available and thus does not have to be learned. Most recently, AlphaGo\\xa0 introduced a value network that directly estimates the value of state in Go in order to better approximate the value of leaf-node states during tree search. Our VPN takes a similar approach by predicting the value of abstract future states during tree search using a value function approximator. Temporal-difference search\\xa0 (TD search) combined TD-learning with MCTS by computing target values for a value function approximator through MCTS. Our algorithm for training VPN can be viewed as an instance of TD search, but it learns the dynamics of future rewards/values instead of being given a simulator.\\n\\nValue Prediction Network\\n\\nThe value prediction network is developed for semi-Markov decision processes (SMDPs). Let x_(t) be the observation or a history of observations for partially observable MDPs (henceforth referred to as just observation) and let o_(t) be the option\\xa0 at time t. Each option maps observations to primitive actions, and the following Bellman equation holds for all policies π: $Q^{\\\\pi}(\\\\textbf{x}_t,\\\\textbf{o{}}_t) = \\\\mathbb{E}[ \\\\sum_{i=0}^{k-1}\\\\gamma^{i}r_{t+i} + \\\\gamma^{k}V^{\\\\pi}(\\\\textbf{x}_{t+k})]$, where γ is a discount factor, r_(t) is the immediate reward at time t, and k is the number of time steps taken by the option o_(t) before terminating in observation x_(t\\u2005+\\u2005k).\\n\\nA VPN not only learns an option-value function Q_(θ)(x_(t),o_(t)) through a neural network parameterized by θ like model-free RL, but also learns the dynamics of the rewards/values to perform planning. We describe the architecture of VPN in Section\\xa03.1. In Section\\xa03.2, we describe how to perform planning using VPN. Section\\xa03.3 describes how to train VPN in a Q-Learning-like framework\\xa0.\\n\\n[One-step rollout]\\n\\n[Multi-step rollout]\\n\\n[fig:arch]\\n\\nArchitecture\\n\\nThe VPN consists of the following modules parameterized by θ\\u2004=\\u2004{θ^(enc),\\u2006θ^(value),\\u2006θ^(out),\\u2006θ^(trans)}:\\n$$\\\\begin{aligned}\\n\\\\mbox{\\\\textbf{Encoding} } & f^{enc}_{\\\\theta}: \\\\textbf{x} \\\\mapsto \\\\textbf{s} \\n& \\n\\\\mbox{\\\\textbf{Value} } & f^{value}_{\\\\theta}: \\\\textbf{s} \\\\mapsto V_{\\\\theta}(\\\\textbf{s})  \\n\\\\\\\\\\n\\\\mbox{\\\\textbf{Outcome} } & f^{out}_{\\\\theta}: \\\\textbf{s},\\\\textbf{o{}} \\\\mapsto r,\\\\gamma \\n&\\n\\\\mbox{\\\\textbf{Transition} } & f^{trans}_{\\\\theta}: \\\\textbf{s},\\\\textbf{o{}} \\\\mapsto \\\\textbf{s}\\'\\\\end{aligned}$$\\n\\n-   Encoding module maps the observation (x) to the abstract state (s\\u2004∈\\u2004ℝ^(m)) using neural networks (e.g., CNN for visual observations). Thus, s is an abstract-state representation which will be learned by the network (and not an environment state or even an approximation to one).\\n\\n-   Value module estimates the value of the abstract-state (V_(θ)(s)). Note that the value module is not a function of the observation, but a function of the abstract-state.\\n\\n-   Outcome module predicts the option-reward (r\\u2004∈\\u2004ℝ) for executing the option o at abstract-state s. If the option takes k primitive actions before termination, the outcome module should predict the discounted sum of the k immediate rewards as a scalar. The outcome module also predicts the option-discount (γ\\u2004∈\\u2004ℝ) induced by the number of steps taken by the option.\\n\\n-   Transition module transforms the abstract-state to the next abstract-state (s′\\u2004∈\\u2004ℝ^(m)) in an option-conditional manner.\\n\\nFigure\\xa01 illustrates the core module which performs 1-step rollout by composing the above modules: f_(θ)^(core)\\u2004:\\u2004s,\\u2006o\\u2004↦\\u2004r,\\u2006γ,\\u2006V_(θ)(s′),\\u2006s′. The core module takes an abstract-state and option as input and makes separate option-conditional predictions of the option-reward (henceforth, reward), the option-discount (henceforth, discount), and the value of the abstract-state at option-termination. By combining the predictions, we can estimate the Q-value as follows: Q_(θ)(s,o)\\u2004=\\u2004r\\u2005+\\u2005γV_(θ)(s′). In addition, the VPN recursively applies the core module to predict the sequence of future abstract-states as well as rewards and discounts given an initial abstract-state and a sequence of options as illustrated in Figure\\xa02.\\n\\nPlanning\\n\\nVPN has the ability to simulate the future and plan based on the simulated future abstract-states. Although many existing planning methods (e.g., MCTS) can be applied to the VPN, we implement a simple planning method which performs rollouts using the VPN up to a certain depth (say d), henceforth denoted as planning depth, and aggregates all intermediate value estimates as described in Algorithm\\xa0[alg:planning] and Figure\\xa0[fig:planning]. More formally, given an abstract-state s\\u2004=\\u2004f_(θ)^(enc)(x) and an option o, the Q-value calculated from d-step planning is defined as:\\n$$\\\\begin{aligned}\\nQ^d_{\\\\theta}(\\\\textbf{s},\\\\textbf{o{}}) & = r+\\\\gamma V^d_\\\\theta(\\\\textbf{s}\\')\\n&\\nV^d_{\\\\theta}(\\\\textbf{s})=\\\\begin{cases}\\nV_{\\\\theta}(\\\\textbf{s}) & \\\\mbox{if } d=1 \\\\\\\\\\n\\\\frac{1}{d}V_{\\\\theta}(\\\\textbf{s})+\\\\frac{d-1}{d}\\\\max_{\\\\textbf{o{}}}Q^{d-1}_{\\\\theta}(\\\\textbf{s},\\\\textbf{o{}}) & \\\\mbox{if } d>1 ,\\n\\\\end{cases}\\n\\\\label{eq:value-d}\\\\end{aligned}$$\\nwhere s′\\u2004=\\u2004f_(θ)^(trans)(s,o),\\u2006V_(θ)(s)\\u2004=\\u2004f_(θ)^(value)(s), and r,\\u2006γ\\u2004=\\u2004f_(θ)^(out)(s,o). Our planning algorithm is divided into two steps: expansion and backup. At the expansion step (see Figure\\xa03), we recursively simulate options up to a depth of d by unrolling the core module. At the backup step, we compute the weighted average of the direct value estimate V_(θ)(s) and max_(o)Q_(θ)^(d\\u2005−\\u20051)(s,o) to compute V_(θ)^(d)(s) (i.e., value from d-step planning) in Equation\\xa0[eq:value-d]. Note that max_(o)Q_(θ)^(d\\u2005−\\u20051)(s,o) is the average over d\\u2005−\\u20051 possible value estimates. We propose to compute the uniform average over all possible returns by using weights proportional to 1 and d\\u2005−\\u20051 for V_(θ)(s) and max_(o)Q_(θ)^(d\\u2005−\\u20051)(s,o) respectively. Thus, V_(θ)^(d)(s) is the uniform average of d expected returns along the path of the best sequence of options as illustrated in Figure\\xa04.\\n\\nTo reduce the computational cost, we simulate only b-best options at each expansion step based on Q¹(s,o). We also find that choosing only the best option after a certain depth does not compromise the performance much, which is analogous to using a default policy in MCTS beyond a certain depth. This heuristic visits reasonably good abstract states during planning, though a more principled way such as UCT\\xa0 can also be used to balance exploration and exploitation. This planning method is used for choosing options and computing target Q-values during training, as described in the following section.\\n\\n[Expansion ]\\n\\n[Backup ]\\n\\nr,\\u2006γ,\\u2006V(s′),\\u2006s′\\u2004←\\u2004f_(θ)^(core)(s,o) r\\u2005+\\u2005γV(s′) 𝒜← b-best options based on Q¹(s′,o′) q_(o′)← $r + \\\\gamma \\\\left[ \\\\frac{1}{d} V(\\\\textbf{s}\\') + \\\\frac{d-1}{d} \\\\max_{\\\\textbf{o{}}\\'\\\\in\\\\mathcal{A}}q_{\\\\textbf{o{}}\\'} \\\\right]$\\n\\nLearning\\n\\nr0.46 [image]\\n\\nVPN can be trained through any existing value-based RL algorithm for the value predictions combined with supervised learning for reward and discount predictions. In this paper, we present a modification of n-step Q-learning\\xa0 and TD search\\xa0. The main idea is to generate trajectories by following ϵ-greedy policy based on the planning method described in Section\\xa03.2. Given an n-step trajectory x₁,\\u2006o₁,\\u2006r₁,\\u2006γ₁,\\u2006x₂,\\u2006o₂,\\u2006r₂,\\u2006γ₂,\\u2006...,\\u2006x_(n\\u2005+\\u20051) generated by the ϵ-greedy policy, k-step predictions are defined as follows:\\n$$\\\\begin{aligned}\\n\\\\textbf{s}^{k}_t & = \\\\begin{cases}\\nf^{enc}_\\\\theta(\\\\textbf{x}_t) & \\\\mbox{if }k=0 \\\\\\\\\\nf^{trans}_\\\\theta(\\\\textbf{s}^{k-1}_{t-1}, \\\\textbf{o{}}_{t-1}) & \\\\mbox{if } k>0 \\n\\\\end{cases} \\n&\\nv^{k}_t & = f^{value}_\\\\theta(\\\\textbf{s}^k_t)\\n&\\nr^{k}_t,\\\\gamma^{k}_t & = f^{out}_\\\\theta(\\\\textbf{s}^{k-1}_t, \\\\textbf{o{}}_{t}).\\\\end{aligned}$$\\nIntuitively, s_(t)^(k) is the VPN’s k-step prediction of the abstract-state at time t predicted from x_(t\\u2005−\\u2005k) following options o_(t\\u2005−\\u2005k),\\u2006...,\\u2006o_(t\\u2005−\\u20051) in the trajectory as illustrated in Figure\\xa0[fig:learning]. By applying the value and the outcome module, VPN can compute the k-step prediction of the value, the reward, and the discount. The k-step prediction loss at step t is defined as:\\n$$\\\\begin{aligned}\\n\\\\mathcal{L}_t =\\\\sum_{l=1}^{k}\\\\left(R_t-v^l_t\\\\right)^2+\\\\left(r_t-r^l_t\\\\right)^2+\\\\left(\\\\log_\\\\gamma\\\\gamma_t-\\\\log_\\\\gamma\\\\gamma^l_t\\\\right)^2\\n\\\\label{eq:loss}\\\\end{aligned}$$\\nwhere $R_t = \\\\begin{cases} \\nr_t + \\\\gamma_t R_{t+1} & \\\\mbox{if } t \\\\le n \\\\\\\\\\n\\\\max_\\\\textbf{o{}} Q^{d}_{\\\\theta^-}(\\\\textbf{s}_{n+1},\\\\textbf{o{}}) & \\\\mbox {if } t = n+1\\n\\\\end{cases}$ is the target value, and Q_(θ⁻)^(d)(s_(n\\u2005+\\u20051),o) is the Q-value computed by the d-step planning method described in\\xa03.2. Intuitively, ℒ_(t) accumulates losses over 1-step to k-step predictions of values, rewards, and discounts. We find that applying log_(γ) for the discount prediction loss helps optimization, which amounts to computing the squared loss with respect to the number of steps.\\n\\nOur learning algorithm introduces two hyperparameters: the number of prediction steps (k) and planning depth (d_(train)) used for choosing options and computing bootstrapped targets. We also make use of a target network parameterized by θ⁻ which is synchronized with θ after a certain number of steps to stabilize training as suggested by\\xa0. The loss is accumulated over n-steps and the parameter is updated by computing its gradient as follows: $\\\\nabla_{\\\\theta}\\\\mathcal{L} =\\\\sum_{t=1}^{n}\\\\nabla_{\\\\theta}\\\\mathcal{L}_t$. The full algorithm is described in the Appendix.\\n\\nRelationship to Existing Approaches\\n\\nVPN is model-based in the sense that it learns an abstract-state transition function sufficient to predict rewards/discount/values. Meanwhile, VPN can also be viewed as model-free in the sense that it learns to directly estimate the value of the abstract-state. From this perspective, VPN exploits several auxiliary prediction tasks, such as reward and discount predictions to learn a good abstract-state representation. An interesting property of VPN is that its planning ability is used to compute the bootstrapped target as well as choose options during Q-learning. Therefore, as VPN improves the quality of its future predictions, it can not only perform better during evaluation through its improved planning ability, but also generate more accurate target Q-values during training, which encourages faster convergence compared to conventional Q-learning.\\n\\nExperiments\\n\\nOur experiments investigated the following questions: 1) Does VPN outperform model-free baselines (e.g., DQN)? 2) What is the advantage of planning with a VPN over observation-based planning? 3) Is VPN useful for complex domains with high-dimensional sensory inputs, such as Atari games?\\n\\nExperimental Setting\\n\\nNetwork Architecture.\\n\\nA CNN was used as the encoding module of VPN, and the transition module consists of one option-conditional convolution layer which uses different weights depending on the option followed by a few more convolution layers. We used a residual connection\\xa0 from the previous abstract-state to the next abstract-state so that the transition module learns the change of the abstract-state. The outcome module is similar to the transition module except that it does not have a residual connection and two fully-connected layers are used to produce reward and discount. The value module consists of two fully-connected layers. The number of layers and hidden units vary depending on the domain. These details are described in the Appendix.\\n\\nImplementation Details.\\n\\nOur algorithm is based on asynchronous n-step Q-learning\\xa0 where n is 10 and 16 threads are used. The target network is synchronized after every 10K steps. We used the Adam optimizer\\xa0, and the best learning rate and its decay were chosen from {0.0001,0.0002,0.0005,0.001} and {0.98,0.95,0.9,0.8} respectively. The learning rate is multiplied by the decay every 1M steps. Our implementation is based on TensorFlow\\xa0.[1]\\n\\nVPN has four more hyperparameters: 1) the number of predictions steps (k) during training, 2) the plan depth (d_(train)) during training, 3) the plan depth (d_(test)) during evaluation, and 4) the branching factor (b) which indicates the number of options to be simulated for each expansion step during planning. We used k\\u2004=\\u2004d_(train)\\u2004=\\u2004d_(test) throughout the experiment unless otherwise stated. VPN(d) represents our model which learns to predict and simulate up to d-step futures during training and evaluation. The branching factor (b) was set to 4 until depth of 3 and set to 1 after depth of 3, which means that VPN simulates 4-best options up to depth of 3 and only the best option after that.\\n\\nBaselines.\\n\\nWe compared our approach to the following baselines.\\n\\n-   DQN: This baseline directly estimates Q-values as its output and is trained through asynchronous n-step Q-learning. Unlike the original DQN, however, our DQN baseline takes an option as additional input and applies an option-conditional convolution layer to the top of the last encoding convolution layer, which is very similar to our VPN architecture.[2]\\n\\n-   VPN(1): This is identical to our VPN with the same training procedure except that it performs only 1-step rollout to estimate Q-value as shown in Figure\\xa01. This can be viewed as a variation of DQN that predicts reward, discount, and the value of the next state as a decomposition of Q-value.\\n\\n-   OPN(d): We call this Observation Prediction Network (OPN), which is similar to VPN except that it directly predicts future observations. More specifically, we train two independent networks: a model network (f^(model)\\u2004:\\u2004x,\\u2006o\\u2004↦\\u2004r,\\u2006γ,\\u2006x′) which predicts reward, discount, and the next observation, and a value network (f^(value)\\u2004:\\u2004x\\u2004↦\\u2004V(x)) which estimates the value from the observation. The training scheme is similar to our algorithm except that a squared loss for observation prediction is used to train the model network. This baseline performs d-step planning like VPN(d).\\n\\nCollect Domain\\n\\nTask Description.\\n\\nWe defined a simple but challenging 2D navigation task where the agent should collect as many goals as possible within a time limit, as illustrated in Figure\\xa0[fig:collect]. In this task, the agent, goals, and walls are randomly placed for each episode. The agent has four options: move left/right/up/down to the first crossing branch or the end of the corridor in the chosen direction. The agent is given 20 steps for each episode and receives a positive reward (2.0) when it collects a goal by moving on top of it and a time-penalty (\\u2005−\\u20050.2) for each step. Although it is easy to learn a sub-optimal policy which collects nearby goals, finding the optimal trajectory in each episode requires careful planning because the optimal solution cannot be computed in polynomial time.\\n\\nAn observation is represented as a 3D tensor (ℝ^(3\\u2005×\\u200510\\u2005×\\u200510)) with binary values indicating the presence/absence of each object type. The time remaining is normalized to [0,1] and is concatenated to the 3rd convolution layer of the network as a channel.\\n\\nWe evaluated all architectures first in a deterministic environment and then investigated the robustness in a stochastic environment separately. In the stochastic environment, each goal moves by one block with probability of 0.3 for each step. In addition, each option can be repeated multiple times with probability of 0.3. This makes it difficult to predict and plan the future precisely.\\n\\n[Observation]\\n\\n[DQN’s trajectory]\\n\\n[VPN’s trajectory]\\n\\n[Plan with 20 steps]\\n\\n[Plan with 12 steps]\\n\\n[Deterministic]\\n\\n[Stochastic]\\n\\n[fig:curves]\\n\\nOverall Performance.\\n\\nThe result is summarized in Figure\\xa0[fig:curves]. To understand the quality of different policies, we implemented a greedy algorithm which always collects the nearest goal first and a shortest-path algorithm which finds the optimal solution through exhaustive search assuming that the environment is deterministic. Note that even a small gap in terms of reward can be qualitatively substantial as indicated by the small gap between greedy and shortest-path algorithms.\\n\\nThe results show that many architectures learned a better-than-greedy policy in the deterministic and stochastic environments except that OPN baselines perform poorly in the stochastic environment. In addition, the performance of VPN is improved as the plan depth increases, which implies that deeper predictions are reliable enough to provide more accurate value estimates of future states. As a result, VPN with 5-step planning represented by ‘VPN(5)’ performs best in both environments.\\n\\nComparison to Model-free Baselines.\\n\\nOur VPNs outperform DQN and VPN(1) baselines by a large margin as shown in Figure\\xa0[fig:curves]. Figure\\xa0[fig:collect] (b-c) shows an example of trajectories of DQN and VPN(5) given the same initial state. Although DQN’s behavior is reasonable, it ended up with collecting one less goal compared to VPN(5). We hypothesize that 6 convolution layers used by DQN and VPN(1) are not expressive enough to find the best route in each episode because finding an optimal path requires a combinatorial search in this task. On the other hand, VPN can perform such a combinatorial search to some extent by simulating future abstract-states, which has advantages over model-free approaches for dealing with tasks that require careful planning.\\n\\nComparison to Observation-based Planning.\\n\\nCompared to OPNs which perform planning based on predicted observations, VPNs perform slightly better or equally well in the deterministic environment. We observed that OPNs can predict future observations very accurately because observations in this task are simple and the environment is deterministic. Nevertheless, VPNs learn faster than OPNs in most cases. We conjecture that it takes additional training steps for OPNs to learn to predict future observations. In contrast, VPNs learn to predict only minimal but sufficient information for planning: reward, discount, and the value of future abstract-states, which may be the reason why VPNs learn faster than OPNs.\\n\\nIn the stochastic Collect domain, VPNs significantly outperform OPNs. We observed that OPNs tend to predict the average of possible future observations (𝔼_(x)[x]) because OPN is deterministic. Estimating values on such blurry predictions leads to estimating V_(θ)(𝔼_(x)[x]) which is different from the true expected value 𝔼_(x)[V(x)]. On the other hand, VPN is trained to approximate the true expected value because there is no explicit constraint or loss for the predicted abstract state. We hypothesize that this key distinction allows VPN to learn different modes of possible future states more flexibly in the abstract state space. This result suggests that a value-prediction model can be more beneficial than an observation-prediction model when the environment is stochastic and building an accurate observation-prediction model is difficult.\\n\\nr0.46\\n\\n  ---------- --------------- ------ ------ ------------ ------ ------\\n              Deterministic                 Stochastic         \\n                Original      FGs    MWs     Original    FGs    MWs\\n      Greedy      8.61        5.13   7.79      7.58      4.48   7.04\\n    Shortest      9.71        5.82   8.98      7.64      4.36   7.22\\n         DQN      8.66        4.57   7.08      7.85      4.11   6.72\\n      VPN(1)      8.94        4.92   7.64      7.84      4.27   7.15\\n      OPN(5)      9.30        5.45   8.36      7.55      4.09   6.79\\n      VPN(5)      9.29        5.43   8.31      8.11      4.45   7.46\\n  ---------- --------------- ------ ------ ------------ ------ ------\\n\\nGeneralization Performance.\\n\\nOne advantage of model-based RL approach is that it can generalize well to unseen environments as long as the dynamics of the environment remains similar. To see if our VPN has such a property, we evaluated all architectures on two types of previously unseen environments with either reduced number of goals (from 8 to 5) or increased number of walls. It turns out that our VPN is much more robust to the unseen environments compared to model-free baselines (DQN and VPN(1)), as shown in Table\\xa0[tab:generalization]. The model-free baselines perform worse than the greedy algorithm on unseen environments, whereas VPN still performs well. In addition, VPN generalizes as well as OPN which can learn a near-perfect model in the deterministic setting, and VPN significantly outperforms OPN in the stochastic setting. This suggests that VPN has a good generalization property like model-based RL methods and is robust to stochasticity.\\n\\nEffect of Planning Depth.\\n\\nr0.48 [image]\\n\\nTo further investigate the effect of planning depth in a VPN, we measured the average reward in the deterministic environment by varying the planning depth (d_(test)) from 1 to 10 during evaluation after training VPN with a fixed number of prediction steps and planning depth (k,\\u2006d_(train)), as shown in Figure\\xa0[fig:depth]. Since VPN does not learn to predict observations, there is no guarantee that it can perform deeper planning during evaluation (d_(test)) than the planning depth used during training (d_(train)). Interestingly, however, the result in Figure\\xa0[fig:depth] shows that if k\\u2004=\\u2004d_(train)\\u2004>\\u20042, VPN achieves better performance during evaluation through deeper tree search (d_(test)\\u2004>\\u2004d_(train)). We also tested a VPN with k\\u2004=\\u200410 and d_(train)\\u2004=\\u20045 and found that a planning depth of 10 achieved the best performance during evaluation. Thus, with a suitably large number of prediction steps during training, our VPN is able to benefit from deeper planning during evaluation relative to the planning depth during training. Figure\\xa0[fig:example] shows examples of good plans of length greater than 5 found by a VPN trained with planning depth 5. Another observation from Figure\\xa0[fig:depth] is that the performance of planning depth of 1 (d_(test)\\u2004=\\u20041) degrades as the planning depth during training (d_(train)) increases. This means that a VPN can improve its value estimations through long-term planning at the expense of the quality of short-term planning.\\n\\nAtari Games\\n\\nTo investigate how VPN deals with complex visual observations, we evaluated it on several Atari games\\xa0. Unlike in the Collect domain, in Atari games most primitive actions have only small value consequences and it is difficult to hand-design useful extended options. Nevertheless, we explored if VPNs are useful in Atari games even with short-lookahead planning using simple options that repeat the same primitive action over extended time periods by using a frame-skip of 10.[3] We pre-processed the game screen to 84\\u2005×\\u200584 gray-scale images. All architectures take last 4 frames as input. We doubled the number of hidden units of the fully-connected layer for DQN to approximately match the number of parameters. VPN learns to predict rewards and values but not discount (since it is fixed), and was trained to make 3-option-step predictions for planning which means that the agent predicts up to 0.5 seconds ahead in real-time.\\n\\n         Frostbite   Seaquest   Enduro   Alien   Q*Bert   Ms. Pacman   Amidar   Krull   Crazy Climber\\n  ----- ----------- ---------- -------- ------- -------- ------------ -------- ------- ---------------\\n    DQN    3058        2951      326     1804    12592       2804       535     12438       41658\\n    VPN    3811        5628      382     1429    14517       2689       641     15930       54119\\n\\n  : Performance on Atari games. Each number represents average score over 5 top agents.\\n\\n[Learning curves on Atari games. X-axis and y-axis correspond to steps and average reward over 100 episodes respectively. ]\\n\\n[State]\\n\\n[Plan 1 (19.3)]\\n\\n[Plan 2 (18.7)]\\n\\n[Plan 3 (18.4)]\\n\\n[Plan 4 (17.1)]\\n\\n[fig:atari-plan]\\n\\nAs summarized in Table\\xa01 and Figure\\xa07, our VPN outperforms DQN baseline on 7 out of 9 Atari games and learned significantly faster than DQN on Seaquest, QBert, Krull, and Crazy Climber. One possible reason why VPN outperforms DQN is that even 3-step planning is indeed helpful for learning a better policy. Figure\\xa0[fig:atari-plan] shows an example of VPN’s 3-step planning in Seaquest. Our VPN predicts reasonable values given different sequences of actions, which can potentially help choose a better action by looking at the short-term future. Another hypothesis is that the architecture of VPN itself, which has several auxiliary prediction tasks for multi-step future rewards and values, is useful for learning a good abstract-state representation as a model-free agent. Finally, our algorithm which performs planning to compute the target Q-value can potentially speed up learning by generating more accurate targets as it performs value backups multiple times from the simulated futures, as discussed in Section\\xa03.4. These results show that our approach is applicable to complex visual environments without needing to predict observations.\\n\\nConclusion\\n\\nWe introduced value prediction networks (VPNs) as a new deep RL way of integrating planning and learning while simultaneously learning the dynamics of abstract-states that make option-conditional predictions of future rewards/discount/values rather than future observations. Our empirical evaluations showed that VPNs outperform model-free DQN baselines in multiple domains, and outperform traditional observation-based planning in a stochastic domain. An interesting future direction would be to develop methods that automatically learn the options that allow good planning in VPNs.\\n\\nAcknowledgement\\n\\nThis work was supported by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor.\\n\\nComparison between VPN and DQN in the Deterministic Collect\\n\\nObservation DQN’s trajectory VPN’s trajectory VPN’s 10-step plan\\n\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n\\nComparison between VPN and OPN in the Stochastic Collect\\n\\n  ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- --\\n  [t = 1] [t = 1]     [t = 2] [t = 2]     [t = 3] [t = 3]     [t = 4] [t = 4]     [t = 5] [t = 5]     [t = 6] [t = 6]     [t = 7] [t = 7]     \\n                                                                                                                                              \\n  [t = 8] [t = 8]     [t = 9] [t = 9]     [t = 10] [t = 10]   [t = 11] [t = 11]   [t = 12] [t = 12]   [t = 13] [t = 13]   [t = 14] [t = 14]   \\n                                                                                                                                              \\n  [t = 15] [t = 15]   [t = 16] [t = 16]   [t = 17] [t = 17]   [t = 18] [t = 18]   [t = 19] [t = 19]   [t = 20] [t = 20]                       \\n  ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- --\\n\\nExamples of Planning on Atari Games\\n\\n[Examples of VPN’s planning on Atari games. The first column shows initial states, and the following columns show our VPN’s value estimates in parentheses given different sequences of actions. Red and black arrows represent movement actions with and without ‘Fire’. ‘N’ and ‘F’ correspond to ‘No-operation’ and ‘Fire’. (Seaquest) The VPN estimates higher values for moving up to refill the oxygen tank and lower values for moving down to kill enemies because the agent loses a life when the oxygen tank is empty, which is almost running out. (Ms. Pacman) The VPN estimates the lowest value for moving towards an enemy (ghost). It also estimates a low value for moving right because it already has eaten some yellow pellets on the right side. On the other hand, it estimates relatively higher values for moving left and down because it collects more pellets while avoiding the enemy. (Frostbite) The VPN estimates higher values for collecting a nearby fish, which gives a positive reward, and lower values for not collecting it. (Enduro) The VPN estimates higher values for accelerating (fire) and avoiding collision and lower values for colliding with other cars. ]\\n\\nDetails of Learning\\n\\nAlgorithm\\xa0[alg] describes our algorithm for training value prediction network (VPN). We observed that training the outcome module (reward and discount prediction) on additional data collected from a random policy slightly improves the performance because it reduces a bias towards the agent’s behavior. More specifically, we fill a replay memory with R transitions from a random policy before training and sample transitions from the replay memory to train the outcome module. This procedure is described in Line 4 and Lines 20-24 in Algorithm\\xa0[alg]. This method was used only for Collect domain (not for Atari) in our experiment by generating 1M transitions from a random policy.\\n\\nθ: global parameter, θ⁻: global target network parameter, T: global step counter d: plan depth, k: number of prediction steps t\\u2004←\\u20040 and T\\u2004←\\u20040 M[1...R]← Store R transitions (s,o,r,γ,s′) using a random policy Clear gradients dθ\\u2004←\\u20040 Synchronize thread-specific parameter θ′\\u2004←\\u2004θ t_(start)\\u2004←\\u2004t s_(t)← Observe state a_(t)← argmax_(o)Q_(θ′)^(d)(s_(t),o_(t)) or random option based on ϵ-greedy policy r_(t),\\u2006γ_(t),\\u2006s_(t\\u2005+\\u20051)← Execute o_(t) t\\u2004←\\u2004t\\u2005+\\u20051 and T\\u2004←\\u2004T\\u2005+\\u20051 $R=\\\\begin{cases}\\n0 & \\\\mbox{if } s_t \\\\mbox{ is terminal}\\\\\\\\\\n\\\\max_o Q^{d}_{\\\\theta^-}(s_t,o) & \\\\mbox{if } s_t \\\\mbox{ is non-terminal}\\n\\\\end{cases}$ R\\u2004←\\u2004r_(i)\\u2005+\\u2005γ_(i)R $d\\\\theta \\\\gets d\\\\theta + \\\\nabla_{\\\\theta\\'}\\\\left[\\\\sum_{l=1}^{k}\\\\left(R-v^{l}_i\\\\right)^2 + \\\\left(r_i-r^{l}_i\\\\right)^2 + \\\\left(\\\\log_\\\\gamma \\\\gamma_i- \\\\log_\\\\gamma\\\\gamma^{l}_i\\\\right)^2 \\\\right]$ t′← Sample an index from 1,\\u20062,\\u2006...,\\u2006R s_(i),\\u2006a_(i),\\u2006r_(i),\\u2006γ_(i),\\u2006s_(i\\u2005+\\u20051)← Retrieve a transition from M[i] $d\\\\theta \\\\gets d\\\\theta + \\\\nabla_{\\\\theta\\'}\\\\left[\\\\sum_{l=1}^{k} \\\\left(r_i-r^{l}_i\\\\right)^2 + \\\\left(\\\\log_\\\\gamma \\\\gamma_i- \\\\log_\\\\gamma\\\\gamma^{l}_i\\\\right)^2 \\\\right]$ Perform asynchronous update of θ using dθ Update the target network θ⁻\\u2004←\\u2004θ\\n\\nDetails of Hyperparameters\\n\\n[Transition module used for Collect domain. The first convolution layer uses different weights depending on the given option. Sigmoid activation function is used for the last 1x1 convolution such that its output forms a mask. This mask is multiplied to the output from the 3rd convolution layer. Note that there is a residual connection from s to s′. Thus, the transition module learns the change of the consecutive abstract states. ]\\n\\nCollect\\n\\nThe encoding module of our VPN consists of Conv(32-3x3-1)-Conv(32-3x3-1)-Conv(64-4x4-2) where Conv(N-KxK-S) represents N filters with size of KxK with a stride of S. The transition module is illustrated in Figure\\xa029. It consists of OptionConv(64-3x3-1)-Conv(64-3x3-1)-Conv(64-3x3-1) and a separate Conv(64-1x1-1) for the mask which is multiplied to the output of the 3rd convolution layer of the transition module. ‘OptionConv’ uses different convolution weights depending on the given option. We also used a residual connection from the previous abstract state to the next abstract state such that the transition module learns the difference between two states. The outcome module has OptionConv(64-3x3-1)-Conv(64-3x3-1)-FC(64)-FC(2) where FC(N) represents a fully-connected layer with N hidden units. The value module consists of FC(64)-FC(1). Exponential linear unit (ELU)\\xa0 was used as an activation function for all architectures.\\n\\nOur DQN baseline consists of the encoding module followed by the transition module followed by the value module. Thus, the overall architecture is very similar to VPN except that it does not have the outcome module. To match the number of parameters, we used 256 hidden units for DQN’s value module. We found that this architecture outperforms the original DQN architecture\\xa0 on Collect domain and several Atari games.\\n\\nThe model network of OPN baseline has the same architecture as VPN except that it has an additional decoding module which consists of Deconv(64-4x4-2)-Deconv(32-3x3-1)-Deconv(32-3x3-1). This module is applied to the predicted abstract-state so that it can predict the future observations. The value network of OPN has the same architecture as our DQN baseline.\\n\\nA discount factor of 0.98 was used, and the target network was synchronized after every 10K steps. The epsilon for ϵ-greedy policy was linearly decreased from 1 to 0.05 for the first 1M steps.\\n\\nAtari Games\\n\\nThe encoding module consists of Conv(16-8x8-4)-Conv(32-4x4-2), and the transition module has OptionConv(32-3x3-1)-Conv(32-3x3-1) with a mask and a residual connection as described above. The outcome module has OptionConv(32-3x3-1)-Conv(32-3x3-1)-FC(128)-FC(1), and the value module consists of FC(128)-FC(1). The DQN baseline has the same encoding module followed by the transition module and the value module, and we used 256 hidden units for the value module of DQN to approximately match the number of parameters. The other hyperparameters are same as the ones used in the Collect domain except that a discount factor of 0.99 was used.\\n\\n[1] The code is available on https://github.com/junhyukoh/value-prediction-network.\\n\\n[2] This architecture outperformed the original DQN architecture in our preliminary experiments.\\n\\n[3] Much of the previous work on Atari games has used a frame-skip of 4. Though using a larger frame-skip generally makes training easier, it may make training harder in some games if they require more fine-grained control\\xa0.\\n\\n\\nPlease answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, and metric?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df_train_f1_all_templates.at[10, 'prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'Stanford Cars 5-way (5-shot)', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'Stanford Dogs 5-way (1-shot)', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'CUB 200 5-way 1-shot', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'Stanford Cars 5-way (1-shot)', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'CUB 200 5-way 5-shot', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'Mini-Imagenet 5-way (1-shot)', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'Stanford Dogs 5-way (5-shot)', 'Metric': 'Accuracy'}}, {'LEADERBOARD': {'Task': 'Few-Shot Image Classification', 'Dataset': 'Mini-Imagenet 5-way (5-shot)', 'Metric': 'Accuracy'}}]\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(df_train_f1_all_templates.at[100, 'answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_f1_all_templates.to_parquet('../data/df_train_tdm_long_f1_all_templates.parquet')\n",
    "df_dev_f1_all_templates.to_parquet('../data/df_dev_tdm_long_f1_all_templates.parquet')\n",
    "df_train_f2_all_templates.to_parquet('../data/df_train_tdm_long_f2_all_templates.parquet')\n",
    "df_dev_f2_all_templates.to_parquet('../data/df_dev_tdm_long_f2_all_templates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_f1_all_templates = pd.read_parquet('../data/df_train_f1_all_templates.parquet')\n",
    "# df_dev_f1_all_templates = pd.read_parquet('../data/df_dev_f1_all_templates.parquet')\n",
    "# df_train_f2_all_templates = pd.read_parquet('../data/df_train_f2_all_templates.parquet')\n",
    "# df_dev_f2_all_templates = pd.read_parquet('../data/df_dev_f2_all_templates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589c70794b754927a85c894034d98eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f361330203444e6b5695183567c4926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf64ec8884f454d83831aff481a2e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99371edbdb1400d87ca83b067489857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776cd1b7ca3c41cd93fc3c0b689fc52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cdfc533bed420eb6158197e1915e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48a7f5f6a8448bfbaa2946fe7b6819e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a037055ba742c49984780b1eae1d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be004325be04ba78a61b5e39ce456fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55916ad517014f9a9acd1701ae13617c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e8434dcbec4927b9a875495d091892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6abccbf5b449e58828e8e4f28d5306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    fold1: DatasetDict({\n",
      "        train: Dataset({\n",
      "            features: ['prompt', 'answer', '__index_level_0__'],\n",
      "            num_rows: 82680\n",
      "        })\n",
      "        validation: Dataset({\n",
      "            features: ['prompt', 'answer', '__index_level_0__'],\n",
      "            num_rows: 35295\n",
      "        })\n",
      "    })\n",
      "    fold2: DatasetDict({\n",
      "        train: Dataset({\n",
      "            features: ['prompt', 'answer', '__index_level_0__'],\n",
      "            num_rows: 82695\n",
      "        })\n",
      "        validation: Dataset({\n",
      "            features: ['prompt', 'answer', '__index_level_0__'],\n",
      "            num_rows: 35280\n",
      "        })\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# dataset_train_f1_all_templates = Dataset.from_pandas(df_train_f1_all_templates)\n",
    "# dataset_dev_f1_all_templates = Dataset.from_pandas(df_dev_f1_all_templates)\n",
    "# dataset_train_f2_all_templates = Dataset.from_pandas(df_train_f2_all_templates)\n",
    "# dataset_dev_f2_all_templates = Dataset.from_pandas(df_dev_f2_all_templates)\n",
    "\n",
    "\n",
    "\n",
    "# f1 = DatasetDict({\n",
    "#         \"train\": dataset_train_f1_all_templates,\n",
    "#         \"validation\": dataset_dev_f1_all_templates\n",
    "#     })\n",
    "# f2 = DatasetDict({\n",
    "#         \"train\": dataset_train_f2_all_templates,\n",
    "#         \"validation\": dataset_dev_f2_all_templates\n",
    "#     })\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "# dataset = DatasetDict({\n",
    "#     'fold1': DatasetDict({\n",
    "#         \"train\": dataset_train_f1_all_templates,\n",
    "#         \"validation\": dataset_dev_f1_all_templates\n",
    "#     }),\n",
    "#     'fold2': DatasetDict({\n",
    "#         \"train\": dataset_train_f2_all_templates,\n",
    "#         \"validation\": dataset_dev_f2_all_templates\n",
    "#     })\n",
    "# })\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'fold1': DatasetDict({\n",
    "        \"train\": Dataset.from_parquet('../data/df_train_tdm_long_f1_all_templates.parquet'),\n",
    "        \"validation\": Dataset.from_parquet('../data/df_dev_tdm_long_f1_all_templates.parquet')\n",
    "    }),\n",
    "    'fold2': DatasetDict({\n",
    "        \"train\": Dataset.from_parquet('../data/df_train_tdm_long_f2_all_templates.parquet'),\n",
    "        \"validation\": Dataset.from_parquet('../data/df_dev_tdm_long_f2_all_templates.parquet')\n",
    "    })\n",
    "})\n",
    "\n",
    "# dataset_fold1 = DatasetDict({\n",
    "#     \"train\": Dataset.from_parquet('../data/df_train_f1_all_templates.parquet'),\n",
    "#     \"validation\": Dataset.from_parquet('../data/df_dev_f1_all_templates.parquet')\n",
    "#     })\n",
    "\n",
    "# dataset_fold2 =  DatasetDict({\n",
    "#     \"train\": Dataset.from_parquet('../data/df_train_f2_all_templates.parquet'),\n",
    "#     \"validation\": Dataset.from_parquet('../data/df_dev_f2_all_templates.parquet')\n",
    "#     })\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     'fold1': f1,\n",
    "#     'fold2': f2\n",
    "# })\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": dataset_train_f1_all_templates,\n",
    "#     \"validation\": dataset_dev_f1_all_templates\n",
    "#     })\n",
    "\n",
    "print(dataset)\n",
    "# print(dataset_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f5a6ccf52e44dfbfa641a269163a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/82680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16274efdc9ec41258e048009505c844e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/35295 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1f1f24585e456fa2feafa362f7fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/82695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b267301227a47d29155615c60a2d9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/35280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"../data/LLLM_LONG_TDM_ALL_TEMPLATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"../data/LLLM_LONG_TDM_ALL_TEMPLATE\"\n",
    "\n",
    "# reloaded_encoded_dataset = datasets.load_from_disk(\"../data/dataset/LLLM_TDMS_ALL_TEMPLATE\")\n",
    "# reloaded_encoded_dataset = DatasetDict.load_from_disk(\"../data/LLLM_TDMS_ALL_TEMPLATE\")\n",
    "\n",
    "dataset_fold1 = DatasetDict.load_from_disk(f\"{root_directory}/fold1\")\n",
    "dataset_fold2 = DatasetDict.load_from_disk(f\"{root_directory}/fold2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Title\\tValue Prediction Network\\n\\nAbstract:\\tThis paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation.\\n\\nIntroduction\\n\\nModel-based reinforcement learning (RL) approaches attempt to learn a model that predicts future observations conditioned on actions and can thus be used to simulate the real environment and do multi-step lookaheads for planning. We will call such models an observation-prediction model to distinguish it from another form of model introduced in this paper. Building an accurate observation-prediction model is often very challenging when the observation space is large\\xa0 (e.g., high-dimensional pixel-level image frames), and even more difficult when the environment is stochastic. Therefore, a natural question is whether it is possible to plan without predicting future observations.\\n\\nIn fact, raw observations may contain information unnecessary for planning, such as dynamically changing backgrounds in visual observations that are irrelevant to their value/utility. The starting point of this work is the premise that what planning truly requires is the ability to predict the rewards and values of future states. An observation-prediction model relies on its predictions of observations to predict future rewards and values. What if we could predict future rewards and values directly without predicting future observations? Such a model could be more easily learnable for complex domains or more flexible for dealing with stochasticity. In this paper, we address the problem of learning and planning from a value-prediction model that can directly generate/predict the value/reward of future states without generating future observations.\\n\\nOur main contribution is a novel neural network architecture we call the Value Prediction Network (VPN). The VPN combines model-based RL (i.e., learning the dynamics of an abstract state space sufficient for computing future rewards and values) and model-free RL (i.e., mapping the learned abstract states to rewards and values) in a unified framework. In order to train a VPN, we propose a combination of temporal-difference search\\xa0 (TD search) and n-step Q-learning\\xa0. In brief, VPNs learn to predict values via Q-learning and rewards via supervised learning. At the same time, VPNs perform lookahead planning to choose actions and compute bootstrapped target Q-values.\\n\\nOur empirical results on a 2D navigation task demonstrate the advantage of VPN over model-free baselines (e.g., Deep Q-Network\\xa0). We also show that VPN is more robust to stochasticity in the environment than an observation-prediction model approach. Furthermore, we show that our VPN outperforms DQN on several Atari games\\xa0 even with short-lookahead planning, which suggests that our approach can be potentially useful for learning better abstract-state representations and reducing sample-complexity.\\n\\nRelated Work\\n\\nModel-based Reinforcement Learning.\\n\\nDyna-Q\\xa0 integrates model-free and model-based RL by learning an observation-prediction model and using it to generate samples for Q-learning in addition to the model-free samples obtained by acting in the real environment. Gu et al.\\xa0 extended these ideas to continuous control problems. Our work is similar to Dyna-Q in the sense that planning and learning are integrated into one architecture. However, VPNs perform a lookahead tree search to choose actions and compute bootstrapped targets, whereas Dyna-Q uses a learned model to generate imaginary samples. In addition, Dyna-Q learns a model of the environment separately from a value function approximator. In contrast, the dynamics model in VPN is combined with the value function approximator in a single neural network and indirectly learned from reward and value predictions through backpropagation.\\n\\nAnother line of work\\xa0 uses observation-prediction models not for planning, but for improving exploration. A key distinction from these prior works is that our method learns abstract-state dynamics not to predict future observations, but instead to predict future rewards/values. For continuous control problems, deep learning has been combined with model predictive control (MPC)\\xa0, a specific way of using an observation-prediction model. In cases where the observation-prediction model is differentiable with respect to continuous actions, backpropagation can be used to find the optimal action\\xa0 or to compute value gradients\\xa0. In contrast, our work focuses on learning and planning using lookahead for discrete control problems.\\n\\nOur VPNs are related to Value Iteration Networks\\xa0 (VINs) which perform value iteration (VI) by approximating the Bellman-update through a convolutional neural network (CNN). However, VINs perform VI over the entire state space, which in practice requires that 1) the state space is small and representable as a vector with each dimension corresponding to a separate state and 2) the states have a topology with local transition dynamics (e.g., 2D grid). VPNs do not have these limitations and are thus more generally applicable, as we will show empirically in this paper.\\n\\nVPN is close to and in-part inspired by Predictron\\xa0 in that a recurrent neural network (RNN) acts as a transition function over abstract states. VPN can be viewed as a grounded Predictron in that each rollout corresponds to the transition in the environment, whereas each rollout in Predictron is purely abstract. In addition, Predictrons are limited to uncontrolled settings and thus policy evaluation, whereas our VPNs can learn an optimal policy in controlled settings.\\n\\nModel-free Deep Reinforcement Learning.\\n\\nMnih et al.\\xa0 proposed the Deep Q-Network (DQN) architecture which learns to estimate Q-values using deep neural networks. A lot of variations of DQN have been proposed for learning better state representation\\xa0, including the use of memory-based networks for handling partial observability\\xa0, estimating both state-values and advantage-values as a decomposition of Q-values\\xa0, learning successor state representations\\xa0, and learning several auxiliary predictions in addition to the main RL values\\xa0. Our VPN can be viewed as a model-free architecture which 1) decomposes Q-value into reward, discount, and the value of the next state and 2) uses multi-step reward/value predictions as auxiliary tasks to learn a good representation. A key difference from the prior work listed above is that our VPN learns to simulate the future rewards/values which enables planning. Although STRAW\\xa0 can maintain a sequence of future actions using an external memory, it cannot explicitly perform planning by simulating future rewards/values.\\n\\nMonte-Carlo Planning.\\n\\nMonte-Carlo Tree Search (MCTS) methods\\xa0 have been used for complex search problems, such as the game of Go, where a simulator of the environment is already available and thus does not have to be learned. Most recently, AlphaGo\\xa0 introduced a value network that directly estimates the value of state in Go in order to better approximate the value of leaf-node states during tree search. Our VPN takes a similar approach by predicting the value of abstract future states during tree search using a value function approximator. Temporal-difference search\\xa0 (TD search) combined TD-learning with MCTS by computing target values for a value function approximator through MCTS. Our algorithm for training VPN can be viewed as an instance of TD search, but it learns the dynamics of future rewards/values instead of being given a simulator.\\n\\nValue Prediction Network\\n\\nThe value prediction network is developed for semi-Markov decision processes (SMDPs). Let x_(t) be the observation or a history of observations for partially observable MDPs (henceforth referred to as just observation) and let o_(t) be the option\\xa0 at time t. Each option maps observations to primitive actions, and the following Bellman equation holds for all policies π: $Q^{\\\\pi}(\\\\textbf{x}_t,\\\\textbf{o{}}_t) = \\\\mathbb{E}[ \\\\sum_{i=0}^{k-1}\\\\gamma^{i}r_{t+i} + \\\\gamma^{k}V^{\\\\pi}(\\\\textbf{x}_{t+k})]$, where γ is a discount factor, r_(t) is the immediate reward at time t, and k is the number of time steps taken by the option o_(t) before terminating in observation x_(t\\u2005+\\u2005k).\\n\\nA VPN not only learns an option-value function Q_(θ)(x_(t),o_(t)) through a neural network parameterized by θ like model-free RL, but also learns the dynamics of the rewards/values to perform planning. We describe the architecture of VPN in Section\\xa03.1. In Section\\xa03.2, we describe how to perform planning using VPN. Section\\xa03.3 describes how to train VPN in a Q-Learning-like framework\\xa0.\\n\\n[One-step rollout]\\n\\n[Multi-step rollout]\\n\\n[fig:arch]\\n\\nArchitecture\\n\\nThe VPN consists of the following modules parameterized by θ\\u2004=\\u2004{θ^(enc),\\u2006θ^(value),\\u2006θ^(out),\\u2006θ^(trans)}:\\n$$\\\\begin{aligned}\\n\\\\mbox{\\\\textbf{Encoding} } & f^{enc}_{\\\\theta}: \\\\textbf{x} \\\\mapsto \\\\textbf{s} \\n& \\n\\\\mbox{\\\\textbf{Value} } & f^{value}_{\\\\theta}: \\\\textbf{s} \\\\mapsto V_{\\\\theta}(\\\\textbf{s})  \\n\\\\\\\\\\n\\\\mbox{\\\\textbf{Outcome} } & f^{out}_{\\\\theta}: \\\\textbf{s},\\\\textbf{o{}} \\\\mapsto r,\\\\gamma \\n&\\n\\\\mbox{\\\\textbf{Transition} } & f^{trans}_{\\\\theta}: \\\\textbf{s},\\\\textbf{o{}} \\\\mapsto \\\\textbf{s}\\'\\\\end{aligned}$$\\n\\n-   Encoding module maps the observation (x) to the abstract state (s\\u2004∈\\u2004ℝ^(m)) using neural networks (e.g., CNN for visual observations). Thus, s is an abstract-state representation which will be learned by the network (and not an environment state or even an approximation to one).\\n\\n-   Value module estimates the value of the abstract-state (V_(θ)(s)). Note that the value module is not a function of the observation, but a function of the abstract-state.\\n\\n-   Outcome module predicts the option-reward (r\\u2004∈\\u2004ℝ) for executing the option o at abstract-state s. If the option takes k primitive actions before termination, the outcome module should predict the discounted sum of the k immediate rewards as a scalar. The outcome module also predicts the option-discount (γ\\u2004∈\\u2004ℝ) induced by the number of steps taken by the option.\\n\\n-   Transition module transforms the abstract-state to the next abstract-state (s′\\u2004∈\\u2004ℝ^(m)) in an option-conditional manner.\\n\\nFigure\\xa01 illustrates the core module which performs 1-step rollout by composing the above modules: f_(θ)^(core)\\u2004:\\u2004s,\\u2006o\\u2004↦\\u2004r,\\u2006γ,\\u2006V_(θ)(s′),\\u2006s′. The core module takes an abstract-state and option as input and makes separate option-conditional predictions of the option-reward (henceforth, reward), the option-discount (henceforth, discount), and the value of the abstract-state at option-termination. By combining the predictions, we can estimate the Q-value as follows: Q_(θ)(s,o)\\u2004=\\u2004r\\u2005+\\u2005γV_(θ)(s′). In addition, the VPN recursively applies the core module to predict the sequence of future abstract-states as well as rewards and discounts given an initial abstract-state and a sequence of options as illustrated in Figure\\xa02.\\n\\nPlanning\\n\\nVPN has the ability to simulate the future and plan based on the simulated future abstract-states. Although many existing planning methods (e.g., MCTS) can be applied to the VPN, we implement a simple planning method which performs rollouts using the VPN up to a certain depth (say d), henceforth denoted as planning depth, and aggregates all intermediate value estimates as described in Algorithm\\xa0[alg:planning] and Figure\\xa0[fig:planning]. More formally, given an abstract-state s\\u2004=\\u2004f_(θ)^(enc)(x) and an option o, the Q-value calculated from d-step planning is defined as:\\n$$\\\\begin{aligned}\\nQ^d_{\\\\theta}(\\\\textbf{s},\\\\textbf{o{}}) & = r+\\\\gamma V^d_\\\\theta(\\\\textbf{s}\\')\\n&\\nV^d_{\\\\theta}(\\\\textbf{s})=\\\\begin{cases}\\nV_{\\\\theta}(\\\\textbf{s}) & \\\\mbox{if } d=1 \\\\\\\\\\n\\\\frac{1}{d}V_{\\\\theta}(\\\\textbf{s})+\\\\frac{d-1}{d}\\\\max_{\\\\textbf{o{}}}Q^{d-1}_{\\\\theta}(\\\\textbf{s},\\\\textbf{o{}}) & \\\\mbox{if } d>1 ,\\n\\\\end{cases}\\n\\\\label{eq:value-d}\\\\end{aligned}$$\\nwhere s′\\u2004=\\u2004f_(θ)^(trans)(s,o),\\u2006V_(θ)(s)\\u2004=\\u2004f_(θ)^(value)(s), and r,\\u2006γ\\u2004=\\u2004f_(θ)^(out)(s,o). Our planning algorithm is divided into two steps: expansion and backup. At the expansion step (see Figure\\xa03), we recursively simulate options up to a depth of d by unrolling the core module. At the backup step, we compute the weighted average of the direct value estimate V_(θ)(s) and max_(o)Q_(θ)^(d\\u2005−\\u20051)(s,o) to compute V_(θ)^(d)(s) (i.e., value from d-step planning) in Equation\\xa0[eq:value-d]. Note that max_(o)Q_(θ)^(d\\u2005−\\u20051)(s,o) is the average over d\\u2005−\\u20051 possible value estimates. We propose to compute the uniform average over all possible returns by using weights proportional to 1 and d\\u2005−\\u20051 for V_(θ)(s) and max_(o)Q_(θ)^(d\\u2005−\\u20051)(s,o) respectively. Thus, V_(θ)^(d)(s) is the uniform average of d expected returns along the path of the best sequence of options as illustrated in Figure\\xa04.\\n\\nTo reduce the computational cost, we simulate only b-best options at each expansion step based on Q¹(s,o). We also find that choosing only the best option after a certain depth does not compromise the performance much, which is analogous to using a default policy in MCTS beyond a certain depth. This heuristic visits reasonably good abstract states during planning, though a more principled way such as UCT\\xa0 can also be used to balance exploration and exploitation. This planning method is used for choosing options and computing target Q-values during training, as described in the following section.\\n\\n[Expansion ]\\n\\n[Backup ]\\n\\nr,\\u2006γ,\\u2006V(s′),\\u2006s′\\u2004←\\u2004f_(θ)^(core)(s,o) r\\u2005+\\u2005γV(s′) 𝒜← b-best options based on Q¹(s′,o′) q_(o′)← $r + \\\\gamma \\\\left[ \\\\frac{1}{d} V(\\\\textbf{s}\\') + \\\\frac{d-1}{d} \\\\max_{\\\\textbf{o{}}\\'\\\\in\\\\mathcal{A}}q_{\\\\textbf{o{}}\\'} \\\\right]$\\n\\nLearning\\n\\nr0.46 [image]\\n\\nVPN can be trained through any existing value-based RL algorithm for the value predictions combined with supervised learning for reward and discount predictions. In this paper, we present a modification of n-step Q-learning\\xa0 and TD search\\xa0. The main idea is to generate trajectories by following ϵ-greedy policy based on the planning method described in Section\\xa03.2. Given an n-step trajectory x₁,\\u2006o₁,\\u2006r₁,\\u2006γ₁,\\u2006x₂,\\u2006o₂,\\u2006r₂,\\u2006γ₂,\\u2006...,\\u2006x_(n\\u2005+\\u20051) generated by the ϵ-greedy policy, k-step predictions are defined as follows:\\n$$\\\\begin{aligned}\\n\\\\textbf{s}^{k}_t & = \\\\begin{cases}\\nf^{enc}_\\\\theta(\\\\textbf{x}_t) & \\\\mbox{if }k=0 \\\\\\\\\\nf^{trans}_\\\\theta(\\\\textbf{s}^{k-1}_{t-1}, \\\\textbf{o{}}_{t-1}) & \\\\mbox{if } k>0 \\n\\\\end{cases} \\n&\\nv^{k}_t & = f^{value}_\\\\theta(\\\\textbf{s}^k_t)\\n&\\nr^{k}_t,\\\\gamma^{k}_t & = f^{out}_\\\\theta(\\\\textbf{s}^{k-1}_t, \\\\textbf{o{}}_{t}).\\\\end{aligned}$$\\nIntuitively, s_(t)^(k) is the VPN’s k-step prediction of the abstract-state at time t predicted from x_(t\\u2005−\\u2005k) following options o_(t\\u2005−\\u2005k),\\u2006...,\\u2006o_(t\\u2005−\\u20051) in the trajectory as illustrated in Figure\\xa0[fig:learning]. By applying the value and the outcome module, VPN can compute the k-step prediction of the value, the reward, and the discount. The k-step prediction loss at step t is defined as:\\n$$\\\\begin{aligned}\\n\\\\mathcal{L}_t =\\\\sum_{l=1}^{k}\\\\left(R_t-v^l_t\\\\right)^2+\\\\left(r_t-r^l_t\\\\right)^2+\\\\left(\\\\log_\\\\gamma\\\\gamma_t-\\\\log_\\\\gamma\\\\gamma^l_t\\\\right)^2\\n\\\\label{eq:loss}\\\\end{aligned}$$\\nwhere $R_t = \\\\begin{cases} \\nr_t + \\\\gamma_t R_{t+1} & \\\\mbox{if } t \\\\le n \\\\\\\\\\n\\\\max_\\\\textbf{o{}} Q^{d}_{\\\\theta^-}(\\\\textbf{s}_{n+1},\\\\textbf{o{}}) & \\\\mbox {if } t = n+1\\n\\\\end{cases}$ is the target value, and Q_(θ⁻)^(d)(s_(n\\u2005+\\u20051),o) is the Q-value computed by the d-step planning method described in\\xa03.2. Intuitively, ℒ_(t) accumulates losses over 1-step to k-step predictions of values, rewards, and discounts. We find that applying log_(γ) for the discount prediction loss helps optimization, which amounts to computing the squared loss with respect to the number of steps.\\n\\nOur learning algorithm introduces two hyperparameters: the number of prediction steps (k) and planning depth (d_(train)) used for choosing options and computing bootstrapped targets. We also make use of a target network parameterized by θ⁻ which is synchronized with θ after a certain number of steps to stabilize training as suggested by\\xa0. The loss is accumulated over n-steps and the parameter is updated by computing its gradient as follows: $\\\\nabla_{\\\\theta}\\\\mathcal{L} =\\\\sum_{t=1}^{n}\\\\nabla_{\\\\theta}\\\\mathcal{L}_t$. The full algorithm is described in the Appendix.\\n\\nRelationship to Existing Approaches\\n\\nVPN is model-based in the sense that it learns an abstract-state transition function sufficient to predict rewards/discount/values. Meanwhile, VPN can also be viewed as model-free in the sense that it learns to directly estimate the value of the abstract-state. From this perspective, VPN exploits several auxiliary prediction tasks, such as reward and discount predictions to learn a good abstract-state representation. An interesting property of VPN is that its planning ability is used to compute the bootstrapped target as well as choose options during Q-learning. Therefore, as VPN improves the quality of its future predictions, it can not only perform better during evaluation through its improved planning ability, but also generate more accurate target Q-values during training, which encourages faster convergence compared to conventional Q-learning.\\n\\nExperiments\\n\\nOur experiments investigated the following questions: 1) Does VPN outperform model-free baselines (e.g., DQN)? 2) What is the advantage of planning with a VPN over observation-based planning? 3) Is VPN useful for complex domains with high-dimensional sensory inputs, such as Atari games?\\n\\nExperimental Setting\\n\\nNetwork Architecture.\\n\\nA CNN was used as the encoding module of VPN, and the transition module consists of one option-conditional convolution layer which uses different weights depending on the option followed by a few more convolution layers. We used a residual connection\\xa0 from the previous abstract-state to the next abstract-state so that the transition module learns the change of the abstract-state. The outcome module is similar to the transition module except that it does not have a residual connection and two fully-connected layers are used to produce reward and discount. The value module consists of two fully-connected layers. The number of layers and hidden units vary depending on the domain. These details are described in the Appendix.\\n\\nImplementation Details.\\n\\nOur algorithm is based on asynchronous n-step Q-learning\\xa0 where n is 10 and 16 threads are used. The target network is synchronized after every 10K steps. We used the Adam optimizer\\xa0, and the best learning rate and its decay were chosen from {0.0001,0.0002,0.0005,0.001} and {0.98,0.95,0.9,0.8} respectively. The learning rate is multiplied by the decay every 1M steps. Our implementation is based on TensorFlow\\xa0.[1]\\n\\nVPN has four more hyperparameters: 1) the number of predictions steps (k) during training, 2) the plan depth (d_(train)) during training, 3) the plan depth (d_(test)) during evaluation, and 4) the branching factor (b) which indicates the number of options to be simulated for each expansion step during planning. We used k\\u2004=\\u2004d_(train)\\u2004=\\u2004d_(test) throughout the experiment unless otherwise stated. VPN(d) represents our model which learns to predict and simulate up to d-step futures during training and evaluation. The branching factor (b) was set to 4 until depth of 3 and set to 1 after depth of 3, which means that VPN simulates 4-best options up to depth of 3 and only the best option after that.\\n\\nBaselines.\\n\\nWe compared our approach to the following baselines.\\n\\n-   DQN: This baseline directly estimates Q-values as its output and is trained through asynchronous n-step Q-learning. Unlike the original DQN, however, our DQN baseline takes an option as additional input and applies an option-conditional convolution layer to the top of the last encoding convolution layer, which is very similar to our VPN architecture.[2]\\n\\n-   VPN(1): This is identical to our VPN with the same training procedure except that it performs only 1-step rollout to estimate Q-value as shown in Figure\\xa01. This can be viewed as a variation of DQN that predicts reward, discount, and the value of the next state as a decomposition of Q-value.\\n\\n-   OPN(d): We call this Observation Prediction Network (OPN), which is similar to VPN except that it directly predicts future observations. More specifically, we train two independent networks: a model network (f^(model)\\u2004:\\u2004x,\\u2006o\\u2004↦\\u2004r,\\u2006γ,\\u2006x′) which predicts reward, discount, and the next observation, and a value network (f^(value)\\u2004:\\u2004x\\u2004↦\\u2004V(x)) which estimates the value from the observation. The training scheme is similar to our algorithm except that a squared loss for observation prediction is used to train the model network. This baseline performs d-step planning like VPN(d).\\n\\nCollect Domain\\n\\nTask Description.\\n\\nWe defined a simple but challenging 2D navigation task where the agent should collect as many goals as possible within a time limit, as illustrated in Figure\\xa0[fig:collect]. In this task, the agent, goals, and walls are randomly placed for each episode. The agent has four options: move left/right/up/down to the first crossing branch or the end of the corridor in the chosen direction. The agent is given 20 steps for each episode and receives a positive reward (2.0) when it collects a goal by moving on top of it and a time-penalty (\\u2005−\\u20050.2) for each step. Although it is easy to learn a sub-optimal policy which collects nearby goals, finding the optimal trajectory in each episode requires careful planning because the optimal solution cannot be computed in polynomial time.\\n\\nAn observation is represented as a 3D tensor (ℝ^(3\\u2005×\\u200510\\u2005×\\u200510)) with binary values indicating the presence/absence of each object type. The time remaining is normalized to [0,1] and is concatenated to the 3rd convolution layer of the network as a channel.\\n\\nWe evaluated all architectures first in a deterministic environment and then investigated the robustness in a stochastic environment separately. In the stochastic environment, each goal moves by one block with probability of 0.3 for each step. In addition, each option can be repeated multiple times with probability of 0.3. This makes it difficult to predict and plan the future precisely.\\n\\n[Observation]\\n\\n[DQN’s trajectory]\\n\\n[VPN’s trajectory]\\n\\n[Plan with 20 steps]\\n\\n[Plan with 12 steps]\\n\\n[Deterministic]\\n\\n[Stochastic]\\n\\n[fig:curves]\\n\\nOverall Performance.\\n\\nThe result is summarized in Figure\\xa0[fig:curves]. To understand the quality of different policies, we implemented a greedy algorithm which always collects the nearest goal first and a shortest-path algorithm which finds the optimal solution through exhaustive search assuming that the environment is deterministic. Note that even a small gap in terms of reward can be qualitatively substantial as indicated by the small gap between greedy and shortest-path algorithms.\\n\\nThe results show that many architectures learned a better-than-greedy policy in the deterministic and stochastic environments except that OPN baselines perform poorly in the stochastic environment. In addition, the performance of VPN is improved as the plan depth increases, which implies that deeper predictions are reliable enough to provide more accurate value estimates of future states. As a result, VPN with 5-step planning represented by ‘VPN(5)’ performs best in both environments.\\n\\nComparison to Model-free Baselines.\\n\\nOur VPNs outperform DQN and VPN(1) baselines by a large margin as shown in Figure\\xa0[fig:curves]. Figure\\xa0[fig:collect] (b-c) shows an example of trajectories of DQN and VPN(5) given the same initial state. Although DQN’s behavior is reasonable, it ended up with collecting one less goal compared to VPN(5). We hypothesize that 6 convolution layers used by DQN and VPN(1) are not expressive enough to find the best route in each episode because finding an optimal path requires a combinatorial search in this task. On the other hand, VPN can perform such a combinatorial search to some extent by simulating future abstract-states, which has advantages over model-free approaches for dealing with tasks that require careful planning.\\n\\nComparison to Observation-based Planning.\\n\\nCompared to OPNs which perform planning based on predicted observations, VPNs perform slightly better or equally well in the deterministic environment. We observed that OPNs can predict future observations very accurately because observations in this task are simple and the environment is deterministic. Nevertheless, VPNs learn faster than OPNs in most cases. We conjecture that it takes additional training steps for OPNs to learn to predict future observations. In contrast, VPNs learn to predict only minimal but sufficient information for planning: reward, discount, and the value of future abstract-states, which may be the reason why VPNs learn faster than OPNs.\\n\\nIn the stochastic Collect domain, VPNs significantly outperform OPNs. We observed that OPNs tend to predict the average of possible future observations (𝔼_(x)[x]) because OPN is deterministic. Estimating values on such blurry predictions leads to estimating V_(θ)(𝔼_(x)[x]) which is different from the true expected value 𝔼_(x)[V(x)]. On the other hand, VPN is trained to approximate the true expected value because there is no explicit constraint or loss for the predicted abstract state. We hypothesize that this key distinction allows VPN to learn different modes of possible future states more flexibly in the abstract state space. This result suggests that a value-prediction model can be more beneficial than an observation-prediction model when the environment is stochastic and building an accurate observation-prediction model is difficult.\\n\\nr0.46\\n\\n  ---------- --------------- ------ ------ ------------ ------ ------\\n              Deterministic                 Stochastic         \\n                Original      FGs    MWs     Original    FGs    MWs\\n      Greedy      8.61        5.13   7.79      7.58      4.48   7.04\\n    Shortest      9.71        5.82   8.98      7.64      4.36   7.22\\n         DQN      8.66        4.57   7.08      7.85      4.11   6.72\\n      VPN(1)      8.94        4.92   7.64      7.84      4.27   7.15\\n      OPN(5)      9.30        5.45   8.36      7.55      4.09   6.79\\n      VPN(5)      9.29        5.43   8.31      8.11      4.45   7.46\\n  ---------- --------------- ------ ------ ------------ ------ ------\\n\\nGeneralization Performance.\\n\\nOne advantage of model-based RL approach is that it can generalize well to unseen environments as long as the dynamics of the environment remains similar. To see if our VPN has such a property, we evaluated all architectures on two types of previously unseen environments with either reduced number of goals (from 8 to 5) or increased number of walls. It turns out that our VPN is much more robust to the unseen environments compared to model-free baselines (DQN and VPN(1)), as shown in Table\\xa0[tab:generalization]. The model-free baselines perform worse than the greedy algorithm on unseen environments, whereas VPN still performs well. In addition, VPN generalizes as well as OPN which can learn a near-perfect model in the deterministic setting, and VPN significantly outperforms OPN in the stochastic setting. This suggests that VPN has a good generalization property like model-based RL methods and is robust to stochasticity.\\n\\nEffect of Planning Depth.\\n\\nr0.48 [image]\\n\\nTo further investigate the effect of planning depth in a VPN, we measured the average reward in the deterministic environment by varying the planning depth (d_(test)) from 1 to 10 during evaluation after training VPN with a fixed number of prediction steps and planning depth (k,\\u2006d_(train)), as shown in Figure\\xa0[fig:depth]. Since VPN does not learn to predict observations, there is no guarantee that it can perform deeper planning during evaluation (d_(test)) than the planning depth used during training (d_(train)). Interestingly, however, the result in Figure\\xa0[fig:depth] shows that if k\\u2004=\\u2004d_(train)\\u2004>\\u20042, VPN achieves better performance during evaluation through deeper tree search (d_(test)\\u2004>\\u2004d_(train)). We also tested a VPN with k\\u2004=\\u200410 and d_(train)\\u2004=\\u20045 and found that a planning depth of 10 achieved the best performance during evaluation. Thus, with a suitably large number of prediction steps during training, our VPN is able to benefit from deeper planning during evaluation relative to the planning depth during training. Figure\\xa0[fig:example] shows examples of good plans of length greater than 5 found by a VPN trained with planning depth 5. Another observation from Figure\\xa0[fig:depth] is that the performance of planning depth of 1 (d_(test)\\u2004=\\u20041) degrades as the planning depth during training (d_(train)) increases. This means that a VPN can improve its value estimations through long-term planning at the expense of the quality of short-term planning.\\n\\nAtari Games\\n\\nTo investigate how VPN deals with complex visual observations, we evaluated it on several Atari games\\xa0. Unlike in the Collect domain, in Atari games most primitive actions have only small value consequences and it is difficult to hand-design useful extended options. Nevertheless, we explored if VPNs are useful in Atari games even with short-lookahead planning using simple options that repeat the same primitive action over extended time periods by using a frame-skip of 10.[3] We pre-processed the game screen to 84\\u2005×\\u200584 gray-scale images. All architectures take last 4 frames as input. We doubled the number of hidden units of the fully-connected layer for DQN to approximately match the number of parameters. VPN learns to predict rewards and values but not discount (since it is fixed), and was trained to make 3-option-step predictions for planning which means that the agent predicts up to 0.5 seconds ahead in real-time.\\n\\n         Frostbite   Seaquest   Enduro   Alien   Q*Bert   Ms. Pacman   Amidar   Krull   Crazy Climber\\n  ----- ----------- ---------- -------- ------- -------- ------------ -------- ------- ---------------\\n    DQN    3058        2951      326     1804    12592       2804       535     12438       41658\\n    VPN    3811        5628      382     1429    14517       2689       641     15930       54119\\n\\n  : Performance on Atari games. Each number represents average score over 5 top agents.\\n\\n[Learning curves on Atari games. X-axis and y-axis correspond to steps and average reward over 100 episodes respectively. ]\\n\\n[State]\\n\\n[Plan 1 (19.3)]\\n\\n[Plan 2 (18.7)]\\n\\n[Plan 3 (18.4)]\\n\\n[Plan 4 (17.1)]\\n\\n[fig:atari-plan]\\n\\nAs summarized in Table\\xa01 and Figure\\xa07, our VPN outperforms DQN baseline on 7 out of 9 Atari games and learned significantly faster than DQN on Seaquest, QBert, Krull, and Crazy Climber. One possible reason why VPN outperforms DQN is that even 3-step planning is indeed helpful for learning a better policy. Figure\\xa0[fig:atari-plan] shows an example of VPN’s 3-step planning in Seaquest. Our VPN predicts reasonable values given different sequences of actions, which can potentially help choose a better action by looking at the short-term future. Another hypothesis is that the architecture of VPN itself, which has several auxiliary prediction tasks for multi-step future rewards and values, is useful for learning a good abstract-state representation as a model-free agent. Finally, our algorithm which performs planning to compute the target Q-value can potentially speed up learning by generating more accurate targets as it performs value backups multiple times from the simulated futures, as discussed in Section\\xa03.4. These results show that our approach is applicable to complex visual environments without needing to predict observations.\\n\\nConclusion\\n\\nWe introduced value prediction networks (VPNs) as a new deep RL way of integrating planning and learning while simultaneously learning the dynamics of abstract-states that make option-conditional predictions of future rewards/discount/values rather than future observations. Our empirical evaluations showed that VPNs outperform model-free DQN baselines in multiple domains, and outperform traditional observation-based planning in a stochastic domain. An interesting future direction would be to develop methods that automatically learn the options that allow good planning in VPNs.\\n\\nAcknowledgement\\n\\nThis work was supported by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsor.\\n\\nComparison between VPN and DQN in the Deterministic Collect\\n\\nObservation DQN’s trajectory VPN’s trajectory VPN’s 10-step plan\\n\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n[Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ] [Examples of trajectories and planning on the deterministic Collect domain. The first column shows initial observations, and the following two columns show trajectories of DQN and VPN respectively. It is shown that DQN sometimes chooses a non-optimal option and ends up with collecting fewer goals than VPN. The last column visualizes VPN’s 10 option-step planning from the initial state. Note that VPN’s initial plans do not always match with its actual trajectories because the VPN re-plans at every step as it observes its new state. ]\\n\\nComparison between VPN and OPN in the Stochastic Collect\\n\\n  ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- --\\n  [t = 1] [t = 1]     [t = 2] [t = 2]     [t = 3] [t = 3]     [t = 4] [t = 4]     [t = 5] [t = 5]     [t = 6] [t = 6]     [t = 7] [t = 7]     \\n                                                                                                                                              \\n  [t = 8] [t = 8]     [t = 9] [t = 9]     [t = 10] [t = 10]   [t = 11] [t = 11]   [t = 12] [t = 12]   [t = 13] [t = 13]   [t = 14] [t = 14]   \\n                                                                                                                                              \\n  [t = 15] [t = 15]   [t = 16] [t = 16]   [t = 17] [t = 17]   [t = 18] [t = 18]   [t = 19] [t = 19]   [t = 20] [t = 20]                       \\n  ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- ------------------- --\\n\\nExamples of Planning on Atari Games\\n\\n[Examples of VPN’s planning on Atari games. The first column shows initial states, and the following columns show our VPN’s value estimates in parentheses given different sequences of actions. Red and black arrows represent movement actions with and without ‘Fire’. ‘N’ and ‘F’ correspond to ‘No-operation’ and ‘Fire’. (Seaquest) The VPN estimates higher values for moving up to refill the oxygen tank and lower values for moving down to kill enemies because the agent loses a life when the oxygen tank is empty, which is almost running out. (Ms. Pacman) The VPN estimates the lowest value for moving towards an enemy (ghost). It also estimates a low value for moving right because it already has eaten some yellow pellets on the right side. On the other hand, it estimates relatively higher values for moving left and down because it collects more pellets while avoiding the enemy. (Frostbite) The VPN estimates higher values for collecting a nearby fish, which gives a positive reward, and lower values for not collecting it. (Enduro) The VPN estimates higher values for accelerating (fire) and avoiding collision and lower values for colliding with other cars. ]\\n\\nDetails of Learning\\n\\nAlgorithm\\xa0[alg] describes our algorithm for training value prediction network (VPN). We observed that training the outcome module (reward and discount prediction) on additional data collected from a random policy slightly improves the performance because it reduces a bias towards the agent’s behavior. More specifically, we fill a replay memory with R transitions from a random policy before training and sample transitions from the replay memory to train the outcome module. This procedure is described in Line 4 and Lines 20-24 in Algorithm\\xa0[alg]. This method was used only for Collect domain (not for Atari) in our experiment by generating 1M transitions from a random policy.\\n\\nθ: global parameter, θ⁻: global target network parameter, T: global step counter d: plan depth, k: number of prediction steps t\\u2004←\\u20040 and T\\u2004←\\u20040 M[1...R]← Store R transitions (s,o,r,γ,s′) using a random policy Clear gradients dθ\\u2004←\\u20040 Synchronize thread-specific parameter θ′\\u2004←\\u2004θ t_(start)\\u2004←\\u2004t s_(t)← Observe state a_(t)← argmax_(o)Q_(θ′)^(d)(s_(t),o_(t)) or random option based on ϵ-greedy policy r_(t),\\u2006γ_(t),\\u2006s_(t\\u2005+\\u20051)← Execute o_(t) t\\u2004←\\u2004t\\u2005+\\u20051 and T\\u2004←\\u2004T\\u2005+\\u20051 $R=\\\\begin{cases}\\n0 & \\\\mbox{if } s_t \\\\mbox{ is terminal}\\\\\\\\\\n\\\\max_o Q^{d}_{\\\\theta^-}(s_t,o) & \\\\mbox{if } s_t \\\\mbox{ is non-terminal}\\n\\\\end{cases}$ R\\u2004←\\u2004r_(i)\\u2005+\\u2005γ_(i)R $d\\\\theta \\\\gets d\\\\theta + \\\\nabla_{\\\\theta\\'}\\\\left[\\\\sum_{l=1}^{k}\\\\left(R-v^{l}_i\\\\right)^2 + \\\\left(r_i-r^{l}_i\\\\right)^2 + \\\\left(\\\\log_\\\\gamma \\\\gamma_i- \\\\log_\\\\gamma\\\\gamma^{l}_i\\\\right)^2 \\\\right]$ t′← Sample an index from 1,\\u20062,\\u2006...,\\u2006R s_(i),\\u2006a_(i),\\u2006r_(i),\\u2006γ_(i),\\u2006s_(i\\u2005+\\u20051)← Retrieve a transition from M[i] $d\\\\theta \\\\gets d\\\\theta + \\\\nabla_{\\\\theta\\'}\\\\left[\\\\sum_{l=1}^{k} \\\\left(r_i-r^{l}_i\\\\right)^2 + \\\\left(\\\\log_\\\\gamma \\\\gamma_i- \\\\log_\\\\gamma\\\\gamma^{l}_i\\\\right)^2 \\\\right]$ Perform asynchronous update of θ using dθ Update the target network θ⁻\\u2004←\\u2004θ\\n\\nDetails of Hyperparameters\\n\\n[Transition module used for Collect domain. The first convolution layer uses different weights depending on the given option. Sigmoid activation function is used for the last 1x1 convolution such that its output forms a mask. This mask is multiplied to the output from the 3rd convolution layer. Note that there is a residual connection from s to s′. Thus, the transition module learns the change of the consecutive abstract states. ]\\n\\nCollect\\n\\nThe encoding module of our VPN consists of Conv(32-3x3-1)-Conv(32-3x3-1)-Conv(64-4x4-2) where Conv(N-KxK-S) represents N filters with size of KxK with a stride of S. The transition module is illustrated in Figure\\xa029. It consists of OptionConv(64-3x3-1)-Conv(64-3x3-1)-Conv(64-3x3-1) and a separate Conv(64-1x1-1) for the mask which is multiplied to the output of the 3rd convolution layer of the transition module. ‘OptionConv’ uses different convolution weights depending on the given option. We also used a residual connection from the previous abstract state to the next abstract state such that the transition module learns the difference between two states. The outcome module has OptionConv(64-3x3-1)-Conv(64-3x3-1)-FC(64)-FC(2) where FC(N) represents a fully-connected layer with N hidden units. The value module consists of FC(64)-FC(1). Exponential linear unit (ELU)\\xa0 was used as an activation function for all architectures.\\n\\nOur DQN baseline consists of the encoding module followed by the transition module followed by the value module. Thus, the overall architecture is very similar to VPN except that it does not have the outcome module. To match the number of parameters, we used 256 hidden units for DQN’s value module. We found that this architecture outperforms the original DQN architecture\\xa0 on Collect domain and several Atari games.\\n\\nThe model network of OPN baseline has the same architecture as VPN except that it has an additional decoding module which consists of Deconv(64-4x4-2)-Deconv(32-3x3-1)-Deconv(32-3x3-1). This module is applied to the predicted abstract-state so that it can predict the future observations. The value network of OPN has the same architecture as our DQN baseline.\\n\\nA discount factor of 0.98 was used, and the target network was synchronized after every 10K steps. The epsilon for ϵ-greedy policy was linearly decreased from 1 to 0.05 for the first 1M steps.\\n\\nAtari Games\\n\\nThe encoding module consists of Conv(16-8x8-4)-Conv(32-4x4-2), and the transition module has OptionConv(32-3x3-1)-Conv(32-3x3-1) with a mask and a residual connection as described above. The outcome module has OptionConv(32-3x3-1)-Conv(32-3x3-1)-FC(128)-FC(1), and the value module consists of FC(128)-FC(1). The DQN baseline has the same encoding module followed by the transition module and the value module, and we used 256 hidden units for the value module of DQN to approximately match the number of parameters. The other hyperparameters are same as the ones used in the Collect domain except that a discount factor of 0.99 was used.\\n\\n[1] The code is available on https://github.com/junhyukoh/value-prediction-network.\\n\\n[2] This architecture outperformed the original DQN architecture in our preliminary experiments.\\n\\n[3] Much of the previous work on Atari games has used a frame-skip of 4. Though using a larger frame-skip generally makes training easier, it may make training harder in some games if they require more fine-grained control\\xa0.\\n\\n\\nPlease answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, and metric?',\n",
       " 'answer': \"[{'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Seaquest', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Amidar', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Krull', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Alien', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Enduro', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Ms. Pacman', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Crazy Climber', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Q*Bert', 'Metric': 'Score'}}, {'LEADERBOARD': {'Task': 'Atari Games', 'Dataset': 'Atari 2600 Frostbite', 'Metric': 'Score'}}]\",\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fold1['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036d1c9003ee43acb0c2b89783ef7317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04be517c49014350a3a4aed13af1beb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06c39c7b19ff4102a059335937096a77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07e9a50ed7014724aaf6f083e481cc01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaa4609ff63a4451a2f27d87265b3f71",
      "placeholder": "​",
      "style": "IPY_MODEL_e8fcd026196044a08250eeb8f0c9bdd8",
      "value": " 792k/792k [00:01&lt;00:00, 537kB/s]"
     }
    },
    "080d2b5824c340848a99a988f65199bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b95b3978ff674a119683e3ac0fe936e9",
       "IPY_MODEL_dfae349d2c6e4efb93933fc4c011fe97",
       "IPY_MODEL_21a2c896f1c24781b4fe5834cea2454c"
      ],
      "layout": "IPY_MODEL_ba52a3af729042e9b11f1d4e22a553eb"
     }
    },
    "0effbee2d34943fc8d6d0d3a265d0766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f2ce18b656a4e7ead8774f9c28b4eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "1144779119e843109365495f972af363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68df245747594d27b8db61a63ea30b36",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ea5fb72af6e46438af4ad47cbbd32d2",
      "value": 2
     }
    },
    "13466f14b2484f7dbfd7ffe2ecc395f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "13f3d839ed0249c082da5c8da1ebd386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee070df2cc9b42d6891e5ae557040066",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a926a4c6b7f345b2a021410743ed5d53",
      "value": 500
     }
    },
    "1572ca2178b64d0d9accd4c7620d0f96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16f0f99dbcfa4ff8b459aa5d3d9c74cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "170cc57e9f92469597b6aab8962a8ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c607df922234a20b8f72f92b965c452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e915a28fc764077977f06cf12c95c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21a2c896f1c24781b4fe5834cea2454c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b410e5b0ca02405f9d71072f1e34b2ba",
      "placeholder": "​",
      "style": "IPY_MODEL_44389af5674642c1891cc2bf208838a6",
      "value": " 242M/242M [00:03&lt;00:00, 72.8MB/s]"
     }
    },
    "22702932b5dd4bdca2ca98bd75b20a93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23fc564de44144718f5fd9bbe921ae80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26ffcb55db4149d38c34570982fa0fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2921977fec8c4f54b727f6aa12334d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fb7ca7667254da58ef251e6c2e4483e",
      "placeholder": "​",
      "style": "IPY_MODEL_91f420fdfedb464a8455c52b05bbdaad",
      "value": "Downloading: 100%"
     }
    },
    "29edae08655f4c478cddb3e5441e8ae6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40915c810d8e41f6a7bcc8faa84ba0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7feb6429c4e24fd89aab0a3b0efd38eb",
      "placeholder": "​",
      "style": "IPY_MODEL_e88c4810f71346019c6d9765cacc3284",
      "value": " 500/500 [00:27&lt;00:00, 18.35it/s]"
     }
    },
    "44389af5674642c1891cc2bf208838a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45746813cd3c4d82ada51bf58274a791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b42a824910bf495289c9f56f241fc6f6",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_604014d142594f4f87ce70e164d94a14",
      "value": 500
     }
    },
    "4c02d0ce16fe4c6187a2c8b952e8419f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22702932b5dd4bdca2ca98bd75b20a93",
      "placeholder": "​",
      "style": "IPY_MODEL_170cc57e9f92469597b6aab8962a8ce3",
      "value": "Downloading: 100%"
     }
    },
    "4ea5fb72af6e46438af4ad47cbbd32d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f45cc2640f44395a635a928f6d2f4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec023301cdad42cdabc32be9ee003f2a",
       "IPY_MODEL_cb9a3ee490e248398094c6789cc8526e",
       "IPY_MODEL_40915c810d8e41f6a7bcc8faa84ba0bb"
      ],
      "layout": "IPY_MODEL_96e050ff888943fc90182209e4d72227"
     }
    },
    "5f5740fe5a3e45289881a5ed5a498764": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "604014d142594f4f87ce70e164d94a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6519d08cb83a4e7386bdda75f8f16861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68df245747594d27b8db61a63ea30b36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c2c9da14016449a8c13e08b395f7b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cd3ede915c84c629bf46d234388b02c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbad20d361c6425cbb716354a9e8d328",
      "placeholder": "​",
      "style": "IPY_MODEL_96a9941107bd4e9b984755365e80df22",
      "value": " 500/500 [00:27&lt;00:00, 18.43it/s]"
     }
    },
    "6f01127a146d443a973c0c3674663559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc6fda81438846b8bd09a059bc0b5553",
       "IPY_MODEL_45746813cd3c4d82ada51bf58274a791",
       "IPY_MODEL_6cd3ede915c84c629bf46d234388b02c"
      ],
      "layout": "IPY_MODEL_13466f14b2484f7dbfd7ffe2ecc395f7"
     }
    },
    "71b01ab113c341fc8e4c8a2e36f336f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e915a28fc764077977f06cf12c95c04",
      "placeholder": "​",
      "style": "IPY_MODEL_bffcb591f4d74ebab31550f134f89dca",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "74f7576971d74113aa0605f41eb5fb89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c69b41b3fb8748cea2e7770562b47ce3",
      "placeholder": "​",
      "style": "IPY_MODEL_a04320425bd94c79a497cc7dd9b144ce",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 69.2kB/s]"
     }
    },
    "7c90ba46512d4a94a1971f5f4b4d97d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7feb6429c4e24fd89aab0a3b0efd38eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "853f16aaaa5c4d13a67b143f2e946c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4a7c18179414629ab20376d4180a8a7",
      "placeholder": "​",
      "style": "IPY_MODEL_d5e1a9a72304460a9021598550913edc",
      "value": " 1000/1000 [02:05&lt;00:00,  7.96it/s, loss=1.16, v_num=2]"
     }
    },
    "8a1d7051e6ce4cbe8ad268b12751f4ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c02d0ce16fe4c6187a2c8b952e8419f",
       "IPY_MODEL_cffb3f880b7f45719ad19c6fbb082948",
       "IPY_MODEL_07e9a50ed7014724aaf6f083e481cc01"
      ],
      "layout": "IPY_MODEL_23fc564de44144718f5fd9bbe921ae80"
     }
    },
    "8fb7ca7667254da58ef251e6c2e4483e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f420fdfedb464a8455c52b05bbdaad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93adec39ba514f24a582ddbd529e01a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96a9941107bd4e9b984755365e80df22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96e050ff888943fc90182209e4d72227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "9ab910f3fc8540f591c0db3dffc74cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb3484e0e3d14bd7b8b7b0163fc6c81a",
      "placeholder": "​",
      "style": "IPY_MODEL_6519d08cb83a4e7386bdda75f8f16861",
      "value": " 2/2 [00:00&lt;00:00, 23.71it/s]"
     }
    },
    "9cfda34969e044acb3368d547caf95ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2b40eec0a5d4ec189a327d95a1cfd90",
      "placeholder": "​",
      "style": "IPY_MODEL_f9fe48215ce14b91b0a57cd3c7096334",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "9d8a386189cc4a9c81b836fb41fa4f64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29edae08655f4c478cddb3e5441e8ae6",
      "placeholder": "​",
      "style": "IPY_MODEL_de53a4b226e541d28da4547ac58579ff",
      "value": "Epoch 2: 100%"
     }
    },
    "9d96616dfac14f2d8535a07cc5e13711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71b01ab113c341fc8e4c8a2e36f336f9",
       "IPY_MODEL_13f3d839ed0249c082da5c8da1ebd386",
       "IPY_MODEL_ab368c585dfa4b9ab4f97c7cd9d52f77"
      ],
      "layout": "IPY_MODEL_fdda03cbcaae4da1815b38ba5d40a7f0"
     }
    },
    "a04320425bd94c79a497cc7dd9b144ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a926a4c6b7f345b2a021410743ed5d53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa8d3f0d5edc43519b03b9360708f8b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaa4609ff63a4451a2f27d87265b3f71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab368c585dfa4b9ab4f97c7cd9d52f77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1572ca2178b64d0d9accd4c7620d0f96",
      "placeholder": "​",
      "style": "IPY_MODEL_c5ae98203edb4bdb83443d3f05b08247",
      "value": " 500/500 [00:27&lt;00:00, 18.34it/s]"
     }
    },
    "ae41b724aa4f469f8c408848d2a9de1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fccbbd51784e428697cd8a47d9b94317",
      "max": 1206,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04be517c49014350a3a4aed13af1beb4",
      "value": 1206
     }
    },
    "afa135917c1f4273bfdcc9e6f8c56683": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "b410e5b0ca02405f9d71072f1e34b2ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b42a824910bf495289c9f56f241fc6f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b572e3cd384c46499d0e8e4f7d52123a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b95b3978ff674a119683e3ac0fe936e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eef3c52f6b534523870e3ea61223014b",
      "placeholder": "​",
      "style": "IPY_MODEL_1c607df922234a20b8f72f92b965c452",
      "value": "Downloading: 100%"
     }
    },
    "ba52a3af729042e9b11f1d4e22a553eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb3484e0e3d14bd7b8b7b0163fc6c81a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bffcb591f4d74ebab31550f134f89dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2b40eec0a5d4ec189a327d95a1cfd90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5ae98203edb4bdb83443d3f05b08247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c69b41b3fb8748cea2e7770562b47ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6ccf24b960c4b25a59265c68780a0e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d8a386189cc4a9c81b836fb41fa4f64",
       "IPY_MODEL_e40763a82dc34304aca5e042c9e77375",
       "IPY_MODEL_853f16aaaa5c4d13a67b143f2e946c3a"
      ],
      "layout": "IPY_MODEL_0f2ce18b656a4e7ead8774f9c28b4eda"
     }
    },
    "cb9a3ee490e248398094c6789cc8526e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b572e3cd384c46499d0e8e4f7d52123a",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93adec39ba514f24a582ddbd529e01a0",
      "value": 500
     }
    },
    "cbad20d361c6425cbb716354a9e8d328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cce9d70e2dc94fc398e63efce16ce4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cfda34969e044acb3368d547caf95ef",
       "IPY_MODEL_1144779119e843109365495f972af363",
       "IPY_MODEL_9ab910f3fc8540f591c0db3dffc74cb5"
      ],
      "layout": "IPY_MODEL_afa135917c1f4273bfdcc9e6f8c56683"
     }
    },
    "cffb3f880b7f45719ad19c6fbb082948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06c39c7b19ff4102a059335937096a77",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26ffcb55db4149d38c34570982fa0fca",
      "value": 791656
     }
    },
    "d5e1a9a72304460a9021598550913edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da395e550e3f40379416bc4b2ae0acd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de53a4b226e541d28da4547ac58579ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dfae349d2c6e4efb93933fc4c011fe97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa8d3f0d5edc43519b03b9360708f8b2",
      "max": 242065649,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da395e550e3f40379416bc4b2ae0acd0",
      "value": 242065649
     }
    },
    "e340d5c2a56247918e94384810ea21d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e40763a82dc34304aca5e042c9e77375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f5740fe5a3e45289881a5ed5a498764",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0effbee2d34943fc8d6d0d3a265d0766",
      "value": 1000
     }
    },
    "e88c4810f71346019c6d9765cacc3284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8fcd026196044a08250eeb8f0c9bdd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf189ef3899443fb90e40e5abd4f088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2921977fec8c4f54b727f6aa12334d09",
       "IPY_MODEL_ae41b724aa4f469f8c408848d2a9de1f",
       "IPY_MODEL_74f7576971d74113aa0605f41eb5fb89"
      ],
      "layout": "IPY_MODEL_7c90ba46512d4a94a1971f5f4b4d97d8"
     }
    },
    "ec023301cdad42cdabc32be9ee003f2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e340d5c2a56247918e94384810ea21d9",
      "placeholder": "​",
      "style": "IPY_MODEL_6c2c9da14016449a8c13e08b395f7b23",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "ee070df2cc9b42d6891e5ae557040066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eef3c52f6b534523870e3ea61223014b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4a7c18179414629ab20376d4180a8a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9fe48215ce14b91b0a57cd3c7096334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc6fda81438846b8bd09a059bc0b5553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_036d1c9003ee43acb0c2b89783ef7317",
      "placeholder": "​",
      "style": "IPY_MODEL_16f0f99dbcfa4ff8b459aa5d3d9c74cd",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "fccbbd51784e428697cd8a47d9b94317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdda03cbcaae4da1815b38ba5d40a7f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
