{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers --user\n",
    "# !pip install -q accelerate --user\n",
    "# !pip install -q bitsandbytes --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 19:36:20.335179: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-09 19:36:20.389140: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-09 19:36:21.264273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-Tune Llama2-7b on custom dataset\n",
    "import os, ipdb\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch, random\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\\\n",
    ", BitsAndBytesConfig, HfArgumentParser, TrainingArguments, TrainerCallback, pipeline\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "from evaluation_metrics import Metrics\n",
    "\n",
    "# from ../evaluation_metrics import Metrics\n",
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # or \"true\", depending on your needs\n",
    "\n",
    "# pd.options.display.max_rows , pd.options.display.max_columns  = 100,100  \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import ipywidgets as widget\n",
    "# widget.IntSlider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: Optional[str] = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n",
    "    log_with: Optional[str] = field(default=\"wandb\", metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "\n",
    "    dataset_name: Optional[str] = field(default=\"lvwerra/stack-exchange-paired\", metadata={\"help\": \"the dataset name\"})\n",
    "    subset: Optional[str] = field(default=\"data/finetune\", metadata={\"help\": \"the subset to use\"})\n",
    "    split: Optional[str] = field(default=\"train\", metadata={\"help\": \"the split to use\"})\n",
    "    size_valid_set: Optional[int] = field(default=4000, metadata={\"help\": \"the size of the validation set\"})\n",
    "    streaming: Optional[bool] = field(default=True, metadata={\"help\": \"whether to stream the dataset\"})\n",
    "    shuffle_buffer: Optional[int] = field(default=5000, metadata={\"help\": \"the shuffle buffer size\"})\n",
    "    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"the sequence length\"})\n",
    "    num_workers: Optional[int] = field(default=4, metadata={\"help\": \"the number of workers\"})\n",
    "\n",
    "    max_steps: Optional[int] = field(default=500, metadata={\"help\": \"the maximum number of sgd steps\"})\n",
    "    logging_steps: Optional[int] = field(default=10, metadata={\"help\": \"the logging frequency\"})\n",
    "    save_steps: Optional[int] = field(default=10, metadata={\"help\": \"the saving frequency\"})\n",
    "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"the per device train batch size\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the per device eval batch size\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=2, metadata={\"help\": \"the gradient accumulation steps\"})\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
    "    )\n",
    "    group_by_length: Optional[bool] = field(default=False, metadata={\"help\": \"whether to group by length\"})\n",
    "    packing: Optional[bool] = field(default=True, metadata={\"help\": \"whether to use packing for SFTTrainer\"})\n",
    "\n",
    "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
    "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
    "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
    "\n",
    "    learning_rate: Optional[float] = field(default=1e-4, metadata={\"help\": \"the learning rate\"})\n",
    "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
    "    num_warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"the number of warmup steps\"})\n",
    "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
    "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
    "\n",
    "    output_dir: Optional[str] = field(default=\"./results\", metadata={\"help\": \"the output directory\"})\n",
    "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
    "\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args = parser.parse_args_into_dataclasses([])[0]\n",
    "\n",
    "if script_args.group_by_length and script_args.packing:\n",
    "    raise ValueError(\"Cannot use both packing and group by length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# script_args.per_device_train_batch_size,\n",
    "script_args.gradient_accumulation_steps,\n",
    "# script_args.per_device_eval_batch_size,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# script_args.model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "# script_args.size = \"13b\"\n",
    "\n",
    "# script_args.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "script_args.model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# script_args.model_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "\n",
    "script_args.size = \"7b\"\n",
    "\n",
    "\n",
    "# script_args.checkpoint = \"checkpoint-25200\"\n",
    "script_args.seq_length = 2400\n",
    "\n",
    "# script_args.dataset_name = \"./data/LLLM_DOCTEAT_TDM_ALL_TEMPLATE/fold2\"\n",
    "# script_args.output_dir = \"./model_ckpt/docteat_tdm_f2_all_template\"\n",
    "# script_args.run_name = \"sft_llama2_docteat_tdm_f2_all_Template\"\n",
    "\n",
    "# # script_args.dataset_name = \"./data/LLLM_DOCTEAT_TDMS_ALL_TEMPLATE/fold2\"\n",
    "# # script_args.output_dir = f\"./model_ckpt/docteat_llama2_{script_args.size}_tdms_f2_all_template\"\n",
    "# # script_args.run_name = f\"sft_docteat_llama2_{script_args.size}_tdms_f2_all_Template\"\n",
    "# script_args.dataset_name = \"../data/LLLM_DOCTEAT_TDMS_ALL_TEMPLATE/fold1\"\n",
    "# script_args.output_dir = f\"../model_ckpt/docteat_llama2_{script_args.size}_tdms_f1_all_template\"\n",
    "# script_args.run_name = f\"sft_docteat_llama2_{script_args.size}_tdms_f1_all_Template\"\n",
    "# script_args.seq_length = 1024\n",
    "# script_args.per_device_train_batch_size = 4\n",
    "# script_args.gradient_accumulation_steps = 2\n",
    "# # multi GPU\n",
    "# script_args.per_device_eval_batch_size = 1\n",
    "\n",
    "i = 1\n",
    "script_args.test_dataset = f\"../data/LLLM_LONG_SUMMARIZED_TDMS_SQUAD_{i}/fold1\"\n",
    "# script_args.dataset_name = \"../data/LLLM_LONG_TDMS_ALL_TEMPLATE/fold1\"\n",
    "script_args.dataset_name = \"../data/LLLM_LONG_SUMMARIZED_TDMS_ALL_TEMPLATE/fold1\"\n",
    "script_args.output_dir = f\"../model_ckpt/long_summ_mistralai_{script_args.size}_tdms_f1_all_template_seq_{script_args.seq_length}\"\n",
    "script_args.run_name = f\"sft_long_summ_mistralai_{script_args.size}_tdms_f1_all_template_seq_{script_args.seq_length}\"\n",
    "script_args.per_device_train_batch_size = 3\n",
    "script_args.gradient_accumulation_steps = 2\n",
    "script_args.per_device_eval_batch_size = 2\n",
    "\n",
    "script_args.save_steps = 2\n",
    "script_args.logging_steps = 2\n",
    "script_args.streaming = False\n",
    "script_args.num_train_epochs = 2\n",
    "script_args.save_total_limit = 1\n",
    "script_args.max_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_args.seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = prepare_sample_text(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "\n",
    "    return total_characters / total_tokens\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_sample_text(example):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['answer']}\"\n",
    "    # text = f\"{example['prompt']}\\n{example['answer']}\"\n",
    "    return text\n",
    "\n",
    "def create_datasets(tokenizer, args):\n",
    "    # dataset = load_dataset(\n",
    "    #     args.dataset_name,\n",
    "    #     data_dir=args.subset,\n",
    "    #     split=args.split,\n",
    "    #     use_auth_token=True,\n",
    "    #     num_proc=args.num_workers if not args.streaming else None,\n",
    "    #     streaming=args.streaming,\n",
    "    # )\n",
    "    \n",
    "    dataset = DatasetDict.load_from_disk(f\"{args.dataset_name}\")\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    \n",
    "    # if args.streaming:\n",
    "    #     print(\"Loading the dataset in streaming mode\")\n",
    "    #     valid_data = dataset.take(args.size_valid_set)\n",
    "    #     train_data = dataset.skip(args.size_valid_set)\n",
    "    #     train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
    "    # else:\n",
    "    \n",
    "    # # dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "    # train_data = dataset[\"train\"]\n",
    "    # valid_data = dataset[\"validation\"]\n",
    "\n",
    "    train_data = dataset[\"train\"].shard(num_shards=2000, index=0)\n",
    "    valid_data = dataset[\"validation\"].shard(num_shards=2000, index=0)\n",
    "    \n",
    "    print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n",
    "\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer, nb_examples=600)\n",
    "    # chars_per_token = chars_token_ratio(train_data, tokenizer, nb_examples=len(train_data)//2)\n",
    "    # 3.70\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=True,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=False,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    return train_dataset, valid_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=script_args.lora_r,\n",
    "    lora_alpha=script_args.lora_alpha,\n",
    "    lora_dropout=script_args.lora_dropout,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     script_args.model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map={\"\": 0},\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=\"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\",\n",
    "#     # use_auth_token=True,\n",
    "# )\n",
    "\n",
    "# base_model.config.use_cache = False\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     script_args.model_name, \n",
    "#     use_auth_token=\"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\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/stanford_alpaca/issues/133#issuecomment-1483893538\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=script_args.output_dir,\n",
    "#     per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "#     gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "#     per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "#     learning_rate=script_args.learning_rate,\n",
    "#     logging_steps=script_args.logging_steps,\n",
    "#     # max_steps=script_args.max_steps,\n",
    "#     # report_to=script_args.log_with,\n",
    "#     save_steps=script_args.save_steps,\n",
    "#     group_by_length=script_args.group_by_length,\n",
    "#     lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "#     warmup_steps=script_args.num_warmup_steps,\n",
    "#     optim=script_args.optimizer_type,\n",
    "#     # bf16=True,\n",
    "#     # fp16=True,\n",
    "#     fp16=False,\n",
    "#     bf16=False,\n",
    "#     remove_unused_columns=False,\n",
    "#     num_train_epochs = script_args.num_train_epochs,\n",
    "#     run_name=script_args.run_name,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     save_strategy=\"steps\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset, eval_dataset \u001b[38;5;241m=\u001b[39m create_datasets(\u001b[43mtokenizer\u001b[49m, script_args)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset, eval_dataset = create_datasets(tokenizer, script_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs available: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected_steps = ((len(train_dataset) // (script_args.per_device_train_batch_size * script_args.gradient_accumulation_steps)) * script_args.num_train_epochs)// num_gpus\n",
    "# expected_steps = (len(train_dataset) // (training_args.per_device_train_batch_size)) * training_args.num_train_epochs\n",
    "\n",
    "print(f\"Expected steps: {expected_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomWandbCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Custom logs you want to add\n",
    "        custom_logs = {\n",
    "            \"training_args\": training_args,\n",
    "            # ... any other custom data\n",
    "        }\n",
    "        wandb.log(custom_logs)  # Log the custom data to wandb\n",
    "\n",
    "class CustomTrainer(SFTTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_train_losses = []\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Perform a training step. The model parameters will be updated.\n",
    "        \"\"\"\n",
    "        # Call the original training step\n",
    "        loss = super().training_step(model, inputs)\n",
    "\n",
    "        current_train_loss = loss.item()\n",
    "\n",
    "        \n",
    "\n",
    "        # Save model if it's among the top 10 best training losses\n",
    "        self.best_train_losses = sorted(self.best_train_losses + [(current_train_loss, self.state.global_step)], key=lambda x: x[0])[:training_args.save_total_limit]\n",
    "\n",
    "        if len(self.best_train_losses) == training_args.save_total_limit:\n",
    "            worst_loss, worst_step = self.best_train_losses[-1]\n",
    "            \n",
    "            if current_train_loss < worst_loss:\n",
    "                # Delete the checkpoint with the worst loss\n",
    "                checkpoint_folder = os.path.join(training_args.output_dir, f'checkpoint-{worst_step}')\n",
    "                # ipdb.set_trace()\n",
    "                if os.path.exists(checkpoint_folder) and is_main_process(self.args.local_rank):\n",
    "                    ipdb.set_trace()\n",
    "                    os.remove(checkpoint_folder)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None):\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys)\n",
    "\n",
    "        # Let's say the key for validation loss in the metrics dictionary is 'eval_loss'\n",
    "        current_val_loss = metrics['eval_loss']\n",
    "\n",
    "        # Sort the losses and get the worst (maximum) loss from our saved checkpoints\n",
    "        self.best_val_losses = sorted(self.best_val_losses + [(current_val_loss, self.state.global_step)], key=lambda x: x[0])[:training_args.save_total_limit]\n",
    "\n",
    "        if len(self.best_val_losses) == training_args.save_total_limit:\n",
    "            worst_loss, worst_step = self.best_val_losses[-1]\n",
    "            ipdb.set_trace()\n",
    "            if current_val_loss < worst_loss:\n",
    "                # Delete the checkpoint with the worst loss\n",
    "                checkpoint_folder = os.path.join(training_args.output_dir, f'checkpoint-{worst_step}')\n",
    "                if os.path.exists(checkpoint_folder) and is_main_process(self.args.local_rank):\n",
    "                    os.remove(checkpoint_folder)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "        \n",
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # type: ignore\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # type: ignore\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    ipdb.set_trace()\n",
    "\n",
    "    print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "    results = Metrics.evaluate_property_wise_json_based(label_list=decoded_labels, prediction_list=decoded_preds)\n",
    "\n",
    "    # ipdb.set_trace()\n",
    "\n",
    "    print(results)\n",
    "    \n",
    "    results.update(Metrics.evaluate_rouge(label_list=decoded_labels, prediction_list=decoded_preds))\n",
    "\n",
    "    # print(results)\n",
    "    \n",
    "    \n",
    "    # clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "    # results = clf_metrics.compute(predictions=[1 if \"unanswerable\" == x.replace(\"</s>\", \"\") else 0 for x in decoded_preds], \n",
    "    #                     references=[1 if \"unanswerable\" == x else 0 for x in decoded_labels]\n",
    "    # )\n",
    "\n",
    "    # rouge = evaluate.load('rouge')\n",
    "    \n",
    "    # rouge_results = rouge.compute(\n",
    "    #     predictions=[pred.replace(\"</s>\", \"\") for pred in decoded_preds],\n",
    "    #     references=decoded_labels\n",
    "    # )\n",
    "    # results.update(rouge_results) \n",
    "\n",
    "    # ipdb.set_trace()\n",
    "    \n",
    "    # result = Metrics.evaluate_property_wise_text_based(label_list=decoded_labels, prediction_list=decoded_preds)\n",
    "    # result.update(Metrics.evaluate_rouge(label_list=decoded_labels, prediction_list=decoded_preds))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_args.save_total_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/tatsu-lab/stanford_alpaca/issues/133#issuecomment-1483893538\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=script_args.output_dir,\n",
    "    per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "    learning_rate=script_args.learning_rate,\n",
    "    logging_steps=script_args.logging_steps,\n",
    "    # max_steps=script_args.max_steps,\n",
    "    report_to=script_args.log_with,\n",
    "    save_steps=script_args.save_steps,\n",
    "    save_total_limit=script_args.save_total_limit,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    # save_strategy = \"no\",\n",
    "    eval_steps = 2,\n",
    "    load_best_model_at_end=True,\n",
    "    group_by_length=script_args.group_by_length,\n",
    "    lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "    warmup_steps=script_args.num_warmup_steps,\n",
    "    optim=script_args.optimizer_type,\n",
    "    # bf16=True,\n",
    "    # fp16=True,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs = script_args.num_train_epochs,\n",
    "    run_name=script_args.run_name,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=script_args.packing,\n",
    "    # max_seq_length=None,\n",
    "    max_seq_length=script_args.seq_length,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# trainer = CustomTrainer(\n",
    "#     model=base_model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     packing=script_args.packing,\n",
    "#     # max_seq_length=None,\n",
    "#     max_seq_length=script_args.seq_length,\n",
    "#     # compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_args,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # 34450\n",
    "# # !rm -r ../model_ckpt/docteat_llama2_13b_tdms_f1_all_template/*\n",
    "# trainer.state.best_model_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MODEL SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_ckpt/long_summ_mistralai_7b_tdms_f1_all_template_seq_2400/best_checkpoint'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{script_args.output_dir}/best_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/save_pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# output_dir = os.path.join(script_args.output_dir, \"final_checkpoint_\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# output_dir = os.path.join(script_args.output_dir, f\"{script_args.run_name}\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model.save_pretrained(output_merged_dir, safe_serialization=True)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"{script_args.output_dir}/best_checkpoint\")\n",
    "\n",
    "trainer.model.save_pretrained(f\"{script_args.output_dir}/save_pretrained\")\n",
    "\n",
    "# output_dir = os.path.join(script_args.output_dir, \"final_checkpoint_\")\n",
    "# output_dir = os.path.join(script_args.output_dir, f\"{script_args.run_name}\")\n",
    "\n",
    "# trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "# # Free memory for merging weights\n",
    "# del base_model\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
    "# model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     script_args.output_dir, \n",
    "#     device_map=\"auto\", \n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     offload_folder = \"offload/\"\n",
    "# )\n",
    "\n",
    "# # model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     # script_args.output_dir,\n",
    "    \n",
    "#     device_map=\"auto\",\n",
    "#     # low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     # load_in_4bit=True,\n",
    "# )\n",
    "\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
    "# model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_ckpt/long_llama2_7b_tdms_f1_all_template_seq_len_3000/checkpoint-3000'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{script_args.output_dir}/{script_args.test_ckpt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dad8a88a77747c09c2b9ab9c764a692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    # script_args.model_name,\n",
    "    # f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "    f\"{script_args.output_dir}/best_checkpoint\",    \n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    "    use_auth_token=\"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\",\n",
    "    # use_auth_token=\"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\"\n",
    ")\n",
    "\n",
    "# model.config.use_cache = False\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    # f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "    f\"{script_args.output_dir}/best_checkpoint\",\n",
    "    use_auth_token=\"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\",\n",
    ")\n",
    "\n",
    "# model = model.cpu()\n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# # model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/LLLM_LONG_SUMMARIZED_TDMS_SQUAD_1/fold1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.config.use_cache = True\n",
    "script_args.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1298"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict.load_from_disk(f\"{script_args.test_dataset}\")\n",
    "    \n",
    "    # if args.streaming:\n",
    "    #     print(\"Loading the dataset in streaming mode\")\n",
    "    #     valid_data = dataset.take(args.size_valid_set)\n",
    "    #     train_data = dataset.skip(args.size_valid_set)\n",
    "    #     train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
    "    # else:\n",
    "    \n",
    "# dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "train_data = dataset[\"train\"].shuffle(seed=42)\n",
    "valid_data = dataset[\"validation\"].shuffle(seed=42)\n",
    "# valid_data = dataset[\"validation\"].shard(num_shards=10, index=0).shuffle(seed=42)\n",
    "\n",
    "# train_data[0]\n",
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1116\n",
      "\n",
      "Question: Title:\tExploring Question Understanding and Adaptation in Neural-Network-Based Question Answering\n",
      "\n",
      "Abstract:\tThe last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.\n",
      "\n",
      "Experiment Results\n",
      "\n",
      "Set-Up\n",
      "\n",
      "We test our models on Stanford Question Answering Dataset (SQuAD) . The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the Wikipedia articles. Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score.\n",
      "\n",
      "We use pre-trained 300-D Glove 840B vectors  to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method  is used for optimization. And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding d_(w) is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension d_(ET) is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5.\n",
      "\n",
      "  Model                                                                      EM       F1       \n",
      "  -------------------------------------------------------------------------- -------- -------- --\n",
      "  Logistic Regression Baseline                                               40.4     51.0     \n",
      "  Match-LSTM with Ans-Ptr (Sentence)                                         54.505   67.748   \n",
      "  Match-LSTM with Ans-Ptr (Boundary)                                         60.474   70.695   \n",
      "  Dynamic Chunk Reader                                                       62.499   70.956   \n",
      "  Fine-Grained Gating                                                        62.446   73.327   \n",
      "  Match-LSTM with Bi-Ans-Ptr (Boundary)                                      64.744   73.743   \n",
      "  Multi-Perspective Matching                                                 65.551   75.118   \n",
      "  Dynamic Coattention Networks                                               66.233   75.896   \n",
      "  BiLSTM [German Research Center for Artificial Intelligence(unpublished)]   68.436   77.070   \n",
      "  BiDAF                                                                      67.974   77.323   \n",
      "  jNet(Ours)                                                                 68.730   77.393   \n",
      "  r-net [Microsoft Research Asia(unpublished)]                               70.062   78.782   \n",
      "  Human Performance                                                          82.304   91.221   \n",
      "\n",
      "Results\n",
      "\n",
      "Overall results\n",
      "\n",
      "Table [tab:result] shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling).\n",
      "\n",
      "  Model                                   EM      F1      \n",
      "  --------------------------------------- ------- ------- --\n",
      "  Baseline                                68.00   77.36   \n",
      "  +Explicit question types (ET)           68.16   77.58   \n",
      "  +TreeLSTM (Q^(TL))                      68.29   77.67   \n",
      "  +TreeLSTM adaptation (Q^(TLa), K=20)    68.73   77.74   \n",
      "  +TreeLSTM adaptation (Q^(TLa), K=100)   69.10   78.38   \n",
      "\n",
      "Table [tab:ablation] shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types (K) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table [tab:result]. Furthermore, after submitted our result, we have experimented with a large value of K and found that when K = 100, we can achieve a better performance of 69.10%/78.38% on the development set.\n",
      "\n",
      "Figure [fig:subfig:a] shows the EM/F1 scores of different question types while Figure [fig:subfig:b] is the question type amount distribution on the development set. In Figure [fig:subfig:a] we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure [fig:subfig:b] we can see the \"what\" question is the major class.\n",
      "\n",
      "Figure 1 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system.\n",
      "\n",
      "[F1 Score Analysis.]\n",
      "\n",
      "Conclusions\n",
      "\n",
      "Closely modelling questions could be of importance for question answering and machine reading. In this paper, we introduce syntactic information to help encode questions in neural networks. We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.\n",
      "\n",
      "\n",
      "Please answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\n",
      "Answer: [{'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'EM', 'Score': '73.010'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'F1', 'Score': '81.517'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'EM', 'Score': '70.607'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1', 'Metric': 'F1', 'Score': '79.821'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1 dev', 'Metric': 'EM', 'Score': '69.10'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SQuAD1.1 dev', 'Metric': 'F1', 'Score': '78.38'}}]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(valid_data))\n",
    "\n",
    "print(f\"Index: {idx}\\n\")\n",
    "\n",
    "print(f\"Question: {valid_data[idx]['prompt']}\")\n",
    "# print(\"\\n#################################################\\n\")\n",
    "print(f\"Answer: {valid_data[idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 1970, but `max_length` is set to 500. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Title:\tExploring Question Understanding and Adaptation in Neural-Network-Based Question Answering\n",
      "\n",
      "Abstract:\tThe last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.\n",
      "\n",
      "Experiment Results\n",
      "\n",
      "Set-Up\n",
      "\n",
      "We test our models on Stanford Question Answering Dataset (SQuAD) . The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the Wikipedia articles. Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score.\n",
      "\n",
      "We use pre-trained 300-D Glove 840B vectors  to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method  is used for optimization. And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding d_(w) is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension d_(ET) is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5.\n",
      "\n",
      "  Model                                                                      EM       F1       \n",
      "  -------------------------------------------------------------------------- -------- -------- --\n",
      "  Logistic Regression Baseline                                               40.4     51.0     \n",
      "  Match-LSTM with Ans-Ptr (Sentence)                                         54.505   67.748   \n",
      "  Match-LSTM with Ans-Ptr (Boundary)                                         60.474   70.695   \n",
      "  Dynamic Chunk Reader                                                       62.499   70.956   \n",
      "  Fine-Grained Gating                                                        62.446   73.327   \n",
      "  Match-LSTM with Bi-Ans-Ptr (Boundary)                                      64.744   73.743   \n",
      "  Multi-Perspective Matching                                                 65.551   75.118   \n",
      "  Dynamic Coattention Networks                                               66.233   75.896   \n",
      "  BiLSTM [German Research Center for Artificial Intelligence(unpublished)]   68.436   77.070   \n",
      "  BiDAF                                                                      67.974   77.323   \n",
      "  jNet(Ours)                                                                 68.730   77.393   \n",
      "  r-net [Microsoft Research Asia(unpublished)]                               70.062   78.782   \n",
      "  Human Performance                                                          82.304   91.221   \n",
      "\n",
      "Results\n",
      "\n",
      "Overall results\n",
      "\n",
      "Table [tab:result] shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling).\n",
      "\n",
      "  Model                                   EM      F1      \n",
      "  --------------------------------------- ------- ------- --\n",
      "  Baseline                                68.00   77.36   \n",
      "  +Explicit question types (ET)           68.16   77.58   \n",
      "  +TreeLSTM (Q^(TL))                      68.29   77.67   \n",
      "  +TreeLSTM adaptation (Q^(TLa), K=20)    68.73   77.74   \n",
      "  +TreeLSTM adaptation (Q^(TLa), K=100)   69.10   78.38   \n",
      "\n",
      "Table [tab:ablation] shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types (K) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table [tab:result]. Furthermore, after submitted our result, we have experimented with a large value of K and found that when K = 100, we can achieve a better performance of 69.10%/78.38% on the development set.\n",
      "\n",
      "Figure [fig:subfig:a] shows the EM/F1 scores of different question types while Figure [fig:subfig:b] is the question type amount distribution on the development set. In Figure [fig:subfig:a] we can see that the average EM/F1 of the \"when\" question is highest and those of the \"why\" question is the lowest. From Figure [fig:subfig:b] we can see the \"what\" question is the major class.\n",
      "\n",
      "Figure 1 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system.\n",
      "\n",
      "[F1 Score Analysis.]\n",
      "\n",
      "Conclusions\n",
      "\n",
      "Closely modelling questions could be of importance for question answering and machine reading. In this paper, we introduce syntactic information to help encode questions in neural networks. We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.\n",
      "\n",
      "\n",
      "Please answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Leonardo Da Vinci?\"\n",
    "prompt = f\"Question: {valid_data[idx]['prompt']}\"\n",
    "# model_pth = f\"{script_args.output_dir}/best_checkpoint\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     f\"{script_args.output_dir}/best_checkpoint\",\n",
    "#     use_auth_token=\"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\",\n",
    "# )\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500#script_args.seq_length+100\n",
    "               )\n",
    "# pipe = pipeline(task=\"text-generation\", model=f\"{script_args.output_dir}/save_pretrained\", tokenizer=tokenizer, max_length=script_args.seq_length)\n",
    "\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: [{'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'Yago11k', 'Metric': 'MRR', 'Score': '0.2364'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'ICEWS05-15', 'Metric': 'MRR', 'Score': '0.632'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'ICEWS14', 'Metric': 'MRR', 'Score': '0.589'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'Wikidata12k', 'Metric': 'MRR', 'Score': '0.3335'}}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {valid_data[idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"Question: {valid_data[idx]['prompt']}\"\n",
    "\n",
    "# result = pipe(f\"{prompt}\")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: [{'LEADERBOARD': {'Task': 'Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMS', 'Score': '0.407'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error', 'Score': '0.064'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMSE', 'Score': '0.392'}}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {valid_data[idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{script_args.o/utput_dir}/{script_args.checkpoint}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ScriptArguments' object has no attribute 'checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoPeftModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# \"../model_ckpt/docteat_llama2_13b_tdms_f2_all_template/sft_docteat_llama2_13b_tdms_f2_all_Template\",\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript_args\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscript_args\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# load_in_4bit=True,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[1;32m      8\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_iuVAGWCqRYwIlzFqErBuZvQoUnexcOTGGj\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mgenerate()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ScriptArguments' object has no attribute 'checkpoint'"
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    # \"../model_ckpt/docteat_llama2_13b_tdms_f2_all_template/sft_docteat_llama2_13b_tdms_f2_all_Template\",\n",
    "    f\"{script_args.output_dir}/{script_args.checkpoint}\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # load_in_4bit=True,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True,\n",
    "    token=\"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\",\n",
    ")\n",
    "\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{script_args.output_dir}/{script_args.checkpoint}\")\n",
    "\n",
    "train_dataset, eval_dataset = create_datasets(tokenizer, script_args)\n",
    "\n",
    "# for example in tqdm(iter(dataset)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(f\"{script_args.dataset_name}\")\n",
    "    \n",
    "    # if args.streaming:\n",
    "    #     print(\"Loading the dataset in streaming mode\")\n",
    "    #     valid_data = dataset.take(args.size_valid_set)\n",
    "    #     train_data = dataset.skip(args.size_valid_set)\n",
    "    #     train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
    "    # else:\n",
    "    \n",
    "# dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"validation\"]\n",
    "\n",
    "valid_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(script_args.output_dir)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# inputs = tokenizer.encode(f\"{train_data[1]['prompt']}\", return_tensors=\"pt\").to(DEV)\n",
    "inputs = tokenizer.encode(f\"{valid_data[0]['prompt']}\", return_tensors=\"pt\").to(DEV)\n",
    "\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    input_ids=inputs,\n",
    "    # temperature=0.2, \n",
    "    # top_p=0.95, \n",
    "    # top_k=40,\n",
    "    # max_new_tokens=1200,\n",
    "    # repetition_penalty=1.3\n",
    ")\n",
    "outputs = model.generate(**generate_kwargs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs[0]).split(\"Answer:\")[-1].replace(\"</s>\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_data[0:3]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters you've provided—`temperature`, `top_p`, `top_k`, `max_new_tokens`, and `repetition_penalty`—are hyperparameters that control the generation behavior of a large language model such as those from the GPT series.\n",
    "\n",
    "Here's a brief description of each:\n",
    "\n",
    "1. **temperature**: Adjusts the \"sharpness\" of the distribution of the next-token probabilities. Lower values (e.g., 0.2) make the outputs more deterministic (i.e., more focused on high-probability outcomes), while higher values make it more random.\n",
    "\n",
    "2. **top_p (nucleus sampling)**: Instead of sampling from the full distribution, it samples from the smallest set of words whose cumulative probability exceeds the value of `top_p`. This can lead to more dynamic and unpredictable outputs.\n",
    "\n",
    "3. **top_k**: Limits the set of tokens considered for generation to the top k probabilities. This can avoid very rare words that can sometimes disrupt the coherence of generated text.\n",
    "\n",
    "4. **max_new_tokens**: The maximum number of tokens to generate in the response. It essentially controls the length of the generated output.\n",
    "\n",
    "5. **repetition_penalty**: If set to a value > 1.0, it penalizes already generated tokens, making repetitions less likely. Conversely, values < 1.0 make repetitions more likely.\n",
    "\n",
    "For \"better\" generation, it depends on your specific use case:\n",
    "\n",
    "- **For more deterministic, focused outputs**: Lower `temperature` (e.g., 0.2), reasonable `top_k` (like 40), and `top_p` closer to 1 (like 0.95) work well.\n",
    "\n",
    "- **For more creative, diverse outputs**: Increase `temperature` (e.g., 0.7 or higher), use a more liberal `top_k` value or rely more on `top_p` (like setting it to 0.85 or so).\n",
    "\n",
    "- **To control verbosity**: Adjust `max_new_tokens`. Setting it to 1200 will allow for very long outputs. If you want shorter outputs, reduce this value.\n",
    "\n",
    "- **To control repetitiveness**: Use the `repetition_penalty`. A value of 1.3, as you've set, should help reduce repetitiveness. Adjust as needed.\n",
    "\n",
    "It's also a good practice to experiment and fine-tune these hyperparameters based on the specific outputs you're looking for and your subjective judgment of the quality. If possible, gather feedback from users or subject matter experts to refine these settings further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "\n",
    "# # Randomly select 500 indices from valid_data\n",
    "# selected_indices = random.sample(range(len(valid_data)), 500)\n",
    "\n",
    "# # Create a new list of selected examples\n",
    "# selected_valid_data = [valid_data[i] for i in selected_indices]\n",
    "\n",
    "# for i, valid_ex in tqdm(enumerate(selected_valid_data), total=500):\n",
    "#     # Your processing code here\n",
    "    \n",
    "for i, valid_ex in tqdm(enumerate(valid_data), total=len(valid_data)):\n",
    "    # if i <= 100 :\n",
    "    #     continue \n",
    "\n",
    "    if len(valid_ex['prompt'].split()) >= 2400:\n",
    "        continue\n",
    "        \n",
    "    inputs = tokenizer.encode(f\"Question: {valid_ex['prompt']}\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        # script_args.model_name,\n",
    "        # f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "        f\"{script_args.output_dir}/best_checkpoint\",    \n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        load_in_4bit=True,\n",
    "        use_auth_token=\"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\",\n",
    "        # use_auth_token=\"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\"\n",
    "    )\n",
    "    \n",
    "    # model.config.use_cache = False\n",
    "    \n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        # f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "        f\"{script_args.output_dir}/best_checkpoint\",\n",
    "        use_auth_token=\"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\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    # prompt = \"Who is Leonardo Da Vinci?\"\n",
    "    prompt = f\"Question: {valid_data[idx]['prompt']}\"\n",
    "    # model_pth = f\"{script_args.output_dir}/best_checkpoint\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\n",
    "    #     f\"{script_args.output_dir}/best_checkpoint\",\n",
    "    #     use_auth_token=\"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\",\n",
    "    # )\n",
    "    \n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=script_args.seq_length+100)\n",
    "    # pipe = pipeline(task=\"text-generation\", model=f\"{script_args.output_dir}/save_pretrained\", tokenizer=tokenizer, max_length=script_args.seq_length)\n",
    "    \n",
    "    # result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "    result = pipe(f\"{prompt}\")\n",
    "    print(result[0]['generated_text'])\n",
    "\n",
    "    preds.append(predictions.split(\"Answer: \")[-1].replace(\"</s>\", \"\"))\n",
    "    labels.append(valid_ex['answer'])\n",
    "    \n",
    "    if i >= 5:\n",
    "        break\n",
    "    \n",
    "    # ipdb.set_trace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Metrics.evaluate_property_wise_json_based(label_list=labels_, prediction_list=preds_)\n",
    "results.update(Metrics.evaluate_rouge(label_list=labels_, prediction_list=preds_))\n",
    "\n",
    "print(f\"Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036d1c9003ee43acb0c2b89783ef7317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04be517c49014350a3a4aed13af1beb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06c39c7b19ff4102a059335937096a77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07e9a50ed7014724aaf6f083e481cc01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaa4609ff63a4451a2f27d87265b3f71",
      "placeholder": "​",
      "style": "IPY_MODEL_e8fcd026196044a08250eeb8f0c9bdd8",
      "value": " 792k/792k [00:01&lt;00:00, 537kB/s]"
     }
    },
    "080d2b5824c340848a99a988f65199bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b95b3978ff674a119683e3ac0fe936e9",
       "IPY_MODEL_dfae349d2c6e4efb93933fc4c011fe97",
       "IPY_MODEL_21a2c896f1c24781b4fe5834cea2454c"
      ],
      "layout": "IPY_MODEL_ba52a3af729042e9b11f1d4e22a553eb"
     }
    },
    "0effbee2d34943fc8d6d0d3a265d0766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f2ce18b656a4e7ead8774f9c28b4eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "1144779119e843109365495f972af363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68df245747594d27b8db61a63ea30b36",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ea5fb72af6e46438af4ad47cbbd32d2",
      "value": 2
     }
    },
    "13466f14b2484f7dbfd7ffe2ecc395f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "13f3d839ed0249c082da5c8da1ebd386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee070df2cc9b42d6891e5ae557040066",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a926a4c6b7f345b2a021410743ed5d53",
      "value": 500
     }
    },
    "1572ca2178b64d0d9accd4c7620d0f96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16f0f99dbcfa4ff8b459aa5d3d9c74cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "170cc57e9f92469597b6aab8962a8ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c607df922234a20b8f72f92b965c452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e915a28fc764077977f06cf12c95c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21a2c896f1c24781b4fe5834cea2454c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b410e5b0ca02405f9d71072f1e34b2ba",
      "placeholder": "​",
      "style": "IPY_MODEL_44389af5674642c1891cc2bf208838a6",
      "value": " 242M/242M [00:03&lt;00:00, 72.8MB/s]"
     }
    },
    "22702932b5dd4bdca2ca98bd75b20a93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23fc564de44144718f5fd9bbe921ae80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26ffcb55db4149d38c34570982fa0fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2921977fec8c4f54b727f6aa12334d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fb7ca7667254da58ef251e6c2e4483e",
      "placeholder": "​",
      "style": "IPY_MODEL_91f420fdfedb464a8455c52b05bbdaad",
      "value": "Downloading: 100%"
     }
    },
    "29edae08655f4c478cddb3e5441e8ae6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40915c810d8e41f6a7bcc8faa84ba0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7feb6429c4e24fd89aab0a3b0efd38eb",
      "placeholder": "​",
      "style": "IPY_MODEL_e88c4810f71346019c6d9765cacc3284",
      "value": " 500/500 [00:27&lt;00:00, 18.35it/s]"
     }
    },
    "44389af5674642c1891cc2bf208838a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45746813cd3c4d82ada51bf58274a791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b42a824910bf495289c9f56f241fc6f6",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_604014d142594f4f87ce70e164d94a14",
      "value": 500
     }
    },
    "4c02d0ce16fe4c6187a2c8b952e8419f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22702932b5dd4bdca2ca98bd75b20a93",
      "placeholder": "​",
      "style": "IPY_MODEL_170cc57e9f92469597b6aab8962a8ce3",
      "value": "Downloading: 100%"
     }
    },
    "4ea5fb72af6e46438af4ad47cbbd32d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f45cc2640f44395a635a928f6d2f4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec023301cdad42cdabc32be9ee003f2a",
       "IPY_MODEL_cb9a3ee490e248398094c6789cc8526e",
       "IPY_MODEL_40915c810d8e41f6a7bcc8faa84ba0bb"
      ],
      "layout": "IPY_MODEL_96e050ff888943fc90182209e4d72227"
     }
    },
    "5f5740fe5a3e45289881a5ed5a498764": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "604014d142594f4f87ce70e164d94a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6519d08cb83a4e7386bdda75f8f16861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68df245747594d27b8db61a63ea30b36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c2c9da14016449a8c13e08b395f7b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cd3ede915c84c629bf46d234388b02c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbad20d361c6425cbb716354a9e8d328",
      "placeholder": "​",
      "style": "IPY_MODEL_96a9941107bd4e9b984755365e80df22",
      "value": " 500/500 [00:27&lt;00:00, 18.43it/s]"
     }
    },
    "6f01127a146d443a973c0c3674663559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc6fda81438846b8bd09a059bc0b5553",
       "IPY_MODEL_45746813cd3c4d82ada51bf58274a791",
       "IPY_MODEL_6cd3ede915c84c629bf46d234388b02c"
      ],
      "layout": "IPY_MODEL_13466f14b2484f7dbfd7ffe2ecc395f7"
     }
    },
    "71b01ab113c341fc8e4c8a2e36f336f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e915a28fc764077977f06cf12c95c04",
      "placeholder": "​",
      "style": "IPY_MODEL_bffcb591f4d74ebab31550f134f89dca",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "74f7576971d74113aa0605f41eb5fb89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c69b41b3fb8748cea2e7770562b47ce3",
      "placeholder": "​",
      "style": "IPY_MODEL_a04320425bd94c79a497cc7dd9b144ce",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 69.2kB/s]"
     }
    },
    "7c90ba46512d4a94a1971f5f4b4d97d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7feb6429c4e24fd89aab0a3b0efd38eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "853f16aaaa5c4d13a67b143f2e946c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4a7c18179414629ab20376d4180a8a7",
      "placeholder": "​",
      "style": "IPY_MODEL_d5e1a9a72304460a9021598550913edc",
      "value": " 1000/1000 [02:05&lt;00:00,  7.96it/s, loss=1.16, v_num=2]"
     }
    },
    "8a1d7051e6ce4cbe8ad268b12751f4ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c02d0ce16fe4c6187a2c8b952e8419f",
       "IPY_MODEL_cffb3f880b7f45719ad19c6fbb082948",
       "IPY_MODEL_07e9a50ed7014724aaf6f083e481cc01"
      ],
      "layout": "IPY_MODEL_23fc564de44144718f5fd9bbe921ae80"
     }
    },
    "8fb7ca7667254da58ef251e6c2e4483e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f420fdfedb464a8455c52b05bbdaad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93adec39ba514f24a582ddbd529e01a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96a9941107bd4e9b984755365e80df22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96e050ff888943fc90182209e4d72227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "9ab910f3fc8540f591c0db3dffc74cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb3484e0e3d14bd7b8b7b0163fc6c81a",
      "placeholder": "​",
      "style": "IPY_MODEL_6519d08cb83a4e7386bdda75f8f16861",
      "value": " 2/2 [00:00&lt;00:00, 23.71it/s]"
     }
    },
    "9cfda34969e044acb3368d547caf95ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2b40eec0a5d4ec189a327d95a1cfd90",
      "placeholder": "​",
      "style": "IPY_MODEL_f9fe48215ce14b91b0a57cd3c7096334",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "9d8a386189cc4a9c81b836fb41fa4f64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29edae08655f4c478cddb3e5441e8ae6",
      "placeholder": "​",
      "style": "IPY_MODEL_de53a4b226e541d28da4547ac58579ff",
      "value": "Epoch 2: 100%"
     }
    },
    "9d96616dfac14f2d8535a07cc5e13711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71b01ab113c341fc8e4c8a2e36f336f9",
       "IPY_MODEL_13f3d839ed0249c082da5c8da1ebd386",
       "IPY_MODEL_ab368c585dfa4b9ab4f97c7cd9d52f77"
      ],
      "layout": "IPY_MODEL_fdda03cbcaae4da1815b38ba5d40a7f0"
     }
    },
    "a04320425bd94c79a497cc7dd9b144ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a926a4c6b7f345b2a021410743ed5d53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa8d3f0d5edc43519b03b9360708f8b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaa4609ff63a4451a2f27d87265b3f71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab368c585dfa4b9ab4f97c7cd9d52f77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1572ca2178b64d0d9accd4c7620d0f96",
      "placeholder": "​",
      "style": "IPY_MODEL_c5ae98203edb4bdb83443d3f05b08247",
      "value": " 500/500 [00:27&lt;00:00, 18.34it/s]"
     }
    },
    "ae41b724aa4f469f8c408848d2a9de1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fccbbd51784e428697cd8a47d9b94317",
      "max": 1206,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04be517c49014350a3a4aed13af1beb4",
      "value": 1206
     }
    },
    "afa135917c1f4273bfdcc9e6f8c56683": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "b410e5b0ca02405f9d71072f1e34b2ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b42a824910bf495289c9f56f241fc6f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b572e3cd384c46499d0e8e4f7d52123a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b95b3978ff674a119683e3ac0fe936e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eef3c52f6b534523870e3ea61223014b",
      "placeholder": "​",
      "style": "IPY_MODEL_1c607df922234a20b8f72f92b965c452",
      "value": "Downloading: 100%"
     }
    },
    "ba52a3af729042e9b11f1d4e22a553eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb3484e0e3d14bd7b8b7b0163fc6c81a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bffcb591f4d74ebab31550f134f89dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2b40eec0a5d4ec189a327d95a1cfd90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5ae98203edb4bdb83443d3f05b08247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c69b41b3fb8748cea2e7770562b47ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6ccf24b960c4b25a59265c68780a0e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d8a386189cc4a9c81b836fb41fa4f64",
       "IPY_MODEL_e40763a82dc34304aca5e042c9e77375",
       "IPY_MODEL_853f16aaaa5c4d13a67b143f2e946c3a"
      ],
      "layout": "IPY_MODEL_0f2ce18b656a4e7ead8774f9c28b4eda"
     }
    },
    "cb9a3ee490e248398094c6789cc8526e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b572e3cd384c46499d0e8e4f7d52123a",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93adec39ba514f24a582ddbd529e01a0",
      "value": 500
     }
    },
    "cbad20d361c6425cbb716354a9e8d328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cce9d70e2dc94fc398e63efce16ce4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cfda34969e044acb3368d547caf95ef",
       "IPY_MODEL_1144779119e843109365495f972af363",
       "IPY_MODEL_9ab910f3fc8540f591c0db3dffc74cb5"
      ],
      "layout": "IPY_MODEL_afa135917c1f4273bfdcc9e6f8c56683"
     }
    },
    "cffb3f880b7f45719ad19c6fbb082948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06c39c7b19ff4102a059335937096a77",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26ffcb55db4149d38c34570982fa0fca",
      "value": 791656
     }
    },
    "d5e1a9a72304460a9021598550913edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da395e550e3f40379416bc4b2ae0acd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de53a4b226e541d28da4547ac58579ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dfae349d2c6e4efb93933fc4c011fe97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa8d3f0d5edc43519b03b9360708f8b2",
      "max": 242065649,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da395e550e3f40379416bc4b2ae0acd0",
      "value": 242065649
     }
    },
    "e340d5c2a56247918e94384810ea21d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e40763a82dc34304aca5e042c9e77375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f5740fe5a3e45289881a5ed5a498764",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0effbee2d34943fc8d6d0d3a265d0766",
      "value": 1000
     }
    },
    "e88c4810f71346019c6d9765cacc3284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8fcd026196044a08250eeb8f0c9bdd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf189ef3899443fb90e40e5abd4f088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2921977fec8c4f54b727f6aa12334d09",
       "IPY_MODEL_ae41b724aa4f469f8c408848d2a9de1f",
       "IPY_MODEL_74f7576971d74113aa0605f41eb5fb89"
      ],
      "layout": "IPY_MODEL_7c90ba46512d4a94a1971f5f4b4d97d8"
     }
    },
    "ec023301cdad42cdabc32be9ee003f2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e340d5c2a56247918e94384810ea21d9",
      "placeholder": "​",
      "style": "IPY_MODEL_6c2c9da14016449a8c13e08b395f7b23",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "ee070df2cc9b42d6891e5ae557040066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eef3c52f6b534523870e3ea61223014b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4a7c18179414629ab20376d4180a8a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9fe48215ce14b91b0a57cd3c7096334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc6fda81438846b8bd09a059bc0b5553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_036d1c9003ee43acb0c2b89783ef7317",
      "placeholder": "​",
      "style": "IPY_MODEL_16f0f99dbcfa4ff8b459aa5d3d9c74cd",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "fccbbd51784e428697cd8a47d9b94317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdda03cbcaae4da1815b38ba5d40a7f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
