{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa5c7f4-8125-4f01-8710-4e7a62b973a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 09:19:44.817078: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-08 09:19:44.868569: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-08 09:19:46.137547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os, ipdb, re\n",
    "import random, evaluate\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "import wandb\n",
    "import ast\n",
    "import re, os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18136c8-f242-4d57-a727-17a4c0b61d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1298"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "# dataset_path = f\"../data/LLLM_DOCTEAT_TDMS_SQUAD_{i}/fold1\"\n",
    "# dataset_path = f\"../data/LLLM_LONG_TDMS_SQUAD_{i}/fold1\"\n",
    "dataset_path = f\"../data/LLLM_LONG_SUMMARIZED_TDMS_SQUAD_{i}/fold1\"\n",
    "\n",
    "\n",
    "# dataset_path = f\"../data/LLLM_LONG_TDMS_DROP_{i}/fold2\"\n",
    "# dataset_path = \"../data/LLLM_DOCTEAT_TDMS_ALL_TEMPLATE/fold1\"\n",
    "# dataset_path = f\"../data/LLLM_LONG_TDMS_SQUAD_{i}/fold1\"\n",
    "# dataset_path = \"../data/LLLM_LONG_TDMS_ALL_TEMPLATE/fold1\"\n",
    "\n",
    "dataset_dict = DatasetDict.load_from_disk(f\"{dataset_path}\")\n",
    "    \n",
    "\n",
    "train_data = dataset_dict[\"train\"].shuffle(seed=42)\n",
    "valid_data = dataset_dict[\"validation\"].shuffle(seed=42)\n",
    "# valid_data = dataset[\"validation\"].shard(num_shards=10, index=0).shuffle(seed=42)\n",
    "\n",
    "# train_data[0]\n",
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0cdb5f8-6a92-4291-96fb-719e75cfc679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Title:\\tCompatibility of Shelah and Stupp’s and Muchnik’s iteration with fragments of monadic second order logic\\n\\nAbstract:\\tWe investigate the relation between the theory of the iterations in the sense of Shelah-Stupp and of Muchnik, resp., and the theory of the base structure for several logics. These logics are obtained from the restriction of set quantification in monadic second order logic to certain subsets like, e.g., finite sets, chains, and finite unions of chains. We show that these theories of the Shelah-Stupp iteration can be reduced to corresponding theories of the base structure. This fails for Muchnik’s iteration.\\n\\nDietrich Kuske\\n\\n\\nPlease answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?',\n",
       " 'answer': 'unanswerable',\n",
       " '__index_level_0__': 2695}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb1a24d-53ba-4113-92c8-da3f82643b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[1704]['prompt'].split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36388ae2-f15e-49d1-9761-397781d62ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[1704]['prompt'].split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd15d58-e3e8-4bf2-a83f-34ddf23baf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Title:\\tLinguistic Input Features Improve Neural Machine Translation\\n\\nAbstract:\\tNeural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that the strong learning capability of neural MT models does not make linguistic features redundant; they can be easily incorporated to provide further improvements in performance. We generalize the embedding layer of the encoder in the attentional encoder–decoder architecture to support the inclusion of arbitrary features, in addition to the baseline word feature. We add morphological features, part-of-speech tags, and syntactic dependency labels as input features to English↔German and English→Romanian neural machine translation systems. In experiments on WMT16 training and test sets, we find that linguistic input features improve model quality according to three metrics: perplexity, Bleu and chrF3. An open-source implementation of our neural MT system is available[1], as are sample files and configurations[2].\\n\\nConclusion\\n\\nIn this paper we investigate whether linguistic input features are beneficial to neural machine translation, and our empirical evidence suggests that this is the case.\\n\\nWe describe a generalization of the encoder in the popular attentional encoder-decoder architecture for neural machine translation that allows for the inclusion of an arbitrary number of input features. We empirically test the inclusion of various linguistic features, including lemmas, part-of-speech tags, syntactic dependency labels, and morphological features, into English↔German, and English→Romanian neural MT systems. Our experiments show that the linguistic features yield improvements over our baseline, resulting in improvements on newstest2016 of 1.5 Bleu for German→English, 0.6 Bleu for English→German, and 1.0 Bleu for English→Romanian.\\n\\nIn the future, we expect several developments that will shed more light on the usefulness of linguistic (or other) input features, and whether they will establish themselves as a core component of neural machine translation. On the one hand, the machine learning capability of neural architectures is likely to increase, decreasing the benefit provided by the features we tested. On the other hand, there is potential to explore the inclusion of novel features for neural MT, which might prove to be even more helpful than the ones we investigated, and the features we investigated may prove especially helpful for some translation settings, such as very low-resourced settings and/or translation settings with a highly inflected source language.\\n\\n[1] https://github.com/rsennrich/nematus\\n\\n[2] https://github.com/rsennrich/wmt16-scripts\\n\\n\\nPlease answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?',\n",
       " 'answer': \"[{'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2016 German-English', 'Metric': 'BLEU score', 'Score': '32.9'}}, {'LEADERBOARD': {'Task': 'Machine Translation', 'Dataset': 'WMT2016 English-German', 'Metric': 'BLEU score', 'Score': '28.4'}}]\",\n",
       " '__index_level_0__': 848}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[random.randint(0, len(valid_data))]\n",
    "\n",
    "# valid_data[1295]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593a05b3-fb9f-4e45-ba68-0fb66bdf98b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title:\\tColorNet - Estimating Colorfulness in Natural Images\\n\\nAbstract:\\tMeasuring the colorfulness of a natural or virtual scene is critical for many applications in image processing field ranging from capturing to display. In this paper, we propose the first deep learning-based colorfulness estimation metric. For this purpose, we develop a color rating model which simultaneously learns to extracts the pertinent characteristic color features and the mapping from feature space to the ideal colorfulness scores for a variety of natural colored images. Additionally, we propose to overcome the lack of adequate annotated dataset problem by combining/aligning two publicly available colorfulness databases using the results of a new subjective test which employs a common subset of both databases. Using the obtained subjectively annotated dataset with 180 colored images, we finally demonstrate the efficacy of our proposed model over the traditional methods, both quantitatively and qualitatively.\\n\\nColourfulness, CNN, Color metric, Deep learning.\\n\\nResults\\n\\nTraining and Implementation Details: We split the dataset of 180 images into training, validation and test sets in an 80%, 10% and 10% setting. For training, the input image size is fixed at 600\\u2005×\\u2005600, and random crops of size 512\\u2005×\\u2005512 are applied to the image. Additional data augmentation techniques such as rotation and flipping are applied to scale up the training dataset.\\n\\nThe ColorNet model is implemented using the Pytorch\\xa0 deep learning library. During training, the batch size is set to 4 and the baseline weights of the feature networks are initialized by training on the ImageNet\\xa0 dataset. The weights of the layers in the rating networks are initialized randomly. In other terms, we fine-tune the feature network and train the rating network layers for our task at hand. A dropout rate of 0.75 is set in the rating network for all the three models. We utilize an ADAM solver\\xa0 with an initial learning rate of 1\\u2005×\\u200510⁻⁴ and 1\\u2005×\\u200510⁻³ for training the feature and rating networks respectively. The learning rates are allowed to decay exponentially with a decay rate of 0.95, after every 10 epochs. The momentum rate is fixed at 0.9 for all epochs. All our models are trained in an end-to-end fashion for 200 epochs. Training is done using a 12 GB NVIDIA Titan-X GPU on an Intel Xeon E7 core i7 machine for 200 epochs which take approximately 2 hours. Inference time is 25 secs for each image.\\n\\nTraditional Colorfulness Methods: We implemented four different colorfulness metrics: Hasler and Süsstrunk (CF_(Hasler))\\xa0 , two versions (CQE₁^(CF) and CQE₂^(CF)) of Panetta et\\xa0al.\\xa0, and Yendrikhovskij et\\xa0al. (CF_(Yendrikhovskij))\\xa0 to compare with the proposed ColorNet model.\\n\\nFor the Hasler and Süsstrunk\\xa0, with a given RGB image, the metric uses the mean μ, and standard deviation σ of the opponent color space vectors v_(rg) and v_(yb) where v_(rg)\\u2004=\\u2004I_(R)\\u2005−\\u2005I_(G) and v_(yb)\\u2004=\\u2004(I_(R)+I_(G))/2\\u2005−\\u2005I_(B). Then, CF_(Hasler) is computed as:\\n$$\\\\label{eqn:haslerCF}\\n    CF_\\\\text{Hasler} = \\\\sqrt{\\\\sigma_{rg}^2 + \\\\sigma_{yb}^2} + 0.3 \\\\times \\\\sqrt{\\\\mu_{rg}^2 + \\\\mu_{yb}^2}$$\\n\\nPanetta et\\xa0al.\\xa0 use the similar color opponent space, but in logarithmic space. The metrics CQE₁^(CF) and CQE₂^(CF) are calculated as:\\n$$\\\\begin{aligned}\\n\\\\label{eqn:cqeCF}\\n\\\\begin{split}\\n    CQE_{1}^{CF} &= 0.02 \\\\times \\\\log\\\\left(\\\\frac{\\\\sigma_{rg}^2}{|\\\\mu_{rg}|^{0.2}}\\\\right) \\\\times \\\\log\\\\left(\\\\frac{\\\\sigma_{yb}^2}{|\\\\mu_{yb}|^{0.2}}\\\\right)\\n\\\\\\\\\\n    CQE_{2}^{CF} &= 0.02 \\\\times \\\\frac{\\\\log(\\\\sigma_{rg}^2) \\\\times \\\\log(\\\\sigma_{yb}^2)}{\\\\log(\\\\sigma_c^2)} \\\\times \\\\frac{\\\\log(\\\\mu_{rg}^2) \\\\times \\\\log(\\\\mu_{yb}^2)}{log(\\\\mu_c^2)}\\n\\\\end{split}\\n\\\\end{aligned}$$\\nwhere v_(rg) and v_(yb) are concatenated to form v_(c), i.e.,\\xa0v_(c)\\u2004=\\u2004[v_(rg),v_(yb)].\\n\\nTo compute CF_(Yendrikhovskij), we use the following formula after the RGB to CIE L*u*v* color space transform, as specified in the paper\\xa0:\\n$$\\\\begin{aligned}\\n\\\\label{eqn:yendrikCF}\\n\\\\begin{split}\\n    S_{u^{*}v^{*}} &= \\\\frac{ \\\\sqrt{ ({{u^{*}}^2 + {v^{*}}^2}) } }{L^{*}+\\\\epsilon}, \\\\epsilon \\\\neq 0\\n\\\\\\\\\\n    CF_\\\\text{Yendrikhovskij} &= \\\\mu_{S_{u^{*}v^{*}}} + \\\\sigma_{S_{u^{*}v^{*}}}\\n\\\\end{split}\\n\\\\end{aligned}$$\\n\\n  ---------------------- ------- -------\\n  Colorfulness Metric      PCC    SROCC\\n  CF_(Hasler)\\xa0            0.841   0.884\\n  CQE₁^(CF)\\xa0              0.895   0.896\\n  CQE₂^(CF)\\xa0              0.312   0.415\\n  CF_(Yendrikhovskij)\\xa0    0.843   0.834\\n  ColorNet-Mobile         0.841   0.774\\n  ColorNet-ResNet         0.916   0.889\\n  ColorNet-VGG            0.937   0.921\\n                                 \\n  ---------------------- ------- -------\\n\\n  : Correlation Coefficient Results.\\n\\nQuantitative Evaluation: Colorfulness metrics are evaluated by computing the Pearson correlation coefficient (PCC) and the Spearman rank-ordered correlation coefficient (SROCC). For testing, we used the 10-fold cross-validation strategy where the whole dataset was divided into 10 non-overlapping pieces 𝒫, and for each iteration, one piece is used for the test, one piece used for validation, and the remaining pieces are used for training (as also described above). For each iteration itr, the unique piece 𝒫_(itr) is used for the test, where all the pieces (𝒫_(itr), itr\\u2004∈\\u2004[1,10]) cover the whole dataset. Finally, the PCC and SROCC results for 10 different test cases are averaged. The performance results are presented in Table\\xa01 where PCC and SROCC for the 4 state-of-the-art colorfulness metrics are computed using the aligned subjective scores from ‘Combined’ dataset. Our proposed ColorNet model with VGG and ResNet based feature network outperforms over the other classical models. This is partly due to the model’s ability to learn rich high-level color feature representation.\\n\\nQualitative Evaluation: For the qualitative evaluation of ColorNet, we crafted a set of images that are not used during the training of the model, by considering two different scenarios: i) change in the dominant colours and ii) change in the color contrast. In Fig.\\xa01, row I shows 4 different cases of the change in the dominant colors, and row II shows the change in the color contrast. We report the objective results for the proposed deep-learning-based colorfulness estimation method, in Fig.\\xa01, and show also CF_(Hasler) for completeness. The results showcase that in both cases i.e.,\\xa0by increasing color contrast and increasing the number of dominant hues, the proposed metric scores increase, thus, validating the understanding of colorfulness of our model. Overall, our results confirm that learning-based models bring huge potential to cater for various implicit aspects of color perception over a wide variety of natural images.\\n\\n[Qualitative Evaluation. Row I depicts the change in dominant colors. Row II depicts the change in the saturation of the colors.]\\n\\nConclusion\\n\\nIn this study, we propose a CNN based model for the estimation of colorfulness ratings. To prepare a well-annotated colored image dataset, we combine two colorfulness databases with subjective user scores, using the results of an anchor subjective experiment with a common subset of images. We compare the results of the proposed model to those of four other traditional colorfulness metrics quantitatively and qualitatively where we observe that our learning-based model effectively rates the colorfulness by catering for the wide variety of natural images. This study constitutes an initial step towards the exploration of color perception in natural images using the deep learning approach. In future work, we aim to delve deeper in learning-based color perception models and analyze the impacts of various associated factors.\\n\\n\\nPlease answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[10]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9211c47-ea40-47b4-9a64-3b19654201bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(f\"Data-to-Text Generation\", valid_data[10]['prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7a1262-fe0c-49fd-a690-7f2317869148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "similarity = fuzz.ratio(\"Data-to-Text Generation\", \"Text Generation\")\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938b94d-b114-4aaf-bf71-e2d5e6821a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e008db-7ce2-466d-86fc-8ddb96a60967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  Title:\\t Dark Model Adaptation: Semantic Image...   \n",
      "1  Title:\\tGeometry Normalization Networks for Ac...   \n",
      "2  Title:\\tA Better Baseline for AVA\\n\\nAbstract:...   \n",
      "3  Title:\\tContextNet: Exploring Context and Deta...   \n",
      "4  Title:\\tGlossBERT: BERT for Word Sense Disambi...   \n",
      "\n",
      "                                              answer  __index_level_0__  \n",
      "0  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  0  \n",
      "1  [{'LEADERBOARD': {'Task': 'Scene Text Detectio...                  1  \n",
      "2  [{'LEADERBOARD': {'Task': 'Action Recognition'...                  2  \n",
      "3  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  3  \n",
      "4  [{'LEADERBOARD': {'Task': 'Word Sense Disambig...                  4  \n"
     ]
    }
   ],
   "source": [
    "dfs = {split: dataset.to_pandas() for split, dataset in dataset_dict.items()}\n",
    "\n",
    "print(dfs['validation'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09af5cb-0cc9-4405-a097-06d1b973e029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title:\\t Dark Model Adaptation: Semantic Image...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Semantic Segmentati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title:\\tGeometry Normalization Networks for Ac...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Scene Text Detectio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title:\\tA Better Baseline for AVA\\n\\nAbstract:...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Action Recognition'...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title:\\tContextNet: Exploring Context and Deta...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Semantic Segmentati...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title:\\tGlossBERT: BERT for Word Sense Disambi...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Word Sense Disambig...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Title:\\t Dark Model Adaptation: Semantic Image...   \n",
       "1  Title:\\tGeometry Normalization Networks for Ac...   \n",
       "2  Title:\\tA Better Baseline for AVA\\n\\nAbstract:...   \n",
       "3  Title:\\tContextNet: Exploring Context and Deta...   \n",
       "4  Title:\\tGlossBERT: BERT for Word Sense Disambi...   \n",
       "\n",
       "                                              answer  __index_level_0__  \n",
       "0  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  0  \n",
       "1  [{'LEADERBOARD': {'Task': 'Scene Text Detectio...                  1  \n",
       "2  [{'LEADERBOARD': {'Task': 'Action Recognition'...                  2  \n",
       "3  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  3  \n",
       "4  [{'LEADERBOARD': {'Task': 'Word Sense Disambig...                  4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['validation'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68b7716-02c4-47df-be65-722e3851c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['train'][\"prompt_lenght\"] = dfs['train'][\"prompt\"].apply(lambda x: len(x.split()))\n",
    "dfs['train'][\"answer_lenght\"] = dfs['train'][\"answer\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "dfs['validation'][\"prompt_lenght\"] = dfs['validation'][\"prompt\"].apply(lambda x: len(x.split()))\n",
    "dfs['validation'][\"answer_lenght\"] = dfs['validation'][\"answer\"].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71239b3-9ae7-4b52-bb39-548d08813963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>prompt_lenght</th>\n",
       "      <th>answer_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3075.000000</td>\n",
       "      <td>3075.000000</td>\n",
       "      <td>3075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1537.000000</td>\n",
       "      <td>1670.739187</td>\n",
       "      <td>55.449756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>887.820365</td>\n",
       "      <td>1149.200513</td>\n",
       "      <td>104.680918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>768.500000</td>\n",
       "      <td>785.500000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1537.000000</td>\n",
       "      <td>1682.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2305.500000</td>\n",
       "      <td>2289.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3074.000000</td>\n",
       "      <td>16491.000000</td>\n",
       "      <td>2561.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       __index_level_0__  prompt_lenght  answer_lenght\n",
       "count        3075.000000    3075.000000    3075.000000\n",
       "mean         1537.000000    1670.739187      55.449756\n",
       "std           887.820365    1149.200513     104.680918\n",
       "min             0.000000      48.000000       1.000000\n",
       "25%           768.500000     785.500000      11.000000\n",
       "50%          1537.000000    1682.000000      27.000000\n",
       "75%          2305.500000    2289.000000      64.000000\n",
       "max          3074.000000   16491.000000    2561.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['train'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b55afea-623d-4571-b065-e1a45cbbb4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>prompt_lenght</th>\n",
       "      <th>answer_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1298.000000</td>\n",
       "      <td>1298.000000</td>\n",
       "      <td>1298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>648.500000</td>\n",
       "      <td>1642.204931</td>\n",
       "      <td>54.721109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>374.844634</td>\n",
       "      <td>1097.751424</td>\n",
       "      <td>94.978970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>324.250000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>648.500000</td>\n",
       "      <td>1656.000000</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>972.750000</td>\n",
       "      <td>2249.750000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1297.000000</td>\n",
       "      <td>10590.000000</td>\n",
       "      <td>1870.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       __index_level_0__  prompt_lenght  answer_lenght\n",
       "count        1298.000000    1298.000000    1298.000000\n",
       "mean          648.500000    1642.204931      54.721109\n",
       "std           374.844634    1097.751424      94.978970\n",
       "min             0.000000      49.000000       1.000000\n",
       "25%           324.250000     801.000000      11.000000\n",
       "50%           648.500000    1656.000000      26.000000\n",
       "75%           972.750000    2249.750000      64.000000\n",
       "max          1297.000000   10590.000000    1870.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['validation'].describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e35b56d3-814d-4ebc-b454-cd7ba08cb456",
   "metadata": {},
   "source": [
    "dataset_path = f\"../data/LLLM_DOCTEAT_TDMS_SQUAD_{i}/fold1\"\n",
    "\n",
    "    Train: 5512 = Old 3753 + 1759 (some short context no leaderboard removed)\n",
    "    Test: 2353 = Old 1608 + 745 (no leaderboard)\n",
    "\n",
    "\n",
    "dataset_path = \"../data/LLLM_DOCTEAT_TDMS_ALL_TEMPLATE/fold1\"\n",
    "\n",
    "    Train: 82680\n",
    "    Test: 35295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f66fe-a9a8-43db-b26f-b7b721830a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9611be63-589f-4578-9184-29869055cc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35295"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2353*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1aa5a74-53a7-457d-99e7-4bf285371ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4822"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1876+2946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a342613a-57b4-4eb3-9c71-e9b49e4258e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5629"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1876+3753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8faa250f-498c-4012-ad33-e722d3aae246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5512 - 3753"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66be50c4-077f-4aa7-a835-3bf9fb736d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2353 - 1608"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246b136-01eb-4dfd-8372-44a343486414",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf0de0fc-79a9-4ae1-bd85-869f65a85cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def describe_list(lst):\n",
    "    # Count\n",
    "    count = len(lst)\n",
    "    \n",
    "    # Mean\n",
    "    mean = np.mean(lst)\n",
    "    \n",
    "    # Standard deviation\n",
    "    std = np.std(lst)\n",
    "    \n",
    "    # Minimum\n",
    "    min_val = np.min(lst)\n",
    "    \n",
    "    # 25th Percentile\n",
    "    percentile_25 = np.percentile(lst, 25)\n",
    "    \n",
    "    # Median\n",
    "    median = np.median(lst)\n",
    "    \n",
    "    # 75th Percentile\n",
    "    percentile_75 = np.percentile(lst, 75)\n",
    "    \n",
    "    # Maximum\n",
    "    max_val = np.max(lst)\n",
    "    \n",
    "    result = {\n",
    "        'count': count,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'min': min_val,\n",
    "        '25%': percentile_25,\n",
    "        '50%': median,\n",
    "        '75%': percentile_75,\n",
    "        'max': max_val\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_df_stats(df): \n",
    "    \n",
    "    unique_labels = df.answer.tolist()\n",
    "    \n",
    "    TDMS = set()\n",
    "    Uniq_task = set()\n",
    "    Uniq_dataset = set()\n",
    "    Uniq_metric = set()\n",
    "    Uniq_score = set()\n",
    "    unanswerable_count = 0\n",
    "    tdms_per_paper = []\n",
    "    \n",
    "    for contrib in unique_labels:\n",
    "        try:\n",
    "            parsed_json = ast.literal_eval(contrib)\n",
    "        except :\n",
    "            # TODO: We need a best way to deal with this \n",
    "\n",
    "            if contrib == 'unanswerable':\n",
    "                unanswerable_count += 1\n",
    "            else:\n",
    "                ipdb.set_trace()\n",
    "                \n",
    "                print(f\"Error parsing: \\n{contrib}\")\n",
    "                # missed += 1\n",
    "                parsed_json =  contrib\n",
    "        tdms_count = 0\n",
    "        for leaderboard in parsed_json:\n",
    "\n",
    "            task = leaderboard['LEADERBOARD']['Task'].strip()\n",
    "            dataset = leaderboard['LEADERBOARD']['Dataset'].strip()\n",
    "            metric = leaderboard['LEADERBOARD']['Metric'].strip()\n",
    "            score = leaderboard['LEADERBOARD']['Score'].strip()\n",
    "\n",
    "            TDMS.add(f\"{task}#{dataset}#{metric}#{score}\")\n",
    "            \n",
    "            Uniq_task.add(task)\n",
    "            Uniq_dataset.add(dataset)\n",
    "            Uniq_metric.add(metric)\n",
    "            Uniq_score.add(score)\n",
    "            tdms_count += 1\n",
    "            # ipdb.set_trace()\n",
    "        \n",
    "        tdms_per_paper.append(tdms_count)\n",
    "        \n",
    "        \n",
    "    print(f\"Number of papers: {len(unique_labels)}\")\n",
    "    print(f\"Unanswerable count: {unanswerable_count}\")\n",
    "    # print(f\"Total leaderboards: {len(path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown')].title.tolist())}\")\n",
    "    print(f\"Avg leaderboard per paper: {round(np.mean(list(tdms_per_paper)), 2)}\")\n",
    "    print(f\"Distinc leaderboard: {len(TDMS)}\")\n",
    "    print(f\"Distinct taks: {len(Uniq_task)}\")\n",
    "    print(f\"Distinc datasets: {len(Uniq_dataset)}\")\n",
    "    print(f\"Distinc metrics: {len(Uniq_metric)}\")\n",
    "    print(f\"Distinc score: {len(Uniq_score)}\")\n",
    "    print(f\"Max leaderboard per paper: {round(np.max(list(tdms_per_paper)), 2)}\")\n",
    "    print(f\"Min leaderboard per paper: {round(np.min(list(tdms_per_paper)), 2)}\")\n",
    "\n",
    "    describe_result = describe_list(tdms_per_paper)\n",
    "\n",
    "    print(f\"\\nResults describe TDMS:\")\n",
    "    for key, value in describe_result.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    return tdms_per_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d8e9311-0d63-4d2f-ad9a-556a72ccc5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>prompt_lenght</th>\n",
       "      <th>answer_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Semantic Segmentati...</td>\n",
       "      <td>0</td>\n",
       "      <td>353</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Geometry Normalization Networks for Accurate S...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Scene Text Detectio...</td>\n",
       "      <td>1</td>\n",
       "      <td>441</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Better Baseline for AVA We introduce a simpl...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Action Recognition'...</td>\n",
       "      <td>2</td>\n",
       "      <td>526</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ContextNet: Exploring Context and Detail for S...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Semantic Segmentati...</td>\n",
       "      <td>3</td>\n",
       "      <td>424</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Efficient Piecewise Training of Deep Structure...</td>\n",
       "      <td>[{'LEADERBOARD': {'Task': 'Semantic Segmentati...</td>\n",
       "      <td>4</td>\n",
       "      <td>406</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Dark Model Adaptation: Semantic Image Segmenta...   \n",
       "1  Geometry Normalization Networks for Accurate S...   \n",
       "2  A Better Baseline for AVA We introduce a simpl...   \n",
       "3  ContextNet: Exploring Context and Detail for S...   \n",
       "4  Efficient Piecewise Training of Deep Structure...   \n",
       "\n",
       "                                              answer  __index_level_0__  \\\n",
       "0  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  0   \n",
       "1  [{'LEADERBOARD': {'Task': 'Scene Text Detectio...                  1   \n",
       "2  [{'LEADERBOARD': {'Task': 'Action Recognition'...                  2   \n",
       "3  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  3   \n",
       "4  [{'LEADERBOARD': {'Task': 'Semantic Segmentati...                  4   \n",
       "\n",
       "   prompt_lenght  answer_lenght  \n",
       "0            353             11  \n",
       "1            441             75  \n",
       "2            526             24  \n",
       "3            424             11  \n",
       "4            406             24  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['validation'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c2c22c-528f-416b-94a1-9364e140636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 5512\n",
      "Unanswerable count: 1872\n",
      "Avg leaderboard per paper: 4.15\n",
      "Distinc leaderboard: 20102\n",
      "Distinct taks: 699\n",
      "Distinc datasets: 2551\n",
      "Distinc metrics: 1280\n",
      "Distinc score: 9833\n",
      "Max leaderboard per paper: 254\n",
      "Min leaderboard per paper: 1\n",
      "\n",
      "Results describe TDMS:\n",
      "count: 5512\n",
      "mean: 4.145500725689405\n",
      "std: 8.27826236580898\n",
      "min: 1\n",
      "25%: 1.0\n",
      "50%: 2.0\n",
      "75%: 4.0\n",
      "max: 254\n"
     ]
    }
   ],
   "source": [
    "tdms_per_paper_train = get_df_stats(dfs['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a27ce95d-a412-45d2-95fd-45bbf543952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 2353\n",
      "Unanswerable count: 804\n",
      "Avg leaderboard per paper: 15.4\n",
      "Distinc leaderboard: 8691\n",
      "Distinct taks: 497\n",
      "Distinc datasets: 1639\n",
      "Distinc metrics: 886\n",
      "Distinc score: 5116\n",
      "Max leaderboard per paper: 170\n",
      "Min leaderboard per paper: 1\n",
      "\n",
      "Results describe TDMS:\n",
      "count: 2353\n",
      "mean: 15.399915002124947\n",
      "std: 15.016051476594187\n",
      "min: 1\n",
      "25%: 2.0\n",
      "50%: 7.0\n",
      "75%: 34.0\n",
      "max: 170\n"
     ]
    }
   ],
   "source": [
    "tdms_per_paper_validation = get_df_stats(dfs['validation'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "391ebdb0-82a1-44b1-bc17-46c590fcffd1",
   "metadata": {},
   "source": [
    "Fold 1\n",
    "\n",
    "    Train:\n",
    "        Number of papers: 5512\n",
    "        Unanswerable count: 1872\n",
    "        Avg leaderboard per paper: 4.15\n",
    "        Distinc leaderboard: 20102\n",
    "        Distinct taks: 699\n",
    "        Distinc datasets: 2551\n",
    "        Distinc metrics: 1280\n",
    "        Distinc score: 9833\n",
    "        Max leaderboard per paper: 254\n",
    "        Min leaderboard per paper: 1\n",
    "        \n",
    "        Results describe TDMS:\n",
    "        count: 5512\n",
    "        mean: 4.145500725689405\n",
    "        std: 8.27826236580898\n",
    "        min: 1\n",
    "        25%: 1.0\n",
    "        50%: 2.0\n",
    "        75%: 4.0\n",
    "        max: 254\n",
    "\n",
    "    Validation:\n",
    "        Number of papers: 2353\n",
    "        Unanswerable count: 804\n",
    "        Avg leaderboard per paper: 15.4\n",
    "        Distinc leaderboard: 8691\n",
    "        Distinct taks: 497\n",
    "        Distinc datasets: 1639\n",
    "        Distinc metrics: 886\n",
    "        Distinc score: 5116\n",
    "        Max leaderboard per paper: 170\n",
    "        Min leaderboard per paper: 1\n",
    "        \n",
    "        Results describe TDMS:\n",
    "        count: 2353\n",
    "        mean: 15.399915002124947\n",
    "        std: 15.016051476594187\n",
    "        min: 1\n",
    "        25%: 2.0\n",
    "        50%: 7.0\n",
    "        75%: 34.0\n",
    "        max: 170\n",
    "\n",
    "\n",
    "Fold 2\n",
    "\n",
    "    Train:\n",
    "        Number of papers: 5513\n",
    "        Unanswerable count: 1872\n",
    "        Avg leaderboard per paper: 4.52\n",
    "        Distinc leaderboard: 20346\n",
    "        Distinct taks: 708\n",
    "        Distinc datasets: 2570\n",
    "        Distinc metrics: 1286\n",
    "        Distinc score: 9805\n",
    "        Max leaderboard per paper: 254\n",
    "        Min leaderboard per paper: 1\n",
    "        \n",
    "        Results describe TDMS:\n",
    "        count: 5513\n",
    "        mean: 4.521676038454562\n",
    "        std: 8.144238205416363\n",
    "        min: 1\n",
    "        25%: 2.0\n",
    "        50%: 2.0\n",
    "        75%: 4.0\n",
    "        max: 254\n",
    "\n",
    "    Validation:\n",
    "        Number of papers: 2352\n",
    "        Unanswerable count: 804\n",
    "        Avg leaderboard per paper: 4.04\n",
    "        Distinc leaderboard: 8474\n",
    "        Distinct taks: 477\n",
    "        Distinc datasets: 1597\n",
    "        Distinc metrics: 894\n",
    "        Distinc score: 5162\n",
    "        Max leaderboard per paper: 146\n",
    "        Min leaderboard per paper: 1\n",
    "        \n",
    "        Results describe TDMS:\n",
    "        count: 2352\n",
    "        mean: 4.038265306122449\n",
    "        std: 7.181578265158172\n",
    "        min: 1\n",
    "        25%: 1.0\n",
    "        50%: 2.0\n",
    "        75%: 4.0\n",
    "        max: 146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "afdb5ca0-1b9b-47dd-ba3e-94a7f9e51da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.33"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((4.15+4.52)/2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9e6e4b55-0cac-4bae-ace4-eb77396d47f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.72"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((15.4+4.04)/2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e7049-3ddf-41e7-aa6a-d9556d992f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(np.max(list(tdms_per_paper)), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f6cdd-c817-4243-91ca-3a19f64d0bf8",
   "metadata": {},
   "source": [
    "## Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e98b32-62b2-4954-ae15-a69b3945d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffad128-2a34-4942-b642-e44a7ed1aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = [\"2105.02723v1\",\"2104.06378v1\",\"2104.04946v1\",\"1912.03330v1\", \"1912.02738v4\",\"1912.01326v3\", \"1912.00998v2\"]\n",
    "latex = f\"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/{id_[0]}.tex\"\n",
    "# latex = \"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/1312.6114v10.tex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993bce4f-c010-4263-8820-655f7b7acc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_sections(latex_file):\n",
    "    \n",
    "    if len(latex_file.rsplit(\"/\",1)) != 2:\n",
    "        return \n",
    "    \n",
    "    base_source, filename = latex_file.rsplit(\"/\",1)\n",
    "\n",
    "    if len(filename.rsplit(\".\",1)) != 2:\n",
    "        return \n",
    "    \n",
    "    file_id, file_ext = filename.rsplit(\".\",1)\n",
    "    \n",
    "    \n",
    "    # Read the input LaTeX file\n",
    "    with open(latex_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # # Define the pattern to match sections to be removed\n",
    "    # # This pattern looks for the string \\section{Introduction} or \\section{Experiment setup}\n",
    "    # # followed by any content until the next occurrence of \\section or the end of the file\n",
    "    # pattern = re.compile(\n",
    "    #     r'\\\\section\\{Introduction\\}.*?(?=\\\\section|\\Z)|'\n",
    "    #     r'\\\\section\\{Related work\\}.*?(?=\\\\section|\\Z)|'\n",
    "    #     r'\\\\section\\{Experiment setup\\}.*?(?=\\\\section|\\Z)',\n",
    "    #     re.DOTALL\n",
    "    # )\n",
    "\n",
    "    # Define the pattern to match sections to be removed\n",
    "    # This pattern looks for the string \\section, followed by any number of characters,\n",
    "    # followed by either 'Introduction' or 'Experiment setup', followed by any characters\n",
    "    # until the next occurrence of \\section or the end of the file.\n",
    "    # \\s* matches any whitespace characters, and [^}]* matches any character except '}'\n",
    "    pattern = re.compile(\n",
    "        r'\\\\section\\*?\\s*\\{[^}]*\\b(Introduction(s?)|Related work(s?)|Future work(s?)|Background(s?)|Discussion(s?)|Methodology|Appendix|Supplementary|Supplemental)\\b[^}]*\\}.*?(?=\\\\section|\\\\end\\{document\\}|\\\\bibliography|\\Z)',\n",
    "        # r'\\\\section\\s*\\{[^}]*\\b(Introduction|Related work|Future work)\\b[^}]*\\}.*?(?=\\\\section|\\Z)',\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "\n",
    "    # pattern = re.compile(\n",
    "    #     r'''\n",
    "    #     (                             # Start capturing group\n",
    "    #         \\\\section                 # Match \\section\n",
    "    #         \\*?                       # Match optional *\n",
    "    #         \\s*                       # Match optional whitespace\n",
    "    #         \\{                        # Match {\n",
    "    #         (?!                       # Start negative lookahead\n",
    "    #             Result(s?)               # Negative lookahead for Results\n",
    "    #             |                     # Or\n",
    "    #             Experimentation(s?)       # Negative lookahead for Experimentation\n",
    "    #             |\n",
    "    #             Experiment(s?)\n",
    "    #             |\n",
    "    #             Conclusion\n",
    "    #         )                         # End negative lookahead\n",
    "    #         [^}]*                     # Match any characters except }\n",
    "    #         \\}                        # Match }\n",
    "    #         .*?                       # Match any characters (non-greedy)\n",
    "    #         (?=\\\\section|\\Z)          # Positive lookahead for next \\section or end of string\n",
    "    #     )                             # End capturing group\n",
    "    #     ''',\n",
    "    #     re.DOTALL | re.IGNORECASE | re.VERBOSE\n",
    "    # )\n",
    "\n",
    "\n",
    "\n",
    "    # Remove the matched content\n",
    "    content_new = re.sub(pattern, '', content)\n",
    "\n",
    "    if not os.path.exists(f\"{base_source}/edits\"):\n",
    "        os.makedirs(f\"{base_source}/edits\")\n",
    "        \n",
    "    if os.path.exists(f\"{base_source}/edits/{file_id}_edit.{file_ext}\"):\n",
    "        os.remove(f\"{base_source}/edits/{file_id}_edit.{file_ext}\")\n",
    "            \n",
    "    # Write the modified content back to the file\n",
    "    with open(f\"{base_source}/edits/{file_id}_edit.{file_ext}\", 'w', encoding='utf-8') as file:\n",
    "        file.write(content_new)\n",
    "\n",
    "# Replace 'yourfile.tex' with the path to your LaTeX file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351a86b8-d14a-4b4f-a8d3-ca07ca2f3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = \"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/sample\"\n",
    "for file_id in os.listdir(f\"{source_folder}\"):\n",
    "    latex_file = f\"{source_folder}/{file_id}\"\n",
    "    remove_sections(latex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de03e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = [\"2105.02723v1\",\"2104.06378v1\",\"2104.04946v1\",\"1912.03330v1\", \"1912.02738v4\",\"1912.01326v3\", \"1912.00998v2\"]\n",
    "latex_file = f\"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/{id_[0]}.tex\"\n",
    "# latex = \"/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/data_proccess/1312.6114v10.tex\"\n",
    "\n",
    "def compile_latex(latex_file):\n",
    "    # Compile the LaTeX file using pdflatex or any other LaTeX compiler\n",
    "    subprocess.run([\"pdflatex\", latex_file])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143c4a37-2860-4f7a-8385-de857bcfce74",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pdflatex': 'pdflatex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-171a1bc72c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcompile_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatex_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-7bb624957d28>\u001b[0m in \u001b[0;36mcompile_latex\u001b[0;34m(latex_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile_latex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatex_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Compile the LaTeX file using pdflatex or any other LaTeX compiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pdflatex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatex_file\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tdm/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tdm/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    798\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    801\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tdm/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1549\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdflatex': 'pdflatex'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "# latex_file = 'your_latex_file.tex'\n",
    "\n",
    "\n",
    "compile_latex(latex_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aea75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
