{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes\n",
    "# !pip install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnkwYBQ6TWB4"
   },
   "source": [
    "# Fetching Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QnjddTNlS6Dz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 23:59:07.132044: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-10 23:59:08.202173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 23:59:10.306754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/nfs/home/kabenamualus/.local/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os, ipdb\n",
    "import random, evaluate\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Fine-Tune Llama2-7b on custom dataset\n",
    "import os, ipdb\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch, random\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\\\n",
    ", BitsAndBytesConfig, HfArgumentParser, TrainingArguments, TrainerCallback, pipeline\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "\n",
    "# from ../evaluation_metrics import Metrics\n",
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "from evaluation_metrics import Metrics, THRESHOLD\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # or \"true\", depending on your needs\n",
    "\n",
    "# pd.options.display.max_rows , pd.options.display.max_columns  = 100,100  \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: Optional[str] = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": \"the model name\"})\n",
    "    log_with: Optional[str] = field(default=\"wandb\", metadata={\"help\": \"use 'wandb' to log with wandb\"})\n",
    "\n",
    "    dataset_name: Optional[str] = field(default=\"lvwerra/stack-exchange-paired\", metadata={\"help\": \"the dataset name\"})\n",
    "    subset: Optional[str] = field(default=\"data/finetune\", metadata={\"help\": \"the subset to use\"})\n",
    "    split: Optional[str] = field(default=\"train\", metadata={\"help\": \"the split to use\"})\n",
    "    size_valid_set: Optional[int] = field(default=4000, metadata={\"help\": \"the size of the validation set\"})\n",
    "    streaming: Optional[bool] = field(default=True, metadata={\"help\": \"whether to stream the dataset\"})\n",
    "    shuffle_buffer: Optional[int] = field(default=5000, metadata={\"help\": \"the shuffle buffer size\"})\n",
    "    seq_length: Optional[int] = field(default=1024, metadata={\"help\": \"the sequence length\"})\n",
    "    num_workers: Optional[int] = field(default=4, metadata={\"help\": \"the number of workers\"})\n",
    "\n",
    "    max_steps: Optional[int] = field(default=500, metadata={\"help\": \"the maximum number of sgd steps\"})\n",
    "    logging_steps: Optional[int] = field(default=10, metadata={\"help\": \"the logging frequency\"})\n",
    "    save_steps: Optional[int] = field(default=10, metadata={\"help\": \"the saving frequency\"})\n",
    "    per_device_train_batch_size: Optional[int] = field(default=4, metadata={\"help\": \"the per device train batch size\"})\n",
    "    per_device_eval_batch_size: Optional[int] = field(default=1, metadata={\"help\": \"the per device eval batch size\"})\n",
    "    gradient_accumulation_steps: Optional[int] = field(default=2, metadata={\"help\": \"the gradient accumulation steps\"})\n",
    "    gradient_checkpointing: Optional[bool] = field(\n",
    "        default=True, metadata={\"help\": \"whether to use gradient checkpointing\"}\n",
    "    )\n",
    "    group_by_length: Optional[bool] = field(default=False, metadata={\"help\": \"whether to group by length\"})\n",
    "    packing: Optional[bool] = field(default=True, metadata={\"help\": \"whether to use packing for SFTTrainer\"})\n",
    "\n",
    "    lora_alpha: Optional[float] = field(default=16, metadata={\"help\": \"the lora alpha parameter\"})\n",
    "    lora_dropout: Optional[float] = field(default=0.05, metadata={\"help\": \"the lora dropout parameter\"})\n",
    "    lora_r: Optional[int] = field(default=8, metadata={\"help\": \"the lora r parameter\"})\n",
    "\n",
    "    learning_rate: Optional[float] = field(default=1e-4, metadata={\"help\": \"the learning rate\"})\n",
    "    lr_scheduler_type: Optional[str] = field(default=\"cosine\", metadata={\"help\": \"the lr scheduler type\"})\n",
    "    num_warmup_steps: Optional[int] = field(default=100, metadata={\"help\": \"the number of warmup steps\"})\n",
    "    weight_decay: Optional[float] = field(default=0.05, metadata={\"help\": \"the weight decay\"})\n",
    "    optimizer_type: Optional[str] = field(default=\"paged_adamw_32bit\", metadata={\"help\": \"the optimizer type\"})\n",
    "\n",
    "    output_dir: Optional[str] = field(default=\"./results\", metadata={\"help\": \"the output directory\"})\n",
    "    log_freq: Optional[int] = field(default=1, metadata={\"help\": \"the logging frequency\"})\n",
    "\n",
    "\n",
    "parser = HfArgumentParser(ScriptArguments)\n",
    "script_args = parser.parse_args_into_dataclasses([])[0]\n",
    "\n",
    "if script_args.group_by_length and script_args.packing:\n",
    "    raise ValueError(\"Cannot use both packing and group by length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ckpt = \"/checkpoint-4350\"\n",
    "# script_args.model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "# script_args.size = \"13b\"\n",
    "script_args.seq_length = 2400\n",
    "# script_args.seq_length = 1024\n",
    "\n",
    "\n",
    "# script_args.model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "script_args.model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "script_args.size = \"7b\"\n",
    "script_args.seq_length = 240\n",
    "\n",
    "# docteat_llama2_13b_tdm_f1_all_template_seq_len_1024\n",
    "\n",
    "i = 1\n",
    "# # script_args.test_dataset = f\"../data/LLLM_DOCTEAT_TDMS_SQUAD_{i}/fold1\"\n",
    "# # script_args.test_dataset = f\"../data/LLLM_DOCTEAT_TDMS_DROP_{i}/fold2\"\n",
    "# script_args.test_ckpt = \"best_checkpoint\"\n",
    "# # script_args.test_ckpt = \"checkpoint-37500\"\n",
    "# # script_args.test_ckpt = \"checkpoint-25000\"\n",
    "# # script_args.test_ckpt = \"checkpoint-15000\"\n",
    "# # script_args.test_ckpt = \"checkpoint-35500\"\n",
    "\n",
    "# script_args.dataset_name = \"../data/LLLM_DOCTEAT_TDM_ALL_TEMPLATE/fold1\"\n",
    "# script_args.output_dir = f\"../model_ckpt/docteat_flan_t5_large_tdms_f1_all_template\"\n",
    "# script_args.run_name = f\"Evaluate_sft_docteat_llama2_tdms_f1_all_template\"\n",
    "# # script_args.test_ckpt = \"checkpoint-103350\"\n",
    "# script_args.test_ckpt = \"checkpoint-82680\"\n",
    "\n",
    "# script_args.output_dir = f\"../model_ckpt{ckpt}\"\n",
    "# script_args.run_name = \"sft_llama2_docteat_tdm_f2_all_Template\"\n",
    "\n",
    "# script_args.dataset_name = \"../data/LLLM_DOCTEAT_TDMS_ALL_TEMPLATE/fold1\"\n",
    "# script_args.output_dir = f\"../model_ckpt/docteat_llama2_{script_args.size}_tdms_f1_all_template_seq_len_{script_args.seq_length}\"\n",
    "# script_args.run_name = f\"Evaluate_sft_docteat_llama2_{script_args.size}_tdms_f1_all_template_seq_len_{script_args.seq_length}\"\n",
    "# script_args.per_device_train_batch_size = 2\n",
    "# script_args.gradient_accumulation_steps = 2\n",
    "# script_args.per_device_eval_batch_size = 2\n",
    "\n",
    "# multi GPU\n",
    "\n",
    "script_args.test_dataset = f\"../data/LLLM_LONG_TDMS_SQUAD_{i}/fold1\"\n",
    "# script_args.test_ckpt = \"checkpoint-17000\"\n",
    "script_args.test_ckpt = \"checkpoint-3000\"\n",
    "# script_args.seq_length = 2400\n",
    "script_args.dataset_name = \"../data/LLLM_LONG_TDMS_ALL_TEMPLATE/fold1\"\n",
    "script_args.output_dir = f\"../model_ckpt/long_llama2_{script_args.size}_tdms_f1_all_template_seq_len_{script_args.seq_length}\"\n",
    "script_args.run_name = f\"Evaluate_sft_long_llama2_{script_args.size}_tdms_f1_all_template_seq_len_{script_args.seq_length}\"\n",
    "script_args.per_device_train_batch_size = 6\n",
    "script_args.gradient_accumulation_steps = 2\n",
    "\n",
    "# script_args.output_dir = f\"../model_ckpt/long_tdms_f1_all_template{ckpt}\"\n",
    "# script_args.run_name = \"sft_llama2_long_tdms_f1_all_Template\"\n",
    "# script_args.seq_length = 2400\n",
    "# script_args.per_device_train_batch_size = 1\n",
    "# script_args.gradient_accumulation_steps = 1\n",
    "\n",
    "script_args.save_steps = 5000\n",
    "script_args.eval_steps = 5000\n",
    "script_args.evaluation_strategy = 5000\n",
    "script_args.logging_steps = 5000\n",
    "script_args.streaming = False\n",
    "script_args.num_train_epochs = 15\n",
    "script_args.save_total_limit = 50\n",
    "\n",
    "script_args.save_strategy = \"steps\" #\"epoch\"\n",
    "script_args.evaluation_strategy= \"steps\" #\"epoch\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
    "    \"\"\"\n",
    "    Estimate the average number of characters per token in the dataset.\n",
    "    \"\"\"\n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = prepare_sample_text(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "\n",
    "    return total_characters / total_tokens\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_sample_text(example):\n",
    "    \"\"\"Prepare the text from a sample of the dataset.\"\"\"\n",
    "    text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['answer']}\"\n",
    "    # text = f\"{example['prompt']}\\n{example['answer']}\"\n",
    "    return text\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)  # type: ignore\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # type: ignore\n",
    "    \n",
    "    generate_kwargs = dict(\n",
    "        input_ids=preds,\n",
    "        temperature=0.2, \n",
    "        top_p=0.95, \n",
    "        top_k=40,\n",
    "        max_new_tokens=500,\n",
    "        repetition_penalty=1.3\n",
    "    )\n",
    "    ipdb.set_trace()\n",
    "    # outputs = base_model.generate(**generate_kwargs)\n",
    "    predictions = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)  # type: ignore\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    \n",
    "\n",
    "    results = Metrics.evaluate_property_wise_json_based(label_list=decoded_labels, prediction_list=decoded_preds)\n",
    "    results.update(Metrics.evaluate_rouge(label_list=decoded_labels, prediction_list=decoded_preds))\n",
    "        \n",
    "    return results\n",
    "    \n",
    "def create_datasets(tokenizer, args):\n",
    "    # dataset = load_dataset(\n",
    "    #     args.dataset_name,\n",
    "    #     data_dir=args.subset,\n",
    "    #     split=args.split,\n",
    "    #     use_auth_token=True,\n",
    "    #     num_proc=args.num_workers if not args.streaming else None,\n",
    "    #     streaming=args.streaming,\n",
    "    # )\n",
    "    \n",
    "    dataset = DatasetDict.load_from_disk(f\"{args.test_dataset}\")\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "    \n",
    "    # if args.streaming:\n",
    "    #     print(\"Loading the dataset in streaming mode\")\n",
    "    #     valid_data = dataset.take(args.size_valid_set)\n",
    "    #     train_data = dataset.skip(args.size_valid_set)\n",
    "    #     train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
    "    # else:\n",
    "    \n",
    "    # dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "    # valid_data = dataset[\"validation\"].shard(num_shards=10, index=0).shuffle(seed=42)\n",
    "\n",
    "    train_data = dataset[\"train\"]\n",
    "    valid_data = dataset[\"validation\"].shard(num_shards=20, index=0).shuffle(seed=42)\n",
    "    print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n",
    "\n",
    "    chars_per_token = chars_token_ratio(train_data, tokenizer, nb_examples=400)\n",
    "    # chars_per_token = chars_token_ratio(train_data, tokenizer, nb_examples=len(train_data)//2)\n",
    "    # 3.70\n",
    "    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        train_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=True,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    valid_dataset = ConstantLengthDataset(\n",
    "        tokenizer,\n",
    "        valid_data,\n",
    "        formatting_func=prepare_sample_text,\n",
    "        infinite=False,\n",
    "        seq_length=args.seq_length,\n",
    "        chars_per_token=chars_per_token,\n",
    "    )\n",
    "    return train_dataset, valid_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_ckpt/long_llama2_7b_tdms_f1_all_template_seq_len_3000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_ckpt/long_llama2_7b_tdms_f1_all_template_seq_len_3000/checkpoint-3000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{script_args.output_dir}/{script_args.test_ckpt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     script_args.model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map={\"\": 0},\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=\"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\",\n",
    "#     # use_auth_token=True,\n",
    "# )\n",
    "\n",
    "# base_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model,\n",
    "#     quantization_config=quant_config,\n",
    "#     device_map={\"\": 0}\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_ckpt/long_llama2_7b_tdms_f1_all_template_seq_len_3000/save_pretrained'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{script_args.output_dir}/save_pretrained\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoPeftModelForCausalLM.from_pretrained(f\"{script_args.output_dir}/{script_args.test_ckpt}\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# output_merged_dir = os.path.join(script_args.output_dir, \"final_merged_checkpoint\")\n",
    "# model.save_pretrained(output_merged_dir, safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     # f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "#     script_args.model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     # quantization_config=quant_config,\n",
    "#     device_map={\"\": 0},\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=\"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\",\n",
    "#     # use_auth_token=True,\n",
    "# )\n",
    "\n",
    "\n",
    "# # base_model.config.use_cache = False\n",
    "# # base_model.config.pretraining_tp = 1\n",
    "\n",
    "# base_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     # script_args.model_name,\n",
    "#     f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     load_in_4bit=True,\n",
    "#     use_auth_token=\"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\",\n",
    "#     # use_auth_token=\"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\"\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "#     use_auth_token=\"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\",\n",
    "# )\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_args.lora_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # peft_config = LoraConfig(\n",
    "# #     r=script_args.lora_r,\n",
    "# #     lora_alpha=script_args.lora_alpha,\n",
    "# #     lora_dropout=script_args.lora_dropout,\n",
    "# #     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "# #     bias=\"none\",\n",
    "# #     task_type=\"CAUSAL_LM\",\n",
    "# # )\n",
    "\n",
    "# peft_args = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=64,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\n",
    "# #     script_args.model_name, \n",
    "# #     use_auth_token=\"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\",\n",
    "# #     trust_remote_code=True\n",
    "# # )\n",
    "\n",
    "# # tokenizer.pad_token = tokenizer.eos_token\n",
    "# # tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# # tokenizer.add_tokens(AddedToken(\"{\", normalized=False))\n",
    "# # tokenizer.add_tokens(AddedToken(\"}\", normalized=False))\n",
    "\n",
    "\n",
    "# # # https://github.com/tatsu-lab/stanford_alpaca/issues/133#issuecomment-1483893538\n",
    "# # training_args = TrainingArguments(\n",
    "# #     output_dir=script_args.output_dir,\n",
    "# #     per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "# #     gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "# #     per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "# #     learning_rate=script_args.learning_rate,\n",
    "# #     logging_steps=script_args.logging_steps,\n",
    "# #     # max_steps=script_args.max_steps,\n",
    "# #     # report_to=script_args.log_with,\n",
    "# #     save_steps=script_args.save_steps,\n",
    "# #     evaluation_strategy=script_args.evaluation_strategy,\n",
    "# #     save_strategy=script_args.save_strategy,\n",
    "# #     eval_steps = script_args.eval_steps,\n",
    "# #     load_best_model_at_end=True,\n",
    "# #     save_total_limit=script_args.save_total_limit,\n",
    "# #     group_by_length=script_args.group_by_length,\n",
    "# #     lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "# #     warmup_steps=script_args.num_warmup_steps,\n",
    "# #     optim=script_args.optimizer_type,\n",
    "# #     # bf16=True,\n",
    "# #     fp16=True,\n",
    "# #     remove_unused_columns=False,\n",
    "# #     num_train_epochs = script_args.num_train_epochs,\n",
    "# #     run_name=script_args.run_name,\n",
    "# # )\n",
    "\n",
    "# # Set training parameters\n",
    "# training_params = TrainingArguments(\n",
    "#     output_dir=\"./results\",\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     optim=\"paged_adamw_32bit\",\n",
    "#     save_steps=25,\n",
    "#     logging_steps=25,\n",
    "#     learning_rate=2e-4,\n",
    "#     weight_decay=0.001,\n",
    "#     fp16=False,\n",
    "#     bf16=False,\n",
    "#     max_grad_norm=0.3,\n",
    "#     max_steps=-1,\n",
    "#     warmup_ratio=0.03,\n",
    "#     group_by_length=True,\n",
    "#     # lr_scheduler_type=\"constant\",\n",
    "#     # report_to=\"tensorboard\"\n",
    "# )\n",
    "\n",
    "\n",
    "# print(torch.__version__)\n",
    "\n",
    "# train_dataset, eval_dataset = create_datasets(tokenizer, script_args)\n",
    "\n",
    "# len(train_dataset)\n",
    "\n",
    "# num_gpus = torch.cuda.device_count()\n",
    "# print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "\n",
    "# # expected_steps = ((len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs)// num_gpus\n",
    "# # # expected_steps = (len(train_dataset) // (training_args.per_device_train_batch_size)) * training_args.num_train_epochs\n",
    "# # print(f\"Expected steps: {expected_steps}\")\n",
    "\n",
    "# # print(f\"Max token lenght: {tokenizer.model_max_length}\")\n",
    "# # print(f\"Test Batch size: {script_args.per_device_eval_batch_size * script_args.gradient_accumulation_steps * num_gpus }\")\n",
    "# # print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "# # # print(script_args)|\n",
    "# # trainer = SFTTrainer(\n",
    "# #     model=base_model,\n",
    "# #     train_dataset=train_dataset,\n",
    "# #     eval_dataset=eval_dataset,\n",
    "# #     peft_config=peft_config,\n",
    "# #     packing=script_args.packing,\n",
    "# #     # max_seq_length=None,\n",
    "# #     # max_seq_length=script_args.seq_length,\n",
    "# #     tokenizer=tokenizer,\n",
    "# #     args=training_args,\n",
    "# #     compute_metrics=compute_metrics,\n",
    "# # )\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model=base_model,\n",
    "#     train_dataset=train_dataset,\n",
    "#     peft_config=peft_args,\n",
    "#     dataset_text_field=\"text\",\n",
    "#     max_seq_length=None,\n",
    "#     tokenizer=tokenizer,\n",
    "#     args=training_params,\n",
    "#     packing=False,\n",
    "# )\n",
    "\n",
    "# # trainer.evaluate()\n",
    "\n",
    "# # Train model\n",
    "# # trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_ckpt/long_llama2_7b_tdms_f1_all_template_seq_len_3000/checkpoint-3000'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{script_args.output_dir}/{script_args.test_ckpt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca238a5d8174356ae77bbd580922732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbea91f2d434b3f96deca5a1a74b03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    # script_args.model_name,\n",
    "    f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    "    use_auth_token=\"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\",\n",
    "    # use_auth_token=\"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\"\n",
    ")\n",
    "\n",
    "# model.config.use_cache = False\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"{script_args.output_dir}/{script_args.test_ckpt}\",\n",
    "    use_auth_token=\"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\",\n",
    ")\n",
    "\n",
    "# model = model.cpu()\n",
    "# model = model.to(\"cpu\")\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# # model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict.load_from_disk(f\"{script_args.test_dataset}\")\n",
    "    \n",
    "    # if args.streaming:\n",
    "    #     print(\"Loading the dataset in streaming mode\")\n",
    "    #     valid_data = dataset.take(args.size_valid_set)\n",
    "    #     train_data = dataset.skip(args.size_valid_set)\n",
    "    #     train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
    "    # else:\n",
    "    \n",
    "# dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "train_data = dataset[\"train\"].shuffle(seed=42)\n",
    "valid_data = dataset[\"validation\"].shuffle(seed=42)\n",
    "# valid_data = dataset[\"validation\"].shard(num_shards=10, index=0).shuffle(seed=42)\n",
    "\n",
    "# train_data[0]\n",
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 102\n",
      "\n",
      "Question: Title\tFrom Big to Small: Multi-Scale Local Planar Guidance\n",
      "for Monocular Depth Estimation\n",
      "\n",
      "Abstract:\tEstimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoder-decoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction.\n",
      "\n",
      "In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method.\n",
      "\n",
      "Introduction\n",
      "\n",
      "Depth estimation from 2D images has been studied in computer vision for a long time and is nowadays applied to robotics, autonomous driving cars, scene understanding, and 3D reconstructions. Those applications usually utilize, to perform depth estimation, multiple instances of the same scene such as stereo image pairs , multiple frames from moving camera or static captures under different lighting conditions . As depth estimation from multiple observations achieves impressive progress, it naturally leads to depth estimation with a single image since it demands less cost and constraint.\n",
      "\n",
      "However, estimating accurate depth from a single image is challenging, even for a human, because it is an ill-posed problem as infinitely many 3D scenes can project to the same 2D scene. To understand geometric configuration from a single image, humans are considered to use not only local cues such as texture appearance in various lighting and occlusion conditions, perspective, or relative scales to the known objects but also global context such as entire shape or layout of the scene .\n",
      "\n",
      "After the first learning-based monocular depth estimation work from Saxena et al. was introduced, considerable improvements have been made along with rapid advances in deep learning . While most of the state-of-the-art works apply models based on deep convolutional neural networks (DCNNs) in supervised fashion, some works proposed semi- or self-supervised learning methods which do not entirely rely on the ground truth depth data.\n",
      "\n",
      "[Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.]\n",
      "\n",
      "[Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.]\n",
      "\n",
      "[Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.]\n",
      "\n",
      "[Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.]\n",
      "\n",
      "[Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.]\n",
      "\n",
      "[Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.]\n",
      "\n",
      "In the meantime, recent applications based on DCNNs are commonly composed in two parts: encoder for dense feature extraction and decoder for the desired prediction. As a dense feature extractor, very powerful deep networks such as VGG , ResNet or DenseNet are usually adopted. In these networks, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, which can be a bottleneck to obtain desired dense predictions. Therefore, a number of techniques, for example, multi-scale networks , skip connections or multi-layer deconvolutional networks are applied to consolidate feature maps from higher resolutions. Recently, atrous spatial pyramid pooling (ASPP) has been introduced for image semantic segmentation, which can capture large scale variations in observation by applying sparse convolutions with various dilation rates. Since the dilated convolution allows larger receptive field size, recent works in semantic segmentation or depth estimation do not fully reduce the receptive field size by removing last few pooling layers and reconfigure the network with atrous convolutions to reuse pre-trained weights. Consequently, their methods have larger dense features (1/8 of input spatial resolution whereas 1/32 or 1/64 in the original base networks) and perform almost all of the decoding process on that resolution followed by a simple upsampling to recover the original spatial resolution.\n",
      "\n",
      "[image]\n",
      "\n",
      "To define explicit relation in recovering back to the full resolution, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. Specifically, based on an encoding-decoding scheme, at each decoding stage, which has spatial resolutions of 1/8, 1/4, and 1/2, we place a layer that effectively guides input feature maps to the desired depth with local planar assumption. Then, we combine the outputs to predict depth in full resolution. This differs from multi-scale network or image pyramid approaches in two aspects. First, the outputs from the proposed layers are not treated as separated global depth estimation in corresponding resolutions. Instead, we let the layers to learn 4-dimensional plane coefficients and use them together to reconstruct depth estimations in the full resolution for the final estimation. Second, as a consequence of the nonlinear combination, individual spatial cells in each resolution are distinctively trained while the training progresses. We can see example outputs from the proposed layers in Figures 6 and 7. Experiments on the challenging NYU Depth V2 dataset and KITTI dataset demonstrate that the proposed method achieves state-of-the-art results.\n",
      "\n",
      "The rest of the paper is organized as follows. After a concise survey of related works in Section 2, we present in detail the proposed method in Section 3. Then, in Section 4, we provide results on two challenging benchmarks comparing with state-of-the-art works, and using various base networks as an encoder for the proposed network, we see how the performance varies along with each base network. In Section 4, we also provide an ablation study to validate the effectiveness of the proposed method. We conclude the paper in Section 5.\n",
      "\n",
      "Related Work\n",
      "\n",
      "Supervised Monocular Depth Estimation\n",
      "\n",
      "In monocular depth estimation, supervised approaches take a single image and use depth data measured with range sensors such as RGB-D cameras or multi-channel laser scanners as ground truth for supervision in training. Saxena et al. propose a learning-based approach to get a functional mapping from visual cues to depth via Markov random field, and extend it to a patch-based model that first over-segments the input image and learns 3D orientation as well as the location of local planes that are well explained by each patch . Eigen et al. introduce a multi-scale convolutional architecture that learns coarse global depth predictions on one network and progressively refine them using another network. Unlike the previous works in single image depth estimation, their network can learn representations from raw pixels without handcrafted features such as contours, super-pixels, or low-level segmentation. Several works follow the success of this approach by incorporating strong scene priors for surface normal estimation , using conditional random fields to improve accuracy or changing the learning problem from regression to classification . A recent supervised approach from Fu et al. achieves the state-of-the-art result by also taking advantage of changing the regression problem to quantized ordinal regression. Xu et al. propose an architecture that exploits multi-scale estimations derived from inner layers by fusing them within a CRF framework. Gan et al. propose to explicitly model the relationships between different image locations with an affinity layer. Most recently, Yin et al. introduce a method using virtual normal directions that are determined by randomly chosen three points in the reconstructed 3D space as geometric constraints.\n",
      "\n",
      "Semi-Supervised Monocular Depth Estimation\n",
      "\n",
      "There are also attempts to train a depth estimation network in a semi-supervised or weakly supervised fashion. Chen et al. propose a new approach that uses information of relative depth and depth ranking loss function to learn depth predictions in unconstrained images. Recently, to overcome the difficulty in getting high-quality depth data, Kuznietsov et al. introduce a semi-supervised method to train the network using both sparse LiDAR depth data for direct supervision and image alignment loss as a secondary training objective.\n",
      "\n",
      "Self-Supervised Monocular Depth Estimation\n",
      "\n",
      "The self-supervised approach refers to a method that requires only rectified stereo image pairs to train the depth estimation network. Garg et al. and Godard et al. propose self-supervised learning methods that smartly cast the problem from direct depth estimation to image reconstruction. Specifically, with a rectified stereo image pair, their networks try to synthesize one view from the other with estimated disparities and define the error between both as the reconstruction loss for the main training objective. In this way, because learning requires only well rectified, synchronized stereo pairs instead of the ground truth depth data well associated with the corresponding RGB images, it greatly reduces the effort to acquire datasets for new categories of scenes or environments. However, there is some accuracy gap when compared to the current best supervised approach . Garg et al. introduce an encoder-decoder architecture and to train the network using photometric reconstruction error. Xie et al. propose a network that also synthesizes one view from the other, and by using the reconstruction error, they produce a probability distribution of possible disparities for each pixel. Godard et al. finally propose a network architecture fully differentiable thus can perform end-to-end training. They also present a novel left-right consistency loss that improves training and predictions of the network. Most recently, Godard et al. propose a simple but effective architecture benefiting from associated design choices such as a robust reprojection loss, multi-scale sampling, and an auto-masking loss.\n",
      "\n",
      "Video-Based Monocular Depth Estimation\n",
      "\n",
      "There are also approaches using sequential data to perform the monocular depth estimation. Yin et al. propose an architecture consists of two generative sub-networks that are jointly trained by adversarial learning for disparity map estimation organized in a cycle to provide mutual constraints. Mahjourian et al. present an approach that explicitly considers the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. Wang et al. adopt a differentiable pose predictor and train a monocular depth estimation network in an end-to-end fashion while benefiting from the pose predictor.\n",
      "\n",
      "Method\n",
      "\n",
      "In this section, we describe the proposed monocular depth estimation network with a novel local planar guidance layer located on multiple stages in the decoding phase.\n",
      "\n",
      "Network Architecture\n",
      "\n",
      "As it can be seen from Figure [fg:architecture], we follow an encoding-decoding scheme that reduces feature map resolution to H/8 then recovers back to the original resolution H for dense prediction. After the backbone network that we use as a dense feature extractor which produces an H/8 feature map, we place a denser version of atrous spatial pyramid pooling layer as our contextual information extractor with various dilation rates r ∈ {3, 6, 12, 18, 24}. Then, at each stage in the decoding phase, where internal outputs are recovered to the full spatial resolution with a factor of 2, we employ the proposed local planar guidance (LPG) layer to locate geometric guidance to the desired depth estimation. We also place a 1×1 reduction layer to get the finest estimation c̃^(1 × 1) ∈ R^(H × W × 1) after the last upconv layer. Finally, outputs from the proposed layers (i.e., c̃^(k × k)) and c̃^(1 × 1) are concatenated and fed into the final convolutional layer to get the depth estimation d̃.\n",
      "\n",
      "[Examples showing behavior of the proposed network. Top and middle rows show c̃^(k × k) and their focused views. Bottom row shows the final depth estimation result with a focused view. The overestimated boundaries of the vehicle from lpg8x8 (yellow rectangle) and lpg4x4 (green rectangle) are compensated by the outputs from lpg2x2 (blue rectangle) and reduc1x1 (black rectangle) (i.e, c̃^(2 × 2) and c̃^(1 × 1)), resulting the clear boundary in the final estimation.]\n",
      "\n",
      "Multi-Scale Local Planar Guidance\n",
      "\n",
      "Our key idea in this work is to define direct and explicit relations between internal features and the final output in an effective manner. Unlike the existing methods that recover back to the original resolution using simple nearest neighbor upsampling layers and skip connections from encoding stages, we place novel local planar guidance layers which guide features to the full resolution with the local planar assumption and use them together to get the final depth estimation d̃. As can be seen from Figure [fg:architecture], since the proposed layer recovers given an internal feature map to the full resolution H, it can be used as a skip connection inside the decoding phase allowing direct relations between internal features and the final prediction. Specifically, given a feature map having spatial resolution H/k, the proposed layers estimate for each spatial cell a 4D plane coefficients that fit a locally defined k × k patch on the full resolution H, and they are concatenated together for the final prediction through the last convolutional layers.\n",
      "\n",
      "Please note that the proposed LPG layer is not designed to directly estimate global depth values on the corresponding scale because the training loss is only defined in terms of the final depth estimation (provided in Section 3.3). Together with outputs from the other LPG layers and reduc1x1, each output is interpreted to the global depth by contributing as a part of the nonlinear combination through the final convolutional layers. Therefore, they can have distinct ranges, learned as a base or precise relative compensation from the base at a spatial location, as shown in Figures 6 and 7.\n",
      "\n",
      "[The local planar guidance layer. We use a stack of 1 × 1 convolutions to get 4D coefficients estimations. (i.e., H/k × H/k × 4). Then the channels are split to pass through two different activation mechanisms to ensure plane coefficients’ constraint. Finally, they are fed into the planar guidance module to compute locally-defined relative depth estimations.]\n",
      "\n",
      "Here, we use the local planar assumption because, for a k × k region, it enables an efficient reconstruction with only four parameters. If we adopt typical upconvs for the reconstruction, the layers should be learned to have k² values properly instead of four. Therefore, we can expect that our strategy can be more effective because conventional upsampling would not give details on enlarged resolutions, while the local linear assumption can provide effective guidance.\n",
      "\n",
      "To guide features with the local planar assumption, we convert each estimated 4D plane coefficients to k × k local depth cues using ray-plane intersection:\n",
      "$$\\tilde{c}_i=\\frac{n_4}{n_1u_i+n_2v_i+n_3},\n",
      "    \\label{eq:compute_depth}$$\n",
      "where n = (n₁,n₂,n₃,n₄) denotes the estimated plane coefficients, (u_(i),v_(i)) are k × k patch-wise normalized coordinates of pixel i.\n",
      "\n",
      "Figure 8 shows the detail of the proposed layer. Through a stack of 1 × 1 convolutions which repeatedly reduce the number of channels by a factor of 2 until it reaches 3, we get a H/k × H/k × 3 feature map if we assume a square input. Then, we pass the feature map through two different ways to get local plane coefficient estimations: one way is a conversion to a unit normal vector (n₁,n₂,n₃), and the other is a sigmoid function defining the perpendicular distance n₄ between the plane and origin. After the sigmoid function we multiply the output with the maximum distance κ to get real depth values. Because a unit normal vector has only two degrees of freedom (i.e., polar and azimuthal angles θ, ϕ from predefined axes), we regard the first two channels of the given feature map as the angles and convert them to unit normal vectors using following equations.\n",
      "n₁ = sin (θ)cos (ϕ), n₂ = cos (ϕ), n₃ = sin (θ)sin (ϕ).\n",
      "Finally, they are concatenated again and used for estimation of c̃^(k × k) using Equation [eq:compute_depth].\n",
      "\n",
      "We design the local depth cue as an additive depth defined in local regions (i.e., k × k patches). Since features at the same spatial location in different stages are used together to predict the final depth, for efficient representation, we expect that global shapes would be learned at coarser scales while local details at finer scales. Also, they can interact with each other to compensate for erroneous estimations. We can represent the behavior of the last convolutional layer as follows.\n",
      "d̃ = f(W₁c̃^(1 × 1)+W₂c̃^(2 × 2)+W₃c̃^(4 × 4)+W₄c̃^(8 × 8)),\n",
      "where f is an activation function, W_(j), j ∈ {1, 2, 3, 4} denotes a corresponding linear transform representing the convolution. Please note that the proposed network learns on multiple scales, and by defining the training loss only in terms of the final estimation, d̃, we do not enforce parameters for each scale learns with the constant contribution. Therefore, in training, details for regions with sharp curvatures would be learned at finer scales while major structures at coarser scales. Also, there is the last chance in c̃^(1 × 1) to recover broken assumptions if exist in the upsampled estimations (c̃^(k × k), k ∈ {2, 4, 8}). From Figure 7, we can see small details behind the focused vehicle from blue- and black-boxed figures which demonstrate c̃^(2 × 2) and c̃^(1 × 1), respectively, while they are missing in the coarser scales, c̃^(8 × 8) and c̃^(4 × 4). Also, there are thick black estimations in c̃^(1 × 1) and c̃^(2 × 2) on the boundary of the vehicle compensating the over-estimations in c̃^(8 × 8) and c̃^(4 × 4). More examples are provided in the supplementary material.\n",
      "\n",
      "Training Loss\n",
      "\n",
      "In , Eigen et al introduce a scale-invariant error and inspired from it they use a following training loss:\n",
      "$$D(g) = \\frac{1}{T}\\sum_i g_i^2 - \\frac{\\lambda}{T^2}\\left(\\sum_i g_i\\right)^2,\n",
      "    \\label{eq:siloss}$$\n",
      "where g_(i) = log d̃_(i) − log d_(i) with the ground truth depth d_(i), λ = 0.5 and T denotes the number of pixels having valid ground truth values. By rewriting above equation,\n",
      "$$\\begin{aligned}\n",
      "    D(g) = \\frac{1}{T}\\sum_i g_i^2 - \\left(\\frac{1}{T} \\sum_i g_i\\right)^2 + (1-\\lambda)\\left(\\frac{1}{T} \\sum_i g_i\\right)^2,\\end{aligned}$$\n",
      "we can see that it is a sum of the variance and a weighted squared mean of the error in log space. Therefore, setting a higher λ enforces more focusing on minimizing the variance of the error, and we use λ = 0.85 in this work. Also, we observe that properly scaling the range of the loss function improves convergence and the final training result. Finally, we define our training loss L as follows:\n",
      "$$L = \\alpha \\sqrt{D(g)},\n",
      "    \\label{eq:loss}$$\n",
      "where α is a constant we set to 10 for all experiments.\n",
      "\n",
      "Experiments\n",
      "\n",
      "To verify the effectiveness of our approach, we provide experimental results from challenging benchmarks with various settings. After presenting the implementation details of our method, we provide experimental results on two challenging benchmarks covering both indoor and outdoor environments. We also provide scores on the online KITTI evaluation server comparing with published works. Then, we provide an ablation study to discuss a detailed analysis of the proposed core factors, and some qualitative results to demonstrate our approach comparing with competitors.\n",
      "\n",
      "Implementation Details\n",
      "\n",
      "We implement the proposed network using the open deep learning framework PyTorch . For training, we use Adam optimizer with β₁ = 0.9, β₂ = 0.999 and ϵ = 10⁻⁶, learning is scheduled via polynomial decay from base learning rate 10⁻⁴ with power p = 0.9. The total number of epochs is set to 50 with batch size 16 on a desktop equipped with four NVIDIA 1080ti GPUs for all experiments in this work.\n",
      "\n",
      "As the backbone network for dense feature extraction, we use ResNet-101 , ResNext-101 and DenseNet-161 with pretrained weights trained for image classification using ILSVRC dataset . Because weights at early convolutions are known to be well trained for primitive visual features, in the base networks, we fix the first two convolutional layers as well as batch normalization parameters in our training. Following , we use exponential linear units as an activation function, and upconv uses the nearest neighbor upsampling followed by a 3 × 3 convolution layer .\n",
      "\n",
      "To avoid over-fitting, we augment images before input to the network using random horizontal flipping as well as random contrast, brightness, and color adjustment in a range of [0.9, 1.1], with 50% of chance. We also use a random rotation of the input images in ranges of [−1,1] and [−2.5,2.5] degrees for KITTI and NYU datasets, respectively. We train our network on a random crop of size 352 × 704 for KITTI and 416 × 544 for NYU Depth V2 datasets.\n",
      "\n",
      "NYU Depth V2 Dataset\n",
      "\n",
      "The NYU Depth V2 dataset contains 120K RGB and depth pairs having a size of 480 × 640 acquired as video sequences using a Microsoft Kinect from 464 indoor scenes. We follow the official train/test split as previous works, using 249 scenes for training and 215 scenes (654 images) for testing. From the total 120K image-depth pairs, due to asynchronous capturing rates between RGB images and depth maps, we associate and sample them using timestamps by even-spacing in time, resulting in 24231 image-depth pairs for the training set. Using raw depth images and camera projections provided by the dataset, we align the image-depth pairs for accurate pixel registrations. We use κ = 10 for this dataset.\n",
      "\n",
      "KITTI Dataset\n",
      "\n",
      "KITTI provides the dataset with 61 scenes from “city\", “resiential\", “road\" and “campus\" categories. Because existing works commonly use a split proposed by Eigen et al. for the training and testing, we also follow it to compare with those works. Therefore, 697 images covering a total of 29 scenes are used for evaluation, and the remaining 32 scenes of 23,488 images are used for the training. We use κ = 80 for this dataset.\n",
      "\n",
      "Evaluation Result\n",
      "\n",
      "For evaluation, we use following metrics used by previous works:\n",
      "$\\small\\text{Threshold}: \\text{\\% of } \\tilde{d}_i \\text{ s.t.} \\max(\\frac{\\tilde{d}_i}{d_i}, \\frac{d_i}{\\tilde{d}_i})=\\delta<thr,$ $\\small\\text{Abs Rel}: \\frac{1}{|T|}\\sum_{\\tilde{d}\\in T}|\\tilde{d}-d|/d,$\n",
      "$\\small\\text{Sq Rel}: \\frac{1}{|T|}\\sum_{\\tilde{d}\\in T}||\\tilde{d}-d||^2/d,$ $\\small\\text{RMSE}: \\sqrt{\\frac{1}{|T|}\\sum_{\\tilde{d}\\in T}||\\tilde{d}-d||^2},$\n",
      "$\\small\\text{log10}: \\frac{1}{|T|}\\sum_{\\tilde{d}\\in T}|\\log_{10} \\tilde{d}-\\log_{10} d|,$ $\\small\\text{RMSE\\textit{log}}: \\sqrt{\\frac{1}{|T|}\\sum_{\\tilde{d}\\in T}||\\log \\tilde{d}-\\log d||^2},$ where T denotes a collection of pixels that the ground truth values are available.\n",
      "\n",
      "Using NYU Depth V2 dataset, the experimental results given in Table [tb:nyu] show that Ours-DenseNet achieves the state-of-the-art result with a significant margin in both of the inlier measures (i.e., δ < thr) and accuracy metrics (i.e., AbsRel, log10) except only RMSE. Our ResNext-based model also outperforms the method from Yin et al with a significant margin which has the same backbone network, ResNext-101 .\n",
      "\n",
      "In the evaluation using the KITTI dataset provided in Table [tb:result_kitti_eigen], ours outperforms all existing works with a significant margin. Please note that our ResNet-based model already outperforms the methods from Fu et al and Yin et al which use ResNet-101 and ResNext-101 as their backbone network, respectively. Only the root mean squared errors (RMSE) from ours are behind that of Fu et al.’s in the capturing range 0-80m. However, in the capturing range 0-50m, ours-ResNet achieves more than 10% improvement in RMSE from the result of Fu et al. Also, the proposed method achieves notable improvements in the inlier metrics (i.e., δ < thres), meaning more number of correctly estimated pixels as it can be seen from Figures [fg:kitti_qualitative_result] and [fg:nyu_qualitative_result] presenting qualitative comparison to our competitors.\n",
      "\n",
      "We also evaluate the proposed method on the online KITTI benchmark server with a model trained using KITTI’s official split. Apart from the training set, all other settings remain the same as in the experiment using KITTI’s Eigen split. We trained Ours-DenseNet for 50 epochs with 28,654 image-ground truth pairs sampled from the official training and validation set. As shown in Table [tb:kitti_online_benchmark], our method outperforms all the published works.\n",
      "\n",
      "Ablation Study\n",
      "\n",
      "Here, we conduct evaluations with variants of our network to see the effectiveness of the core factors. From the baseline network, which only consists of the base network (i.e., ResNet-101) and a simple upsampling layer, we increment the network with the core modules to see how the added factor improves the performance. The result is given in Table [tb:ablation_study]. As the core factors are added, the overall performance is improved, and the most significant improvement is made by adding the proposed local planar guidance layers. Please note that the LPG layers only require additional 0.1M trainable parameters used by 1x1 reduction layers. The final improvement comes from using the training loss defined in Equation [eq:loss]. Benefited from the robust base network DenseNet-161, ours achieves state-of-the-art performance with the significant margin while it requires the less number of parameters than the baseline.\n",
      "\n",
      "Experiments with Various Base Networks\n",
      "\n",
      "Because the proposed network adopts existing models as an encoder for dense feature extraction, it is desirable to see how the performance varies with various base networks that are widely used for similar applications. By changing the encoder with various models while other settings remained, we experimented with the proposed method using both of the NYU Depth V2 and KITTI’s Eigen split, and provide the result in Tables [tb:various_backbones_nyu] and [tb:various_backbones_kitti]. Note that ResNet-101, ResNext-101 and DenseNet-161 are identical to Ours-ResNet, Ours-ResNext and Ours-DenseNet, respectively, in Tables [tb:nyu] and [tb:result_kitti_eigen]. Interestingly, for the NYU Depth V2 dataset, DenseNet-161 results in the best performance, while for KITTI’s Eigen split, ResNext-101 achieves the state-of-the-art result. We consider this as an effect of the relatively lower variance of the data distribution in the indoor scenes of the NYU Depth V2 dataset, which can lead to a degeneration of performance with very deep models such as ResNext-101 in this experiment. Also, it is notable that our MobileNetV2-based model results in performance drop about only 3% for inlier measures and less than 15% drop for accuracy measures while it contains less than half the number of parameters and shows about three times speedup when compared to our model based on the DenseNet-161.\n",
      "\n",
      "Qualitative Result\n",
      "\n",
      "Finally, we discuss qualitative results from ours and competing works. As we can see from Figures [fg:kitti_qualitative_result] and [fg:nyu_qualitative_result], ours show much more precise object boundaries. However, in results from experiments using KITTI, we can see artifacts in the sky or upper part of the scenes. We consider this as a consequence of the very sparse ground truth depth data. Because certain regions are lacking valid depth values across the dataset, the network cannot be appropriately trained for those regions.\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[Input]\n",
      "\n",
      "[Fu et al. ]\n",
      "\n",
      "[Yin et al. ]\n",
      "\n",
      "[Ours]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[image]\n",
      "\n",
      "[Input]\n",
      "\n",
      "[Fu et al. ]\n",
      "\n",
      "[Yin et al. ]\n",
      "\n",
      "[Ours]\n",
      "\n",
      "Conclusion\n",
      "\n",
      "In this work, we have presented a supervised monocular depth estimation network and achieved state-of-the-art results. Benefiting from recent advances in deep learning, we design a network architecture that uses novel local planar guidance layers, giving an explicit relation from internal feature maps to the desired prediction for better training of the network. By deploying the proposed layers on multiple stages in the decoding phase, we have gained a significant improvement and shown several experimental results on challenging benchmarks to verify it. However, in experiments with the KITTI dataset, we observe frequent artifacts on the upper part of the scenes. We analyze this as an effect of the high sparsity of the ground truth across the dataset. As a consequence, we plan to investigate adopting into our framework a photometric reconstruction loss, which can provide far denser supervision to improve the performance further.\n",
      "\n",
      "\n",
      "Please answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\n",
      "\n",
      "#################################################\n",
      "\n",
      "Answer: [{'LEADERBOARD': {'Task': 'Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMS', 'Score': '0.407'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error', 'Score': '0.064'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMSE', 'Score': '0.392'}}]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(valid_data))\n",
    "\n",
    "print(f\"Index: {idx}\\n\")\n",
    "\n",
    "print(f\"Question: {valid_data[idx]['prompt']}\")\n",
    "print(\"\\n#################################################\\n\")\n",
    "print(f\"Answer: {valid_data[idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Input length of input_ids is 7980, but `max_length` is set to 3000. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.59 GiB. GPU 0 has a total capacty of 23.69 GiB of which 7.24 GiB is free. Including non-PyTorch memory, this process has 16.45 GiB memory in use. Of the allocated memory 15.88 GiB is allocated by PyTorch, and 263.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, max_length\u001b[38;5;241m=\u001b[39mscript_args\u001b[38;5;241m.\u001b[39mseq_length)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# pipe = pipeline(task=\"text-generation\", model=f\"{script_args.output_dir}/save_pretrained\", tokenizer=tokenizer, max_length=script_args.seq_length)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprompt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:201\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1113\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1114\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         )\n\u001b[1;32m   1118\u001b[0m     )\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1127\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1126\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1127\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1026\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1025\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1026\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:263\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/peft/peft_model.py:975\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 975\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/utils.py:2619\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2619\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2620\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:231\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    226\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\n\u001b[1;32m    227\u001b[0m         attn_weights, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    232\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.59 GiB. GPU 0 has a total capacty of 23.69 GiB of which 7.24 GiB is free. Including non-PyTorch memory, this process has 16.45 GiB memory in use. Of the allocated memory 15.88 GiB is allocated by PyTorch, and 263.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# prompt = \"Who is Leonardo Da Vinci?\"\n",
    "prompt = f\"Question: {valid_data[idx]['prompt']}\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=script_args.seq_length)\n",
    "# pipe = pipeline(task=\"text-generation\", model=f\"{script_args.output_dir}/save_pretrained\", tokenizer=tokenizer, max_length=script_args.seq_length)\n",
    "\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoderdecoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction.In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method. We also provide scores on the online KITTI evaluation server comparing with published works The NYU Depth V2 dataset: Result from the ablation study using the NYU Depth V2 dataset Using raw depth images and camera projections provided by the dataset, we align the image-depth pairs for accurate pixel registrations We use κ = 10 for this dataset KITTI provides the dataset with 61 scenes from \"city\", \"resiential\", \"road\" and \"campus\" categories Therefore, 697 images covering a total of 29 scenes are used for evaluation, and the remaining 32 scenes of 23,488 images are used for the training We use κ = 80 for this dataset For evaluation, we use following metrics used by previous works: where T denotes a collection of pixels that the ground truth values are available Using NYU Depth V2 dataset, the experimental results given in show that Ours-DenseNet achieves the state-of-the-art result with a significant margin Table 1: Evaluation results on NYU Depth v2. Ours outper- forms previous works with a significant margin in all mea- sures except only from AbsRel. - AbsRel log10 RMSE Table 2: Performance on KITTI Eigen split. (CS+K) denotes a model pre-trained on Cityscapes dataset [8] and fine-tuned on KITTI. higher is better cap lower is better RMSE Sq Rel Abs Rel RMSE log Table 3: Result on the online KITTI evaluation server. absErrorRel iRMSE sqErrorRel SILog Table 4: Result from the ablation study using the NYU Depth V2 dataset. Baseline: a network composed of only the dense feature extractor and direct estimation from it followed by an upsampling with a factor of 8, A: ASPP module attached after the dense feature extractor, U: using upconv layers in higher is better # Params log10 lower is better RMSE Sq Rel Abs Rel RMSE log Table 5: Experimental results using NYU Depth V2\n",
      "\n",
      "Please answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\n",
      "\n",
      "Answer: [{'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error', 'Score': '0.046'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error (distance to 2m)', 'Score': '0.044'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error (distance to 12m)', 'Score': '0.061'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error (distance to 80m)', 'Score': '0.144'}}, {'LEADERBOARD': {'\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Question: {valid_data[idx]['prompt']}\"\n",
    "\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: [{'LEADERBOARD': {'Task': 'Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMS', 'Score': '0.407'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error', 'Score': '0.064'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMSE', 'Score': '0.392'}}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: {valid_data[idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict.load_from_disk(f\"{script_args.test_dataset}\")\n",
    "    \n",
    "    # if args.streaming:\n",
    "    #     print(\"Loading the dataset in streaming mode\")\n",
    "    #     valid_data = dataset.take(args.size_valid_set)\n",
    "    #     train_data = dataset.skip(args.size_valid_set)\n",
    "    #     train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=None)\n",
    "    # else:\n",
    "    \n",
    "# dataset = dataset.train_test_split(test_size=0.005, seed=None)\n",
    "train_data = dataset[\"train\"].shuffle(seed=42)\n",
    "valid_data = dataset[\"validation\"].shuffle(seed=42)\n",
    "# valid_data = dataset[\"validation\"].shard(num_shards=10, index=0).shuffle(seed=42)\n",
    "\n",
    "# train_data[0]\n",
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data[97]['prompt'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 500 indices from valid_data\n",
    "# selected_indices = random.sample(range(len(valid_data)), 1)\n",
    "# selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 102\n",
      "\n",
      "Question: From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoderdecoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction.In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method. We also provide scores on the online KITTI evaluation server comparing with published works The NYU Depth V2 dataset: Result from the ablation study using the NYU Depth V2 dataset Using raw depth images and camera projections provided by the dataset, we align the image-depth pairs for accurate pixel registrations We use κ = 10 for this dataset KITTI provides the dataset with 61 scenes from \"city\", \"resiential\", \"road\" and \"campus\" categories Therefore, 697 images covering a total of 29 scenes are used for evaluation, and the remaining 32 scenes of 23,488 images are used for the training We use κ = 80 for this dataset For evaluation, we use following metrics used by previous works: where T denotes a collection of pixels that the ground truth values are available Using NYU Depth V2 dataset, the experimental results given in show that Ours-DenseNet achieves the state-of-the-art result with a significant margin Table 1: Evaluation results on NYU Depth v2. Ours outper- forms previous works with a significant margin in all mea- sures except only from AbsRel. - AbsRel log10 RMSE Table 2: Performance on KITTI Eigen split. (CS+K) denotes a model pre-trained on Cityscapes dataset [8] and fine-tuned on KITTI. higher is better cap lower is better RMSE Sq Rel Abs Rel RMSE log Table 3: Result on the online KITTI evaluation server. absErrorRel iRMSE sqErrorRel SILog Table 4: Result from the ablation study using the NYU Depth V2 dataset. Baseline: a network composed of only the dense feature extractor and direct estimation from it followed by an upsampling with a factor of 8, A: ASPP module attached after the dense feature extractor, U: using upconv layers in higher is better # Params log10 lower is better RMSE Sq Rel Abs Rel RMSE log Table 5: Experimental results using NYU Depth V2\n",
      "\n",
      "Please answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\n",
      "\n",
      "#################################################\n",
      "\n",
      "Answer: [{'LEADERBOARD': {'Task': 'Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMS', 'Score': '0.407'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'KITTI Eigen split', 'Metric': 'absolute relative error', 'Score': '0.064'}}, {'LEADERBOARD': {'Task': 'Monocular Depth Estimation', 'Dataset': 'NYU-Depth V2', 'Metric': 'RMSE', 'Score': '0.392'}}]\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0, len(valid_data))\n",
    "\n",
    "print(f\"Index: {idx}\\n\")\n",
    "\n",
    "print(f\"Question: {valid_data[idx]['prompt']}\")\n",
    "print(\"\\n#################################################\\n\")\n",
    "print(f\"Answer: {valid_data[idx]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Question: From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoderdecoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover back to the original resolution for effective dense prediction.In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method. We also provide scores on the online KITTI evaluation server comparing with published works The NYU Depth V2 dataset: Result from the ablation study using the NYU Depth V2 dataset Using raw depth images and camera projections provided by the dataset, we align the image-depth pairs for accurate pixel registrations We use κ = 10 for this dataset KITTI provides the dataset with 61 scenes from \"city\", \"resiential\", \"road\" and \"campus\" categories Therefore, 697 images covering a total of 29 scenes are used for evaluation, and the remaining 32 scenes of 23,488 images are used for the training We use κ = 80 for this dataset For evaluation, we use following metrics used by previous works: where T denotes a collection of pixels that the ground truth values are available Using NYU Depth V2 dataset, the experimental results given in show that Ours-DenseNet achieves the state-of-the-art result with a significant margin Table 1: Evaluation results on NYU Depth v2. Ours outper- forms previous works with a significant margin in all mea- sures except only from AbsRel. - AbsRel log10 RMSE Table 2: Performance on KITTI Eigen split. (CS+K) denotes a model pre-trained on Cityscapes dataset [8] and fine-tuned on KITTI. higher is better cap lower is better RMSE Sq Rel Abs Rel RMSE log Table 3: Result on the online KITTI evaluation server. absErrorRel iRMSE sqErrorRel SILog Table 4: Result from the ablation study using the NYU Depth V2 dataset. Baseline: a network composed of only the dense feature extractor and direct estimation from it followed by an upsampling with a factor of 8, A: ASPP module attached after the dense feature extractor, U: using upconv layers in higher is better # Params log10 lower is better RMSE Sq Rel Abs Rel RMSE log Table 5: Experimental results using NYU Depth V2\n",
      "\n",
      "Please answer a question about this article. If the question is unanswerable, say \"unanswerable\". What are the values for the following properties to construct a Leaderboard for the model introduced in this article: task, dataset, metric, and score?\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item task: monocular depth estimation\n",
      "\\item dataset: NYU Depth V2\n",
      "\\item metric: RMSE log10\n",
      "\\item score: 0.225\n",
      "\\end{itemize}\n",
      "\n",
      "\\section{Related Work}\n",
      "\n",
      "\\subsection{Monocular Depth Estimation}\n",
      "\n",
      "Monocular depth estimation is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. Therefore, the key to solving this problem is to impose reasonable prior knowledge into the network. In the early days, depth estimation is usually done by using the hand-crafted features such as SIFT or HOG and then fed into the linear regression model [1]. However, recent works based on deep convolutional neural networks show great progress with plausible results. In the early days, the convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoderdecoder schemes, repeated strided convolution and spatial\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Question: {valid_data[idx]['prompt']}\"\n",
    "\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "pipe = pipeline(task=\"text-generation\", model=base_model, tokenizer=tokenizer, max_length=script_args.seq_length)\n",
    "\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "result = pipe(f\"<s>[INST] {prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid_data = valid_data.to_pandas()\n",
    "# df_valid_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(df_valid_data.at[35275, 'prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str(df_valid_data.at[35275, 'prompt'])\n",
    "script_args.seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating text using models like GPT-3 or GPT-4 from OpenAI, several hyperparameters can be tweaked to influence the output. Here's a breakdown of the parameters you mentioned:\n",
    "\n",
    "1. **`temperature`**: \n",
    "    - **Purpose**: Adjusts the randomness of the model's outputs.\n",
    "    - **Values**:\n",
    "        - Closer to 0: The model will be more deterministic and more likely to produce the most probable next word at each step.\n",
    "        - Closer to 1: The model's outputs become more random.\n",
    "    - **`temperature=0.2`**: In this case, the model's outputs will be more deterministic and confident. There's less randomness.\n",
    "\n",
    "2. **`top_p` (also known as \"nucleus sampling\")**:\n",
    "    - **Purpose**: Prunes the vocabulary before sampling the next word.\n",
    "    - **Values**:\n",
    "        - At `top_p=1.0`: Use all words in the vocabulary.\n",
    "        - At `top_p=0.95`: Use only the smallest set of words such that their cumulative probability exceeds 0.95.\n",
    "    - **`top_p=0.95`**: The model will only consider the top words that have a cumulative probability of 95%. It helps in reducing the chance of very random words appearing in the generated text.\n",
    "\n",
    "3. **`top_k`**:\n",
    "    - **Purpose**: Restricts the model's prediction to the top `k` most likely next words.\n",
    "    - **Values**: Larger values make outputs more random, while smaller values make it less random.\n",
    "    - **`top_k=40`**: The model will only consider the top 40 words for its next word prediction. It's another way to reduce randomness, but it's typically used in conjunction with `top_p` for better results.\n",
    "\n",
    "4. **`max_new_tokens`**:\n",
    "    - **Purpose**: Limits the length of the generated output.\n",
    "    - **`max_new_tokens=script_args.seq_length`**: The generated output will be limited to the length specified by `script_args.seq_length`. It ensures that the model doesn't generate exceedingly long responses.\n",
    "\n",
    "5. **`repetition_penalty`**:\n",
    "    - **Purpose**: Penalizes words that are already seen in the output, especially if repeated multiple times.\n",
    "    - **Values**:\n",
    "        - Equal to 1: No penalty applied.\n",
    "        - Greater than 1: Apply a penalty.\n",
    "    - **`repetition_penalty=1.0`**: In this case, no penalty is applied for repeated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_kwargs = dict(\n",
    "#     input_ids=eval_dataset,\n",
    "#     temperature=0.1, \n",
    "#     top_p=1.0, \n",
    "#     # top_p=0.9, \n",
    "#     top_k=20,\n",
    "#     max_new_tokens=4000,\n",
    "#     repetition_penalty=1.0\n",
    "# )\n",
    "\n",
    "# outputs = base_model.generate(**generate_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # device = 'cpu'\n",
    "\n",
    "# # tokenizer.cpu()\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# sample_data = valid_data[5]\n",
    "# # inputs = tokenizer.encode(f\"{train_data[100]['prompt'][-7000:]}\", return_tensors=\"pt\").to(device)\n",
    "# # inputs = tokenizer.encode(f\"{train_data[0]['prompt']}\", return_tensors=\"pt\").to(device)\n",
    "# inputs = tokenizer.encode(f\"Question: {sample_data['prompt']}\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['answer']}\"\n",
    "# # generate_kwargs = dict(\n",
    "# #     input_ids=inputs,\n",
    "# #     temperature=0.2, \n",
    "# #     top_p=0.90, \n",
    "# #     top_k=40,\n",
    "# #     max_new_tokens=script_args.seq_length,\n",
    "# #     repetition_penalty=1.1\n",
    "# # )\n",
    "\n",
    "# # generate_kwargs = dict(\n",
    "# #         input_ids=inputs,\n",
    "# #         temperature=0.5, \n",
    "# #         top_p=1.0, \n",
    "# #         # top_p=0.9, \n",
    "# #         top_k=1000,\n",
    "# #         max_new_tokens=4000,\n",
    "# #         repetition_penalty=1.0\n",
    "# #     )\n",
    "\n",
    "# generate_kwargs = dict(\n",
    "#     input_ids=inputs,\n",
    "#     temperature=0.1, \n",
    "#     top_p=1.0, \n",
    "#     # top_p=0.9, \n",
    "#     top_k=20,\n",
    "#     max_new_tokens=4000,\n",
    "#     repetition_penalty=1.0\n",
    "# )\n",
    "\n",
    "\n",
    "# outputs = base_model.generate(**generate_kwargs)\n",
    "# predictions = tokenizer.decode(outputs[0])\n",
    "# print(f\"Label: \\n{sample_data['answer']}\\n\")\n",
    "# print(\"Prediction\")\n",
    "# print(predictions.split(\"Answer: \")[-1].replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs[0].shape\n",
    "# # tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inputs = tokenizer.batch_encode(f\"Question: {valid_ex['prompt']}\", return_tensors=\"pt\").to(device)\n",
    "# inputs = tokenizer.batch_encode(eval_dataset, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# # text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['answer']}\"\n",
    "# # generate_kwargs = dict(\n",
    "# #     input_ids=inputs,\n",
    "# #     temperature=0.2, \n",
    "# #     top_p=0.90, \n",
    "# #     top_k=40,\n",
    "# #     max_new_tokens=script_args.seq_length,\n",
    "# #     repetition_penalty=1.1\n",
    "# # )\n",
    "\n",
    "# generate_kwargs = dict(\n",
    "#     input_ids=inputs,\n",
    "#     temperature=0.2, \n",
    "#     top_p=0.95, \n",
    "#     top_k=40,\n",
    "#     max_new_tokens=500,\n",
    "#     repetition_penalty=1.3\n",
    "# )\n",
    "\n",
    "# outputs = model.generate(**generate_kwargs)\n",
    "# # predictions = tokenizer.batch_decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2353 [04:27<34:52:22, 53.47s/it]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "preds = []\n",
    "\n",
    "# # Randomly select 500 indices from valid_data\n",
    "# selected_indices = random.sample(range(len(valid_data)), 500)\n",
    "\n",
    "# # Create a new list of selected examples\n",
    "# selected_valid_data = [valid_data[i] for i in selected_indices]\n",
    "\n",
    "# for i, valid_ex in tqdm(enumerate(selected_valid_data), total=500):\n",
    "#     # Your processing code here\n",
    "    \n",
    "for i, valid_ex in tqdm(enumerate(valid_data), total=len(valid_data)):\n",
    "    # if i <= 100 :\n",
    "    #     continue \n",
    "\n",
    "    if len(valid_ex['prompt'].split()) >= 2400:\n",
    "        continue\n",
    "        \n",
    "    inputs = tokenizer.encode(f\"Question: {valid_ex['prompt']}\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # text = f\"Question: {example['prompt']}\\n\\nAnswer: {example['answer']}\"\n",
    "    # generate_kwargs = dict(\n",
    "    #     input_ids=inputs,\n",
    "    #     temperature=0.2, \n",
    "    #     top_p=0.90, \n",
    "    #     top_k=40,\n",
    "    #     max_new_tokens=script_args.seq_length,\n",
    "    #     repetition_penalty=1.1\n",
    "    # )\n",
    "    \n",
    "    # generate_kwargs = dict(\n",
    "    #     input_ids=inputs,\n",
    "    #     temperature=0.2, \n",
    "    #     top_p=0.95, \n",
    "    #     top_k=40,\n",
    "    #     max_new_tokens=script_args.seq_length,\n",
    "    #     repetition_penalty=1.3\n",
    "    # )\n",
    "\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=inputs,\n",
    "        temperature=0.5, \n",
    "        top_p=1.0, \n",
    "        # top_p=0.9, \n",
    "        top_k=1000,\n",
    "        max_new_tokens=4000,\n",
    "        repetition_penalty=1.0\n",
    "    )\n",
    "    \n",
    "    outputs = base_model.generate(**generate_kwargs)\n",
    "    predictions = tokenizer.decode(outputs[0])\n",
    "\n",
    "    preds.append(predictions.split(\"Answer: \")[-1].replace(\"</s>\", \"\"))\n",
    "    labels.append(valid_ex['answer'])\n",
    "    \n",
    "    if i >= 5:\n",
    "        break\n",
    "    \n",
    "    # ipdb.set_trace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_ = labels[:200]\n",
    "# preds_ = preds[:200]\n",
    "\n",
    "labels_ = labels\n",
    "preds_ = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabels_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# labels_[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(preds_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/nfs/home/kabenamualus/Research/LLLM-LeaderboardLLM/notebooks/evaluation_metrics.py\u001b[0m(403)\u001b[0;36mmake_list_of_pairs_json_based\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    402 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 403 \u001b[0;31m                \u001b[0mitem1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    404 \u001b[0;31m                \u001b[0mitem2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  L\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'L' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m    398 \u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mitem1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_contribution_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    399 \u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mitem2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_contribution_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    400 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    401 \u001b[0m                \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    402 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 403 \u001b[0;31m                \u001b[0mitem1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    404 \u001b[0m                \u001b[0mitem2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    405 \u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    406 \u001b[0m                    \u001b[0mitem1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    407 \u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    408 \u001b[0m                    \u001b[0mitem2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  item1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '89.5'}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  item2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '82.5%'}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  label_list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'MAP', 'Score': '81.84'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '91.11'}}]\", 'unanswerable', \"[{'LEADERBOARD': {'Task': 'Multi-Label Classification', 'Dataset': 'MS-COCO', 'Metric': 'mAP', 'Score': '77.1'}}, {'LEADERBOARD': {'Task': 'Multi-Label Classification', 'Dataset': 'NUS-WIDE', 'Metric': 'MAP', 'Score': '62.0'}}]\", \"[{'LEADERBOARD': {'Task': 'Image Generation', 'Dataset': 'RC-49', 'Metric': 'Intra-FID', 'Score': '0.389'}}]\", \"[{'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'F-measure (Decay)', 'Score': '1.8'}}, {'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'F-measure (Mean)', 'Score': '65.3'}}, {'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'F-measure (Recall)', 'Score': '73.8'}}, {'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'J&F', 'Score': '68'}}, {'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'Jaccard (Decay)', 'Score': '1.5'}}, {'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'Jaccard (Mean)', 'Score': '70.7'}}, {'LEADERBOARD': {'Task': 'Unsupervised Video Object Segmentation', 'Dataset': 'DAVIS 2016', 'Metric': 'Jaccard (Recall)', 'Score': '83.5'}}]\", \"[{'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'WN18', 'Metric': 'MRR', 'Score': '0.911'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'FB15k', 'Metric': 'Hits@10', 'Score': '0.914'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'FB15k', 'Metric': 'MRR', 'Score': '0.841'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'WN18RR', 'Metric': 'MRR', 'Score': '0.455'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'FB15k-237', 'Metric': 'Hits@10', 'Score': '0.548'}}, {'LEADERBOARD': {'Task': 'Link Prediction', 'Dataset': 'FB15k-237', 'Metric': 'MRR', 'Score': '0.357'}}]\"]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  len(label_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  label_list[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'MAP', 'Score': '81.84'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '91.11'}}]\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  prediction_list[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '82.5%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-10', 'Score': '94.5%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-20', 'Score': '96.9%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-5', 'Score': '92.5%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'mAP', 'Score': '67.9%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'MAP', 'Score': '78.1'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-1', 'Score': '80.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-10', 'Score': '92.0'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-20', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-5', 'Score': '94.1'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '68.8'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '75.8'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-10', 'Score': '88.1'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-20', 'Score': '92.2'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-5', 'Score': '84.2'}}]\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'MAP', 'Score': '81.84'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '91.11'}}]\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '82.5%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-10', 'Score': '94.5%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-20', 'Score': '96.9%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-5', 'Score': '92.5%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'mAP', 'Score': '67.9%'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'MAP', 'Score': '78.1'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-1', 'Score': '80.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-10', 'Score': '92.0'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-20', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501-Dets-0.25', 'Metric': 'Rank-5', 'Score': '94.1'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '68.8'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '75.8'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-10', 'Score': '88.1'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-20', 'Score': '92.2'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-5', 'Score': '84.2'}}]\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  ll\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m    391 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mmake_list_of_pairs_json_based\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    392 \u001b[0m    \u001b[0;31m# make list of (label,prediction,similarity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    393 \u001b[0m    \u001b[0mlist_of_label_prediction_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    394 \u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    395 \u001b[0m        \u001b[0mpair_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    396 \u001b[0m        \u001b[0mlabel_contribution_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_contribution_list_json_based\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    397 \u001b[0m        \u001b[0mprediction_contribution_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_contribution_list_json_based\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    398 \u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mitem1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_contribution_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    399 \u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mitem2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction_contribution_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    400 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    401 \u001b[0m                \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    402 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 403 \u001b[0;31m                \u001b[0mitem1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    404 \u001b[0m                \u001b[0mitem2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    405 \u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    406 \u001b[0m                    \u001b[0mitem1_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    407 \u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    408 \u001b[0m                    \u001b[0mitem2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    409 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    410 \u001b[0m                pair_list.append(\n",
      "\u001b[1;32m    411 \u001b[0m                    \u001b[0;34m(\u001b[0m\u001b[0mitem1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_fuzz_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem1_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem2_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    412 \u001b[0m                )\n",
      "\u001b[1;32m    413 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    414 \u001b[0m        max_selectable_pairs = min(\n",
      "\u001b[1;32m    415 \u001b[0m            \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_contribution_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_contribution_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    416 \u001b[0m        )\n",
      "\u001b[1;32m    417 \u001b[0m        top_similar_pairs = sorted(pair_list, key=lambda x: x[2], reverse=True)[\n",
      "\u001b[1;32m    418 \u001b[0m            \u001b[0;34m:\u001b[0m\u001b[0mmax_selectable_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    419 \u001b[0m        ]\n",
      "\u001b[1;32m    420 \u001b[0m        \u001b[0mlist_of_label_prediction_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_similar_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    421 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    422 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlist_of_label_prediction_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    423 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'MAP', 'Score': '81.84'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '91.11'}}]\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  get_contribution_list_json_based(label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'MAP', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'Market-1501', 'Metric': 'Rank-1', 'Score': '95.7'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'MAP', 'Score': '81.84'}}, {'LEADERBOARD': {'Task': 'Person Re-Identification', 'Dataset': 'DukeMTMC-reID', 'Metric': 'Rank-1', 'Score': '91.11'}}]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  get_contribution_list_json_based(\"unanswerable\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unanswerable']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "results = Metrics.evaluate_property_wise_json_based(label_list=labels_, prediction_list=preds_)\n",
    "results.update(Metrics.evaluate_rouge(label_list=labels_, prediction_list=preds_))\n",
    "\n",
    "print(f\"Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "# clf_metrics.compute(predictions=[1 if \"unanswerable\" == x.replace(\"</s>\", \"\") else 0 for x in preds], \n",
    "#                     references=[1 if \"unanswerable\" == x else 0 for x in labels]\n",
    "#                     # references=[1 for df['answer'].tolist()]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rouge = evaluate.load('rouge')\n",
    "\n",
    "# results = rouge.compute(\n",
    "#     predictions=[pred.replace(\"</s>\", \"\") for pred in preds],\n",
    "#     # predictions=preds,\n",
    "#     references=labels\n",
    "# )\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "# clf_metrics.compute(predictions=[1 if \"unanswerable\" == x.replace(\"</s>\", \"\") else 0 for x in preds], \n",
    "#                     references=[1 if \"unanswerable\" == x else 0 for x in labels]\n",
    "#                     # references=[1 for df['answer'].tolist()]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge = evaluate.load('rouge')\n",
    "\n",
    "# results = rouge.compute(\n",
    "#     predictions=[pred.replace(\"</s>\", \"\") for pred in preds],\n",
    "#     # predictions=preds,\n",
    "#     references=labels\n",
    "# )\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for x in tqdm(preds):\n",
    "#     if \"unanswerable\" in x:\n",
    "#         ipdb.set_trace()\n",
    "#     else:\n",
    "#         i+=1\n",
    "        \n",
    "#         # print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.g5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "036d1c9003ee43acb0c2b89783ef7317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04be517c49014350a3a4aed13af1beb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "06c39c7b19ff4102a059335937096a77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07e9a50ed7014724aaf6f083e481cc01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaa4609ff63a4451a2f27d87265b3f71",
      "placeholder": "​",
      "style": "IPY_MODEL_e8fcd026196044a08250eeb8f0c9bdd8",
      "value": " 792k/792k [00:01&lt;00:00, 537kB/s]"
     }
    },
    "080d2b5824c340848a99a988f65199bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b95b3978ff674a119683e3ac0fe936e9",
       "IPY_MODEL_dfae349d2c6e4efb93933fc4c011fe97",
       "IPY_MODEL_21a2c896f1c24781b4fe5834cea2454c"
      ],
      "layout": "IPY_MODEL_ba52a3af729042e9b11f1d4e22a553eb"
     }
    },
    "0effbee2d34943fc8d6d0d3a265d0766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0f2ce18b656a4e7ead8774f9c28b4eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "1144779119e843109365495f972af363": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68df245747594d27b8db61a63ea30b36",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4ea5fb72af6e46438af4ad47cbbd32d2",
      "value": 2
     }
    },
    "13466f14b2484f7dbfd7ffe2ecc395f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "13f3d839ed0249c082da5c8da1ebd386": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee070df2cc9b42d6891e5ae557040066",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a926a4c6b7f345b2a021410743ed5d53",
      "value": 500
     }
    },
    "1572ca2178b64d0d9accd4c7620d0f96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16f0f99dbcfa4ff8b459aa5d3d9c74cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "170cc57e9f92469597b6aab8962a8ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c607df922234a20b8f72f92b965c452": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e915a28fc764077977f06cf12c95c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21a2c896f1c24781b4fe5834cea2454c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b410e5b0ca02405f9d71072f1e34b2ba",
      "placeholder": "​",
      "style": "IPY_MODEL_44389af5674642c1891cc2bf208838a6",
      "value": " 242M/242M [00:03&lt;00:00, 72.8MB/s]"
     }
    },
    "22702932b5dd4bdca2ca98bd75b20a93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23fc564de44144718f5fd9bbe921ae80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26ffcb55db4149d38c34570982fa0fca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2921977fec8c4f54b727f6aa12334d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fb7ca7667254da58ef251e6c2e4483e",
      "placeholder": "​",
      "style": "IPY_MODEL_91f420fdfedb464a8455c52b05bbdaad",
      "value": "Downloading: 100%"
     }
    },
    "29edae08655f4c478cddb3e5441e8ae6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40915c810d8e41f6a7bcc8faa84ba0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7feb6429c4e24fd89aab0a3b0efd38eb",
      "placeholder": "​",
      "style": "IPY_MODEL_e88c4810f71346019c6d9765cacc3284",
      "value": " 500/500 [00:27&lt;00:00, 18.35it/s]"
     }
    },
    "44389af5674642c1891cc2bf208838a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45746813cd3c4d82ada51bf58274a791": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b42a824910bf495289c9f56f241fc6f6",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_604014d142594f4f87ce70e164d94a14",
      "value": 500
     }
    },
    "4c02d0ce16fe4c6187a2c8b952e8419f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22702932b5dd4bdca2ca98bd75b20a93",
      "placeholder": "​",
      "style": "IPY_MODEL_170cc57e9f92469597b6aab8962a8ce3",
      "value": "Downloading: 100%"
     }
    },
    "4ea5fb72af6e46438af4ad47cbbd32d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f45cc2640f44395a635a928f6d2f4f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ec023301cdad42cdabc32be9ee003f2a",
       "IPY_MODEL_cb9a3ee490e248398094c6789cc8526e",
       "IPY_MODEL_40915c810d8e41f6a7bcc8faa84ba0bb"
      ],
      "layout": "IPY_MODEL_96e050ff888943fc90182209e4d72227"
     }
    },
    "5f5740fe5a3e45289881a5ed5a498764": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "604014d142594f4f87ce70e164d94a14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6519d08cb83a4e7386bdda75f8f16861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68df245747594d27b8db61a63ea30b36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c2c9da14016449a8c13e08b395f7b23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cd3ede915c84c629bf46d234388b02c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbad20d361c6425cbb716354a9e8d328",
      "placeholder": "​",
      "style": "IPY_MODEL_96a9941107bd4e9b984755365e80df22",
      "value": " 500/500 [00:27&lt;00:00, 18.43it/s]"
     }
    },
    "6f01127a146d443a973c0c3674663559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc6fda81438846b8bd09a059bc0b5553",
       "IPY_MODEL_45746813cd3c4d82ada51bf58274a791",
       "IPY_MODEL_6cd3ede915c84c629bf46d234388b02c"
      ],
      "layout": "IPY_MODEL_13466f14b2484f7dbfd7ffe2ecc395f7"
     }
    },
    "71b01ab113c341fc8e4c8a2e36f336f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e915a28fc764077977f06cf12c95c04",
      "placeholder": "​",
      "style": "IPY_MODEL_bffcb591f4d74ebab31550f134f89dca",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "74f7576971d74113aa0605f41eb5fb89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c69b41b3fb8748cea2e7770562b47ce3",
      "placeholder": "​",
      "style": "IPY_MODEL_a04320425bd94c79a497cc7dd9b144ce",
      "value": " 1.21k/1.21k [00:00&lt;00:00, 69.2kB/s]"
     }
    },
    "7c90ba46512d4a94a1971f5f4b4d97d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7feb6429c4e24fd89aab0a3b0efd38eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "853f16aaaa5c4d13a67b143f2e946c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4a7c18179414629ab20376d4180a8a7",
      "placeholder": "​",
      "style": "IPY_MODEL_d5e1a9a72304460a9021598550913edc",
      "value": " 1000/1000 [02:05&lt;00:00,  7.96it/s, loss=1.16, v_num=2]"
     }
    },
    "8a1d7051e6ce4cbe8ad268b12751f4ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c02d0ce16fe4c6187a2c8b952e8419f",
       "IPY_MODEL_cffb3f880b7f45719ad19c6fbb082948",
       "IPY_MODEL_07e9a50ed7014724aaf6f083e481cc01"
      ],
      "layout": "IPY_MODEL_23fc564de44144718f5fd9bbe921ae80"
     }
    },
    "8fb7ca7667254da58ef251e6c2e4483e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f420fdfedb464a8455c52b05bbdaad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93adec39ba514f24a582ddbd529e01a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96a9941107bd4e9b984755365e80df22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96e050ff888943fc90182209e4d72227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "9ab910f3fc8540f591c0db3dffc74cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb3484e0e3d14bd7b8b7b0163fc6c81a",
      "placeholder": "​",
      "style": "IPY_MODEL_6519d08cb83a4e7386bdda75f8f16861",
      "value": " 2/2 [00:00&lt;00:00, 23.71it/s]"
     }
    },
    "9cfda34969e044acb3368d547caf95ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2b40eec0a5d4ec189a327d95a1cfd90",
      "placeholder": "​",
      "style": "IPY_MODEL_f9fe48215ce14b91b0a57cd3c7096334",
      "value": "Sanity Checking DataLoader 0: 100%"
     }
    },
    "9d8a386189cc4a9c81b836fb41fa4f64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29edae08655f4c478cddb3e5441e8ae6",
      "placeholder": "​",
      "style": "IPY_MODEL_de53a4b226e541d28da4547ac58579ff",
      "value": "Epoch 2: 100%"
     }
    },
    "9d96616dfac14f2d8535a07cc5e13711": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71b01ab113c341fc8e4c8a2e36f336f9",
       "IPY_MODEL_13f3d839ed0249c082da5c8da1ebd386",
       "IPY_MODEL_ab368c585dfa4b9ab4f97c7cd9d52f77"
      ],
      "layout": "IPY_MODEL_fdda03cbcaae4da1815b38ba5d40a7f0"
     }
    },
    "a04320425bd94c79a497cc7dd9b144ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a926a4c6b7f345b2a021410743ed5d53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa8d3f0d5edc43519b03b9360708f8b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaa4609ff63a4451a2f27d87265b3f71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab368c585dfa4b9ab4f97c7cd9d52f77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1572ca2178b64d0d9accd4c7620d0f96",
      "placeholder": "​",
      "style": "IPY_MODEL_c5ae98203edb4bdb83443d3f05b08247",
      "value": " 500/500 [00:27&lt;00:00, 18.34it/s]"
     }
    },
    "ae41b724aa4f469f8c408848d2a9de1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fccbbd51784e428697cd8a47d9b94317",
      "max": 1206,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04be517c49014350a3a4aed13af1beb4",
      "value": 1206
     }
    },
    "afa135917c1f4273bfdcc9e6f8c56683": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "b410e5b0ca02405f9d71072f1e34b2ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b42a824910bf495289c9f56f241fc6f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b572e3cd384c46499d0e8e4f7d52123a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b95b3978ff674a119683e3ac0fe936e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eef3c52f6b534523870e3ea61223014b",
      "placeholder": "​",
      "style": "IPY_MODEL_1c607df922234a20b8f72f92b965c452",
      "value": "Downloading: 100%"
     }
    },
    "ba52a3af729042e9b11f1d4e22a553eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb3484e0e3d14bd7b8b7b0163fc6c81a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bffcb591f4d74ebab31550f134f89dca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2b40eec0a5d4ec189a327d95a1cfd90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5ae98203edb4bdb83443d3f05b08247": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c69b41b3fb8748cea2e7770562b47ce3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6ccf24b960c4b25a59265c68780a0e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9d8a386189cc4a9c81b836fb41fa4f64",
       "IPY_MODEL_e40763a82dc34304aca5e042c9e77375",
       "IPY_MODEL_853f16aaaa5c4d13a67b143f2e946c3a"
      ],
      "layout": "IPY_MODEL_0f2ce18b656a4e7ead8774f9c28b4eda"
     }
    },
    "cb9a3ee490e248398094c6789cc8526e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b572e3cd384c46499d0e8e4f7d52123a",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_93adec39ba514f24a582ddbd529e01a0",
      "value": 500
     }
    },
    "cbad20d361c6425cbb716354a9e8d328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cce9d70e2dc94fc398e63efce16ce4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cfda34969e044acb3368d547caf95ef",
       "IPY_MODEL_1144779119e843109365495f972af363",
       "IPY_MODEL_9ab910f3fc8540f591c0db3dffc74cb5"
      ],
      "layout": "IPY_MODEL_afa135917c1f4273bfdcc9e6f8c56683"
     }
    },
    "cffb3f880b7f45719ad19c6fbb082948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06c39c7b19ff4102a059335937096a77",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26ffcb55db4149d38c34570982fa0fca",
      "value": 791656
     }
    },
    "d5e1a9a72304460a9021598550913edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da395e550e3f40379416bc4b2ae0acd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "de53a4b226e541d28da4547ac58579ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dfae349d2c6e4efb93933fc4c011fe97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa8d3f0d5edc43519b03b9360708f8b2",
      "max": 242065649,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da395e550e3f40379416bc4b2ae0acd0",
      "value": 242065649
     }
    },
    "e340d5c2a56247918e94384810ea21d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e40763a82dc34304aca5e042c9e77375": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f5740fe5a3e45289881a5ed5a498764",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0effbee2d34943fc8d6d0d3a265d0766",
      "value": 1000
     }
    },
    "e88c4810f71346019c6d9765cacc3284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8fcd026196044a08250eeb8f0c9bdd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf189ef3899443fb90e40e5abd4f088": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2921977fec8c4f54b727f6aa12334d09",
       "IPY_MODEL_ae41b724aa4f469f8c408848d2a9de1f",
       "IPY_MODEL_74f7576971d74113aa0605f41eb5fb89"
      ],
      "layout": "IPY_MODEL_7c90ba46512d4a94a1971f5f4b4d97d8"
     }
    },
    "ec023301cdad42cdabc32be9ee003f2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e340d5c2a56247918e94384810ea21d9",
      "placeholder": "​",
      "style": "IPY_MODEL_6c2c9da14016449a8c13e08b395f7b23",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "ee070df2cc9b42d6891e5ae557040066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eef3c52f6b534523870e3ea61223014b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4a7c18179414629ab20376d4180a8a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9fe48215ce14b91b0a57cd3c7096334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc6fda81438846b8bd09a059bc0b5553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_036d1c9003ee43acb0c2b89783ef7317",
      "placeholder": "​",
      "style": "IPY_MODEL_16f0f99dbcfa4ff8b459aa5d3d9c74cd",
      "value": "Validation DataLoader 0: 100%"
     }
    },
    "fccbbd51784e428697cd8a47d9b94317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdda03cbcaae4da1815b38ba5d40a7f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
